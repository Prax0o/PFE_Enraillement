{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from collections import deque\n",
    "import progressbar\n",
    "\n",
    "from texttable import Texttable\n",
    "from IPython.display import clear_output\n",
    "import tensorflow as tf\n",
    "tf_config=tf.compat.v1.ConfigProto()\n",
    "tf_config.gpu_options.allow_growth=True\n",
    "sess = tf.compat.v1.Session(config=tf_config)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_2center(x0, y0, x1, y1):\n",
    "    width = x1 - x0\n",
    "    height = y1 - y0\n",
    "    x = x0 + width / 2\n",
    "    y = y0 + height / 2\n",
    "    return x, y\n",
    "\n",
    "def get_4points(x_center, y_center, width, height):\n",
    "    x0 = x_center - width / 2\n",
    "    y0 = y_center - height / 2\n",
    "    x1 = x_center + width / 2\n",
    "    y1 = y_center - height / 2\n",
    "    x2 = x_center + width / 2\n",
    "    y2 = y_center + height / 2\n",
    "    x3 = x_center - width / 2\n",
    "    y3 = y_center + height / 2\n",
    "    return x0, y0, x1, y1, x2, y2, x3, y3\n",
    "\n",
    "# Dessin des rails\n",
    "def init_rails(height, thickness, canvas):\n",
    "    canvas.create_line(0, height-thickness, 500, height-thickness, width=5)\n",
    "    canvas.create_line(0, height+thickness, 500, height+thickness, width=5)\n",
    "\n",
    "# Dessin de la route\n",
    "def init_roads(width, thickness, canvas):\n",
    "    canvas.create_line(width-thickness, 0, width-thickness, 500, width=2)\n",
    "    canvas.create_line(width+thickness, 0, width+thickness, 500, width=2)\n",
    "\n",
    "def rotate_point(x, y, center_x, center_y, angle_degre):\n",
    "    radians = angle_degre * math.pi / 180\n",
    "    x_rotated = (x - center_x) * math.cos(radians) - (y - center_y) * math.sin(radians) + center_x\n",
    "    y_rotated = (x - center_x) * math.sin(radians) + (y - center_y) * math.cos(radians) + center_y\n",
    "    return (x_rotated, y_rotated)\n",
    "\n",
    "def rotate(polygon_id, angle, canvas):\n",
    "    x0, y0, x1, y1, x2, y2, x3, y3 = canvas.coords(polygon_id)\n",
    "    pt0, pt1, pt2, pt3 = [x0, y0], [x1, y1], [x2, y2], [x3, y3]\n",
    "    x_center, y_center = get_2center(*pt0, *pt2)\n",
    "    new_pt0 = rotate_point(*pt0, x_center, y_center, angle)\n",
    "    new_pt1 = rotate_point(*pt1, x_center, y_center, angle)\n",
    "    new_pt2 = rotate_point(*pt2, x_center, y_center, angle)\n",
    "    new_pt3 = rotate_point(*pt3, x_center, y_center, angle)\n",
    "    canvas.coords(polygon_id, *new_pt0, *new_pt1, *new_pt2, *new_pt3)\n",
    "\n",
    "def move_forward(polygon, distance, angle):\n",
    "    \"\"\"Move the given polygon forward by the given distance (in pixels) along the path it faces.\n",
    "\n",
    "  Args:\n",
    "      polygon: The polygon object.\n",
    "      distance: The distance to move the polygon (in pixels).\n",
    "      angle: The angle at which the polygon is facing (in degrees).\n",
    "\n",
    "  Returns:\n",
    "      A tuple representing the amount of pixels to move the polygon in the x and y directions.\n",
    "  \"\"\"\n",
    "    # Convert the angle to radians. \n",
    "    angle -= 90\n",
    "    angle_rad = angle * math.pi / 180\n",
    "\n",
    "    # Calculate the movement in the x and y directions.\n",
    "    dx = distance * math.cos(angle_rad)\n",
    "    dy = distance * math.sin(angle_rad)\n",
    "\n",
    "    return (dx, dy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH = 500        #si 15---8, 120 step----1\n",
    "HEIGHT = 500\n",
    "\n",
    "ROTATION = [-2, 0, +2] \n",
    "MOTION = [-1, 0, +1]\n",
    "SPEED = 2\n",
    "\n",
    "class VehicleSimulator:\n",
    "    def __init__(self):\n",
    "        self.window = tk.Tk()\n",
    "        self.window.title(\"Simulator\")\n",
    "        self.canvas = tk.Canvas(self.window, width=WIDTH, height=HEIGHT)\n",
    "        self.canvas.pack()\n",
    "        \n",
    "        # Draw rails\n",
    "        self.rails_thickness = 20\n",
    "        self.init_rails(HEIGHT/2, self.rails_thickness)\n",
    "        #self.init_rails(HEIGHT/3, self.rails_thickness)\n",
    "       \n",
    "        #horizontal road\n",
    "        self.canvas.create_line(0, 130, 150, 130, width=2)\n",
    "        self.canvas.create_line(0, 198, 150, 198, width=2)\n",
    "\n",
    "        # vertical road\n",
    "        #up\n",
    "        self.canvas.create_line(210, 120, 210, 0, width=2)\n",
    "        self.canvas.create_line(288, 120, 288, 0, width=2)\n",
    "\n",
    "        #down\n",
    "        self.canvas.create_line(210, 300, 210, 500, width=2)\n",
    "        self.canvas.create_line(288, 300, 288, 500, width=2)\n",
    "\n",
    "        #Draw transition zone\n",
    "        self.transition_zone()\n",
    "\n",
    "        #Draw vehicule\n",
    "        self.x = 248\n",
    "        self.y = HEIGHT-50\n",
    "        self.aligned=False\n",
    "        self.aligned_1=False\n",
    "        self.aligned_2=False\n",
    "        self.entered_zone=False\n",
    "        self.in_zone=False\n",
    "        self.motion = MOTION[2]\n",
    "        self.rotation = ROTATION[1]\n",
    "        self.vehicle_angle = 0\n",
    "        self.v_width, self.v_height = 40, 70\n",
    "        self.vehicule(self.x,self.y,self.v_width, self.v_height)\n",
    "\n",
    "        # Create buttons on grid(frame)\n",
    "        self.frame = tk.Frame(self.window)\n",
    "        self.frame.pack()\n",
    "\n",
    "        label = tk.Label(self.frame, text=\"MOTION\")\n",
    "        label.grid(row=0, column=0)\n",
    "        button1 = tk.Button(self.frame, text=\"backward\", command=lambda: self.button_motion(0))\n",
    "        button2 = tk.Button(self.frame, text=\"still\", command=lambda: self.button_motion(1))\n",
    "        self.button_f = tk.Button(self.frame, text=\"forward\", command=lambda: self.button_motion(2))\n",
    "        \n",
    "        button1.grid(row=0, column=1)\n",
    "        button2.grid(row=0, column=2)\n",
    "        self.button_f.grid(row=0, column=3)\n",
    "\n",
    "        label = tk.Label(self.frame, text=\"ROTATION\")\n",
    "        label.grid(row=1, column=0)\n",
    "        self.button_l = tk.Button(self.frame, text=\"left\", command=lambda: self.button_rotation(0))\n",
    "        button5 = tk.Button(self.frame, text=\"still\", command=lambda: self.button_rotation(1))\n",
    "        self.button_r = tk.Button(self.frame, text=\"right\", command=lambda: self.button_rotation(2))\n",
    "        self.button_l.grid(row=1, column=1)\n",
    "        button5.grid(row=1, column=2)\n",
    "        self.button_r.grid(row=1, column=3)\n",
    "\n",
    "        button_reset = tk.Button(self.frame, text=\"RESET\", command=self.reset)\n",
    "        button_reset.grid(row=0, column=4)\n",
    "        button_start = tk.Button(self.frame, text=\"TRAIN\", command=self.train)\n",
    "        button_start.grid(row=1, column=4)\n",
    "\n",
    "        # Text creation \n",
    "        self.text_x_y = self.canvas.create_text(400, 15, text=\"(x,y) = ({},{})\".format(round(self.x,1),round(self.y,1)), font=('Arial', 10))\n",
    "        self.text_vehicle_angle = self.canvas.create_text(400, 45, text=\"vehicle_angle = {}\".format(round(self.vehicle_angle,1)), font=('Arial', 10))\n",
    "        self.text_motion = self.canvas.create_text(400, 60, text=\"motion = {}\".format(round(self.motion,1)), font=('Arial', 10))\n",
    "\n",
    "    # Buttons functions\n",
    "    def button_motion(self, number):\n",
    "        if number==0:\n",
    "            self.motion = MOTION[0]\n",
    "        elif number==1:\n",
    "            self.motion = MOTION[1]\n",
    "        elif number==2:\n",
    "            self.motion = MOTION[2]  \n",
    "\n",
    "        self.rotation = ROTATION[1]\n",
    "        self.update_position()\n",
    "\n",
    "\n",
    "    def button_rotation(self, number):\n",
    "        if number==0:\n",
    "            self.rotation = ROTATION[0]\n",
    "        elif number==1:\n",
    "            self.rotation = ROTATION[1]\n",
    "        elif number==2:\n",
    "            self.rotation = ROTATION[2]\n",
    "\n",
    "        #self.motion = MOTION[1]\n",
    "        self.update_position()\n",
    "    \n",
    "    def sample(self):\n",
    "        l = [0,1,2]\n",
    "        return np.random.choice(l)\n",
    "        \n",
    "\n",
    "    # Draw roads function\n",
    "    def init_roads(self, width, thickness):\n",
    "        self.canvas.create_line(width-thickness, 0, width-thickness, HEIGHT, width=2)\n",
    "        self.canvas.create_line(width+thickness, 0, width+thickness, HEIGHT, width=2)\n",
    "\n",
    "    # Draw transition zone function\n",
    "    def transition_zone(self):\n",
    "        self.canvas.create_line(150, 120, 340, 120, width=1)\n",
    "        self.canvas.create_line(150, 300, 340, 300, width=1)\n",
    "        self.canvas.create_line(150, 120, 150, 300, width=1)\n",
    "        self.canvas.create_line(340, 120, 340, 300, width=1)\n",
    "        self.canvas.create_line(150, 210, 340, 210, width=1)\n",
    "\n",
    "    # Draw rails function   \n",
    "    def init_rails(self,height, thickness):\n",
    "        self.canvas.create_line(340, height-thickness, WIDTH, height-thickness, width=5)\n",
    "        self.canvas.create_line(340, height+thickness, WIDTH, height+thickness, width=5)\n",
    "\n",
    "    # Update position function\n",
    "    def update_position(self):\n",
    "        rotate(self.vehicle, self.rotation, self.canvas)\n",
    "        self.vehicle_angle += self.rotation\n",
    "        movement = move_forward(self.vehicle, self.motion*SPEED, self.vehicle_angle)\n",
    "        self.canvas.move(self.vehicle, movement[0], movement[1])\n",
    "        x0, y0, x1, y1, x2, y2, x3, y3 = self.canvas.coords(self.vehicle)\n",
    "        self.x, self.y = get_2center(x0, y0, x2, y2)\n",
    "        self.update_display()\n",
    "        \n",
    "\n",
    "    # Update text\n",
    "    def update_display(self):\n",
    "        self.canvas.itemconfig(self.text_x_y, text=\"(x,y) = ({},{})\".format(round(self.x,1),round(self.y,1)))\n",
    "        self.canvas.itemconfig(self.text_vehicle_angle, text=\"vehicle_angle = {}\".format(round(self.vehicle_angle,1)))\n",
    "        self.canvas.itemconfig(self.text_motion, text=\"motion = {}\".format(round(self.motion,1)))\n",
    "    \n",
    "    #create vehicule\n",
    "    def vehicule(self,x,y,v_width,v_height):\n",
    "        self.vehicle = self.canvas.create_polygon(*get_4points(x,y,v_width,v_height), fill=\"green\")\n",
    "    \n",
    "    # Reset the vehicle\n",
    "    def reset(self):\n",
    "        self.x = 248\n",
    "        self.y = HEIGHT-150\n",
    "        self.aligned_1=False\n",
    "        self.aligned_2=False\n",
    "        self.entered_zone=False\n",
    "        self.motion = MOTION[2]\n",
    "        self.rotation = ROTATION[1]\n",
    "        self.vehicle_angle = 0\n",
    "        self.v_width, self.v_height = 40, 70\n",
    "        self.canvas.delete(self.vehicle)\n",
    "        self.vehicule(self.x,self.y,self.v_width, self.v_height)\n",
    "        self.update_display()\n",
    "     \n",
    "        return [self.x,self.y]\n",
    "    \n",
    "    def step(self,rotation):\n",
    "        done = False\n",
    "        \n",
    "        reward=-0.1\n",
    "        old_angle=self.vehicle_angle\n",
    "        #mis à jour de position\n",
    "        if rotation == 0:\n",
    "            self.button_l.invoke()\n",
    "        elif rotation == 1:\n",
    "            self.button_f.invoke()\n",
    "        else:\n",
    "            self.button_r.invoke()\n",
    "       \n",
    "        x0, y0, x1, y1, x2, y2, x3, y3 = self.canvas.coords(self.vehicle)\n",
    "        \n",
    "        print(x0, y0, x1, y1, x2, y2, x3, y3)\n",
    "\n",
    "        if y0<300 and y1<300 and self.entered_zone==False:\n",
    "            self.entered_zone=True\n",
    "            reward=10\n",
    "            \n",
    "        \n",
    "        # ateinte de la plateforme (4 coté dedans)\n",
    "        # methode : au dessus de 300pixel et in_zone==False\n",
    "        if y0<300 and y1<300  and y2<300 and y3<300 and self.in_zone==False:\n",
    "            self.in_zone=True\n",
    "        \n",
    "        if y0>300 and y1>300  and y2>300 and y3>300 and self.in_zone==True:\n",
    "            self.in_zone=False\n",
    "            \n",
    "            \n",
    "        # sortir de la route principale (amélioration possible)\n",
    "        if (x0<210 or x1>288) and (y0>300 or y1>300) :\n",
    "            reward=-100\n",
    "            done = True\n",
    "        \n",
    "        \n",
    "        # \n",
    "        if self.entered_zone==True and 0<old_angle<90 and 0<self.vehicle_angle<=90 and self.vehicle_angle>old_angle:\n",
    "            reward=1\n",
    "        '''if (x0-x3 !=0)  and (y0-y3)/(x0-x3)== -2 and self.aligned_1 == False :\n",
    "            self.aligned_1 = True\n",
    "            reward = 5\n",
    "            \n",
    "        if (x0-x3 !=0) and (y0-y3)/(x0-x3)== -1/2 and self.aligned_2 == False :\n",
    "            self.aligned_2 = True \n",
    "            reward = 10'''\n",
    "        #alignement avel les rails\n",
    "        if  abs(230-y0)<=3 and abs(y1-270)<=3 and abs(y3-230)<=3 and abs(y2-270)<=3 and self.aligned==False:\n",
    "            reward=2\n",
    "        if  abs(230-y0)<=1 and abs(y1-270)<=1 and abs(y3-230)<=1 and abs(y2-270)<=1 and self.aligned==False:\n",
    "            self.aligned = True\n",
    "            reward=50\n",
    "            if self.vehicle_angle==90:\n",
    "                reward=70\n",
    "        \n",
    "        \n",
    "        # se désaligner\n",
    "        '''if self.aligned and rotation!=1:\n",
    "            reward=-10\n",
    "            self.aligned=False'''\n",
    "            \n",
    "        \n",
    "        #Punition à la sortie de la zone de transition\n",
    "        if self.in_zone and ((y0<210 or y1<210 or y2<210 or y3<210) or (x0<150 or x1<150 or x2<150 or x3<150) or (y0>300 or y1>300 or y2>300 or y3>300) or ( self.aligned==False and (x0>340 or x1>340 or x2>340 or x3>340))):\n",
    "            reward=-100\n",
    "            done=True\n",
    "            \n",
    "\n",
    "        #success\n",
    "        if self.aligned==True and self.x>=280:\n",
    "            reward=150\n",
    "            done=True\n",
    "        # Angle  reset\n",
    "        self.vehicle_angle %= 360\n",
    "        #time.sleep(0.001)\n",
    "        \n",
    "       \n",
    "        \n",
    "        return (self.x,self.y),reward,done\n",
    "    \n",
    "    def train(self):\n",
    "        for episode in range(episodes):\n",
    "             # On affiche l'épisode en cours\n",
    "            print(\"\\033[1m\" + \"\\033[94m\" + \"###\"*13 + \" Episode {} / {} \".format(episode+1, episodes) + \"###\"*18 + \"\\033[00m\" + \"\\033[0;0m\")\n",
    "\n",
    "            # A chaque épisode, on ré-initialise l'état de l'environnement\n",
    "            state = env.reset()\n",
    "\n",
    "            # La conséquence de l'initialisation est que le jeu ne peut pas être terminé, donc done=False    \n",
    "            done = False\n",
    "\n",
    "            # On initialise à 0 la variable steps qui est le compteur du nombre d'étapes (et donc d'actions) réalisés\n",
    "            # au cours d'un épisode\n",
    "            episode_steps = 0\n",
    "\n",
    "            # On initialise à 0 la durée de l'épisode\n",
    "            episode_duration = 0\n",
    "\n",
    "            # On initialise la récompense cumulée sur un episode\n",
    "            total_reward = 0\n",
    "\n",
    "            # On initialise le nombre d'atterissages réussis\n",
    "            sucessfull_Enrails = 0\n",
    "\n",
    "            # On calcule le nombre de steps maximum à réaliser pour l'épisode en cours\n",
    "            max_steps = get_max_steps(episode)\n",
    "\n",
    "\n",
    "            # On initialise les bars de progression\n",
    "            episode_bar = progressbar.ProgressBar()\n",
    "            episode_bar.start(max_value=get_max_steps(episode+1))\n",
    "\n",
    "\n",
    "            # On lance le chronomètre pour cet épisode\n",
    "            t0 = time.time()\n",
    "\n",
    "            # On commence par itérer tant que l'épisode n'est pas terminé\n",
    "            while not done:\n",
    "                # L'agent choisit l'action à entreprendre \n",
    "                action = agent.choose_action(state)\n",
    "                \n",
    "                # On applique l'action à l'environnement et on récupère les informations de transition\n",
    "                next_state, reward, done = env.step(action)\n",
    "                \n",
    "                # On rafraichit l'affichage de l'environnement\n",
    "                env.window.update()\n",
    "                \n",
    "                # On stocke les informations de transition (S, A, R, S', D) dans le replay_buffer\n",
    "                agent.store_transition(state,action,reward,next_state,done)\n",
    "                \n",
    "                # Une fois les informations stockées, on entraine l'agent sur un échantillon aléatoire du replay_buffer\n",
    "                agent.train_on_batch()\n",
    "                \n",
    "                # Chaque 'agent.update_rate' steps, on transfère les poids du modèle policy vers le modèle target\n",
    "                if overall_steps % agent.update_rate == 0:\n",
    "                    agent.update_target_model()\n",
    "                    \n",
    "                # On réduit la valeur epsilon le facteur d'exploration afin d'explorer plus au début et exploiter plus à la fin\n",
    "                agent.perform_epsilon_decay()\n",
    "                \n",
    "                # On incrémente la récompense cumulée sur un épisode \n",
    "                total_reward += reward\n",
    "                \n",
    "                # On passe à l'état suivant et on continue d'itérer dans la boucle de l'épisode\n",
    "                state = next_state\n",
    "                \n",
    "                # On incrémente le compteur de step\n",
    "                episode_steps += 1\n",
    "                \n",
    "                # Si la récompense immédiate reçue à la fin de l'épisode est +100, alors l'enraillement est réussi\n",
    "                if done and reward == 100:\n",
    "                    sucessfull_Enrails += 1\n",
    "                    \n",
    "                # On arrête le chronomètre lorsque l'épisode est terminé ou qu'on a dépassé le nombre max de steps\n",
    "                if episode_steps >= max_steps or done:\n",
    "                    episode_duration = time.time() - t0\n",
    "                    break\n",
    "\n",
    "            # On appelle la fonction display_metrics pour gérer l'affichage de la progression de l'entrainement\n",
    "            # On récupère la variable is_better pour savoir si l'agent a progressé\n",
    "            better = display_metrics(total_reward = total_reward, sucessfull_landings= sucessfull_Enrails, \n",
    "                                   episode_duration = episode_duration,episode_bar = episode_bar,current_step = episode_steps,\n",
    "                                   done = done,max_steps = max_steps)\n",
    "            # On enregistre le modèle sur le disque s'il y a eu amélioration de la récompense cumulée obtenue\n",
    "            if better: agent.model_policy.save('Model.h5')\n",
    "    \n",
    "env = VehicleSimulator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "\n",
    "    \"\"\"\n",
    "    Classe de l'agent DQN implémentant l'algorithme Deep Q-Network\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size):\n",
    "        \n",
    "        # L'attribut state_size représente la dimension de l'espace d'états, \n",
    "        # c'est-à-dire les dimensions de l'image de l'environnement de jeu\n",
    "        self.state_size = state_size\n",
    "        \n",
    "        # L'attribut action_size représente la dimension de l'espace d'actions\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # L'attribut render représente l'autorisation d'afficher le jeu pendant l'entrainement\n",
    "        self.render = True\n",
    "        \n",
    "        # Hyperparametres\n",
    "        \n",
    "        # L'attribut gamma représente le facteur de dépréciation dans le calcul de la récompense cumulée\n",
    "        self.gamma = 0.99\n",
    "        \n",
    "        # L'attribut epsilon représente le facteur d'exploration initial\n",
    "        self.epsilon = 1 \n",
    "        \n",
    "        # L'attribut lr représente le taux d'apprentissage du réseau de neurones DQN\n",
    "        self.lr = 0.0005 \n",
    "        \n",
    "        # L'attribut batch_size représente la taille de lots utilisée pendant l'entrainement\n",
    "        self.batch_size = 64\n",
    "        \n",
    "        # L'attribut epsilon_min représente le facteur d'exploration minimal \n",
    "        # en deça duquel l'agent ne peut jamais descendre pendant son entrainement\n",
    "        self.epsilon_min = 0.01\n",
    "        \n",
    "        # L'attribut epsilon_decay représente le facteur de décroissance qu'on applique à epsilon \n",
    "        # pour réduire le facteur d'exploration au cours de l'entrainement\n",
    "        self.epsilon_decay = 0.9995\n",
    "        \n",
    "        # L'attribut update_rate représente le nombre d'étapes au terme duquel on transfère \n",
    "        # les poids du réseau \"policy\" vers le réseau \"target\"\n",
    "        self.update_rate = 100\n",
    "        \n",
    "        # L'attribut replay_buffer_size représente la taille du buffer d'expériences utilisé pendant l'entrainement\n",
    "        self.replay_buffer_size = 15000\n",
    "        \n",
    "        # L'attribut replay_buffer_ùin_size représente la taille minimale du buffer d'expériences \n",
    "        # avant de pouvoir commencer à l'utiliser pendant l'entrainement\n",
    "        self.replay_buffer_min_size = 150\n",
    "        \n",
    "        # L'attribut mémory représente le buffer d'expérience. \n",
    "        # C'est un objet deque d'une profondeur de 5000 éléments maximum\n",
    "        self.replay_buffer = deque(maxlen=self.replay_buffer_size)\n",
    "        \n",
    "        self.model_policy = self.build_model()\n",
    "        self.model_target = self.build_model()\n",
    "        \n",
    "        self.update_target_model()\n",
    "    \n",
    "    #\n",
    "    # Méthode pour réduire graduellement la valeur du facteur d'exploration\n",
    "    #\n",
    "    def perform_epsilon_decay(self):\n",
    "        \n",
    "        '''\n",
    "        Méthode permettant de réduire la valeur du facteur d'exploration (epsilon)\n",
    "\n",
    "                Parameters:\n",
    "                        - self : l'instance de classe, permettant d'accéder à tous les attributs de la classe\n",
    "\n",
    "                Returns:\n",
    "                        Rien : la fonction réduit à chaque appel l'attribut epsilon d'un facteur de epsilon_decay\n",
    "                               sans jamais descendre en dessous de epsilon_min\n",
    "        '''\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "    #\n",
    "    # Méthode pour construire un modèle de réseau neuronal\n",
    "    #\n",
    "    def build_model(self):\n",
    "        \n",
    "        '''\n",
    "        Méthode permettant de définir et de compiler un modèle de réseau de neurones\n",
    "\n",
    "                Parameters:\n",
    "                        - self : l'instance de classe, permettant d'accéder à tous les attributs de la classe\n",
    "\n",
    "\n",
    "                Returns:\n",
    "                        model (Keras Model) : Modèle Keras représentant le modèle compilé défini \n",
    "                                              suivant une architecture\n",
    "        '''\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.Dense(128, activation='relu', input_shape=self.state_size))\n",
    "        model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(3, activation='linear'))\n",
    "\n",
    "        model.compile(optimizer=Adam(learning_rate=self.lr), loss='mean_squared_error', metrics=['accuracy'])\n",
    "        \n",
    "        return model\n",
    "            \n",
    " \n",
    "\n",
    "    #\n",
    "    # Méthode pour stocker des explériences (transitions) dans un buffer\n",
    "    #\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        '''\n",
    "        Méthode permettant rajouter une transition dans le buffer d'expérience\n",
    "\n",
    "                Parameters:\n",
    "                        - self : l'instance de classe, permettant d'accéder à tous les attributs de la classe\n",
    "                        - state (Numpy ndarray) : Matrice représentant l'état courant de l'environnement\n",
    "                        - action (int) : Nombre entier représentant l'action entreprise par l'agent\n",
    "                        - reward (float) : Nombre réel représentant la récompense immédiate obtenue de l'environnement\n",
    "                        - next_state (Numpy ndarray) : Matrice représentant l'état suivant de l'environnement\n",
    "\n",
    "\n",
    "                Returns:\n",
    "                        Rien : modification de l'attribut memory pour y rajouter une transition\n",
    "        '''\n",
    "        self.replay_buffer.append((state,action,reward,next_state,done))\n",
    "            \n",
    "\n",
    "        \n",
    "    #\n",
    "    # Méthode pour choisir l'action à entreprendre suivant une politique epsilon-greedy\n",
    "    #\n",
    "    def choose_action(self, state):\n",
    "        \n",
    "        '''\n",
    "        Méthode permettant de choisir l'action à entreprendre par l'action en suivant une politique epsilon-greedy\n",
    "\n",
    "                Parameters:\n",
    "                        - self : l'instance de classe, permettant d'accéder à tous les attributs de la classe\n",
    "                        - state (Numpy ndarray) : Matrice représentant l'état courant de l'environnement\n",
    "\n",
    "\n",
    "                Returns:\n",
    "                        - action (int) : Nombre entier représentant l'action à entreprendre par l'agent\n",
    "        '''\n",
    "        policy = self.model_policy\n",
    "        n = random.uniform(0, 1)\n",
    "        if n < self.epsilon:\n",
    "            action = env.sample()\n",
    "            return action\n",
    "        else:\n",
    "            state_array = tf.expand_dims(state, axis=0)\n",
    "            print(state_array)\n",
    "            temp = self.model_policy.predict(state_array)\n",
    "            return np.argmax(temp[0])\n",
    "        \n",
    "      \n",
    "    \n",
    "    #\n",
    "    # Méthode pour sélectionner des batchs de données dans le replay_buffer\n",
    "    #    \n",
    "    def sample_batches_from_buffer(self):\n",
    "        \n",
    "        '''\n",
    "        Méthode permettant d'échantillonner des vecteurs listes de valeurs à partir du buffer d'expérience\n",
    "\n",
    "                Parameters:\n",
    "                        - self : l'instance de classe, permettant d'accéder à tous les attributs de la classe\n",
    "\n",
    "\n",
    "                Returns:\n",
    "                        - states_list (List) : Liste Python contenant l'ensemble des 'batch_size' états de l'environnement\n",
    "                                          aléatoirement choisis dans le replay_buffer\n",
    "                        - actions_list (List) : Liste Python contenant l'ensemble des 'batch_size' actions de \n",
    "                                           l'environnement aléatoirement choisis dans le replay_buffer\n",
    "                        - rewards_list (List) : Liste Python contenant l'ensemble des 'batch_size' récompenses \n",
    "                                           de l'environnement aléatoirement choisis dans le replay_buffer\n",
    "                        - next_states_list (List) : Liste Python contenant l'ensemble des 'batch_size' états suivants \n",
    "                                               de l'environnement aléatoirement choisis dans le replay_buffer\n",
    "                        - dones_list (List) : Liste Python contenant l'ensemble des 'batch_size' done de \n",
    "                                              l'environnement aléatoirement choisis dans le replay_buffer\n",
    "        '''\n",
    "        \n",
    "        # Par exemple, pour batch_size = 4, on choisit un batch aléatoire de transitions\n",
    "        # batch = [(s1, a1, r1, s1', d1), (s2, a2, r2, s2', d2), (s3, a3, r3, s3', d3), (s4, a4, r4, s4', d4)]\n",
    "        # La fonction doit créer des batchs pour chaque variable (states, actions, rewards, next_states, dones)\n",
    "        # Par exemple :\n",
    "        # states      = [s1, s2, s3, s4]\n",
    "        # actions     = [a1, a2, a3, a4]\n",
    "        # rewards     = [r1, r2, r3, r4]\n",
    "        # next_states = [s1', s2', s3', s4']\n",
    "        # dones       = [d1, d2, d3, d4]\n",
    "        \n",
    "        indices = np.random.randint(0,len(self.replay_buffer), size=self.batch_size)\n",
    "   \n",
    "        batch = [self.replay_buffer[i] for i in indices]\n",
    "        list_index = []\n",
    "        states_list = []\n",
    "        actions_list = []\n",
    "        rewards_list = []\n",
    "        next_states_list = []\n",
    "        dones_list = []\n",
    "        \n",
    "        \n",
    "        # On boucle enfin sur la liste des transitions, et on stocke chaque élément dans la bonne liste\n",
    "        for temp in batch:\n",
    "            states_list.append(temp[0])\n",
    "            actions_list.append(temp[1])\n",
    "            rewards_list.append(temp[2])\n",
    "            next_states_list.append(temp[3])\n",
    "            dones_list.append(temp[4])\n",
    "\n",
    "        return states_list, actions_list, rewards_list, next_states_list, dones_list\n",
    "\n",
    "    \n",
    "\n",
    "    #\n",
    "    # Méthode pour calculer les valeur à cible (de la Q-value) à utiliser comme valeurs d'apprentissage des réseaux de neurones\n",
    "    #    \n",
    "    def compute_targets(self, states_list, actions_list, rewards_list, next_states_list, dones_list):\n",
    "        \n",
    "        '''\n",
    "        Méthode permettant de calculer les valeurs cibles des valeurs d'actions pour les batchs d'états dans actions_list\n",
    "        states_list\n",
    "\n",
    "                Parameters:\n",
    "                        - self : l'instance de classe, permettant d'accéder à tous les attributs de la classe\n",
    "                        - states_list (List) : Liste Python contenant l'ensemble des 'batch_size' états de l'environnement\n",
    "                                          aléatoirement choisis dans le replay_buffer\n",
    "                        - actions_list (List) : Liste Python contenant l'ensemble des 'batch_size' actions de \n",
    "                                           l'environnement aléatoirement choisis dans le replay_buffer\n",
    "                        - rewards_list (List) : Liste Python contenant l'ensemble des 'batch_size' récompenses \n",
    "                                           de l'environnement aléatoirement choisis dans le replay_buffer\n",
    "                        - next_states_list (List) : Liste Python contenant l'ensemble des 'batch_size' états suivants \n",
    "                                               de l'environnement aléatoirement choisis dans le replay_buffer\n",
    "                        - dones_list (List) : Liste Python contenant l'ensemble des 'batch_size' done de \n",
    "                                              l'environnement aléatoirement choisis dans le replay_buffer\n",
    "\n",
    "\n",
    "                Returns:\n",
    "                        - final_targets (List) : Liste Python contenant les 'batch_size' vecteurs contenant \n",
    "                                                       chacun les valeurs de chaque action\n",
    "        '''\n",
    "        \n",
    "        # Par exemple, pour un batch_size = 6, la liste des vecteurs de valeurs cibles pour les 6 états \n",
    "        # states = [s1, s2, s3, s4, s5, s6] choisis aléatoirement dans le replay_buffer aura cette forme \n",
    "        # [[0.1, 0.4, 0.1, 1.1], ==> vecteur des valeurs-cibles d'action pour s1\n",
    "        #  [4.1, 0.9, 0.3, 0.1], ==> vecteur des valeurs-cibles d'action pour s2\n",
    "        #  [0.8, 0.3, 9.8, 9.0], ==> vecteur des valeurs-cibles d'action pour s3\n",
    "        #  [0.1, 0.2, 7.3, 7.9], ==> vecteur des valeurs-cibles d'action pour s4\n",
    "        #  [6.3, 0.1, 2.0, 5.4], ==> vecteur des valeurs-cibles d'action pour s5\n",
    "        #  [3.9, 0.1, 0.3, 5.3]] ==> vecteur des valeurs-cibles d'action pour s6\n",
    "        \n",
    "        #np.squeeze est utilisé ici pour réduire la dimension du np.array\n",
    "        # (batch_size, 8, 1) --> (batch_size, 8)\n",
    "        # Cf https://numpy.org/doc/stable/reference/generated/numpy.squeeze.html   \n",
    "        \n",
    "        states_list = np.squeeze(np.array(states_list))\n",
    "        next_states_list = np.squeeze(np.array(next_states_list))\n",
    "        \n",
    "        current_val = self.model_policy.predict_on_batch(states_list)\n",
    "        final_val = current_val \n",
    "        \n",
    "        future_val = self.model_policy.predict_on_batch(next_states_list)\n",
    "        real_targets = np.array(rewards_list)+ self.gamma * (np.amax(np.array(future_val),axis=1)) * (1 - np.array(dones_list))\n",
    "       \n",
    "        final_val[ [i for i in range(self.batch_size)] , [np.array(actions_list)] ] = real_targets\n",
    "        \n",
    "        return final_val\n",
    "\n",
    "\n",
    "    #\n",
    "    # Méthode pour entrainer le modèle neuronal en utilisant des échantillons de données dans le buffer d'expériences\n",
    "    #\n",
    "    def train_on_batch(self):\n",
    "        \n",
    "        '''\n",
    "        Méthode permettant d'entrainer le modèle neuronal policy \n",
    "\n",
    "                Parameters:\n",
    "                        - self : l'instance de classe, permettant d'accéder à tous les attributs de la classe\n",
    "\n",
    "\n",
    "                Returns:\n",
    "                        - Rien : la fonction échantillonne les données du replay_buffer\n",
    "                                 calcule les target et entraine le réseau \"policy\"\n",
    "        '''\n",
    "        \n",
    "        # On vérifie la taille du replay_buffer : s'il n'y a pas assez d'éléments dans le replay_buffer,\n",
    "        # on ne fait rien\n",
    "        if len(self.replay_buffer) < self.replay_buffer_min_size:\n",
    "            return \n",
    "        else:\n",
    "            states_list, actions_list, rewards_list, next_states_list, dones_list = self.sample_batches_from_buffer();\n",
    "            final_targets = self.compute_targets(states_list, actions_list, rewards_list, next_states_list, dones_list)\n",
    "            self.model_policy.fit(np.squeeze(np.array(states_list)),np.array(final_targets))\n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "    #\n",
    "    # Méthode pour de mettre à jour les poids du réseau \"target\" à partir du réseau local\n",
    "    #\n",
    "    def update_target_model(self):\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        Méthode permettant  de de mettre à jour les poids du réseau \"target\" à partir du réseau local\n",
    "\n",
    "                Parameters:\n",
    "                        - self : l'instance de classe, permettant d'accéder à tous les attributs de la classe\n",
    "\n",
    "\n",
    "                Returns:\n",
    "                        - Rien : transfère les poids de model dans target_model\n",
    "        '''\n",
    "        \n",
    "        wt = self.model_policy.get_weights()\n",
    "        self.model_target.set_weights(wt)\n",
    "        \n",
    "\n",
    "                        \n",
    "                    \n",
    "    #\n",
    "    # Méthode pour charger un fichier de poids dans le modèle local\n",
    "    #\n",
    "    def load(self, name):\n",
    "        \n",
    "        '''\n",
    "        Méthode permettant de charger un fichier de poids dans le modèle local\n",
    "\n",
    "                Parameters:\n",
    "                        - self : l'instance de classe, permettant d'accéder à tous les attributs de la classe\n",
    "\n",
    "\n",
    "                Returns:\n",
    "                        - Rien : Charge les poids d'un fichier de poids stocké au chemin d'accès \"name\" dans model \n",
    "        '''\n",
    "        \n",
    "        print(\"[INFO] : Loading model from disk at \", name, \"\\n\")     \n",
    "        self.policy_model.load_weights(name)\n",
    "\n",
    "    #\n",
    "    # Méthode pour enregistrer les poids de model dans un fichier de poids\n",
    "    #\n",
    "    def save(self, name):\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        Méthode permettant d'enregistrer les poids de model dans un fichier de poids\n",
    "\n",
    "                Parameters:\n",
    "                        - self : l'instance de classe, permettant d'accéder à tous les attributs de la classe\n",
    "\n",
    "\n",
    "                Returns:\n",
    "                        - Rien : Enregistre les paramètres de model dans un fichier de poids stocké au chemin d'accès \"name\"\n",
    "        '''\n",
    "        \n",
    "        print(\"[INFO] : Saving model to disk at \", name, \"\\n\")\n",
    "        self.policy_model.save_weights(name)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensions d'un état environnement après redimensionnement\n",
    "state_size = (2,)\n",
    "\n",
    "# Nombre d'actions possibles dans l'environnement\n",
    "action_size = 3\n",
    "\n",
    "# On instancie notre agent DQN\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "# Nombre d'épisodes pendant lequel il faut entrainer\n",
    "episodes = 1000\n",
    "\n",
    "# Batch size pour échantillonner les transitions\n",
    "batch_size = 32 \n",
    "\n",
    "# Compteur du nombre de timestep\n",
    "overall_steps = 0  \n",
    "\n",
    "# Liste d'enregistrement des récompenses cumulées à chaque épisode\n",
    "episodes_rewards_list = list()\n",
    "\n",
    "# Liste d'enregistrement des moyennes de récompenses cumulées sur les 25 derniers épisodes\n",
    "last25_episodes_rewards_list = list()\n",
    "\n",
    "# Liste d'enregistrement des moyennes de récompenses cumulées sur les 50 derniers épisodes\n",
    "last50_episodes_rewards_list = list()\n",
    "\n",
    "# Liste d'enregistrement des moyennes de récompenses cumulées sur les 100 derniers épisodes\n",
    "last100_episodes_rewards_list = list()\n",
    "\n",
    "# Liste d'enregistrement des atterissages réussis\n",
    "sucessfull_landings_list = list()\n",
    "\n",
    "# Liste d'enregistrement des durées de chaque épisode\n",
    "episode_duration_list = list()\n",
    "\n",
    "# Intervalles d'épisodes avec le nombre de steps maximum. Cela définit une stratégie d'entrainement\n",
    "# Ici STRATEGY_DICT permet d'appliquer la stratégie suivante\n",
    "# Pour les 150 premiers épisodes, l'agent ne doit pas éffectuer plus de 300 steps\n",
    "# Pour les épisodes entre 150 et 400, l'agent ne doit pas effectuer plus de 500 steps\n",
    "# Au delà de 500 épisodes, l'agent ne doit pas effectuer plus de 700 épisodes\n",
    "# Cela permet d'avoir un entrainement plus rapide, et un apprentissage par pallier.\n",
    "STRATEGY_DICT = {\"0-150\":300, \"150-400\":500, \"400-{}\".format(episodes+1):700}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_steps(episode_number):\n",
    "    \n",
    "    '''\n",
    "        Fonction permettant de retourner le nombre maximum de steps à effectuer sur l'épisode en cours\n",
    "\n",
    "                Parameters:\n",
    "                       \n",
    "                        - episode_number (Int) : Nombre entier représentant le numéro de l'épisode actuel\n",
    "                        \n",
    "                Returns:\n",
    "                        - steps_max (Int) : Nombre entier représentant le nombre maximum de steps \n",
    "                                            à effectuer sur l'épisode en cours\n",
    "    '''\n",
    "    \n",
    "    for interval_str, steps_max in STRATEGY_DICT.items():\n",
    "        low_interval = int(interval_str.split(\"-\")[0])\n",
    "        high_interval = int(interval_str.split(\"-\")[1])\n",
    "        \n",
    "        if episode_number < high_interval and episode_number >= low_interval:\n",
    "            \n",
    "            return steps_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_metrics(total_reward, \n",
    "                    sucessfull_landings, \n",
    "                    episode_duration, \n",
    "                    episode_bar, \n",
    "                    current_step, \n",
    "                    done, \n",
    "                    max_steps,\n",
    "                    display_frequency = 10):\n",
    "    \n",
    "    '''\n",
    "        Fonction permettant d'afficher la progression de l'entrainement à chaque action réalisée, d'afficher \n",
    "        un récapitulatif des performances à chaque fin d'épisode, et d'afficher des graphiques d'entrainement\n",
    "        à intervalle régulier\n",
    "\n",
    "                Parameters:\n",
    "                       \n",
    "                        - total_reward (Float) : Nombre réel représentant la récompense cumulée obtenue à la fin \n",
    "                                                 d'un épisode.\n",
    "                        - sucessfull_landings (Int) : Nombre entier représentant, à la fin d'un épisode, le nombre\n",
    "                                                      d'atterrissage réussis par l'agent depuis le début de \n",
    "                                                      l'entrainement \n",
    "                        - episode_duration (Float) : Nombre réel représentant la durée en secondes de l'épisode\n",
    "                        - episode_bar (ProgressBar()) : Objet ProgressBar() représentant un widget destiné à \n",
    "                                                        afficher la progression de l'entrainement\n",
    "                        - current_step (Int) : Nombre entier représentant le nombre de steps réalisés à \n",
    "                                               un instant donné d'un épisode.\n",
    "                        - done (Booléen) : Booléen représentant si l'épisode est terminé ou non\n",
    "                        - episode_number (Int) : Nombre entier représentant le numéro de l'épisode actuel\n",
    "                        - max_steps (Int) : Nombre entier représentant le nombre maximum de steps autorisés\n",
    "                                            pour l'épisode en cours\n",
    "                        - display_frequency (Int) : Nombre entier représentant la fréquence (en nombre d'épisodes)\n",
    "                                                    à laquelle afficher les courbes d'apprentissages de l'agent\n",
    "\n",
    "                        \n",
    "                Returns:\n",
    "                        - is_better (Booléen) : Booléen représentant l'agent a progressé ou non entre les derniers\n",
    "                                                épisodes et l'épisode en cours. L'agent a progressé s'il a augmenté\n",
    "                                                sa récompense cumulée.\n",
    "    '''\n",
    "    \n",
    "    # On commence par initialiser is_better à False. \n",
    "    # On le passera dans la suite à True si les conditions sont réunies\n",
    "    is_better = False\n",
    "    \n",
    "    # Premier cas de traitement : l'épisode est terminé ou l'agent à excéder le nombre maximum de steps autorisés\n",
    "    if done or current_step >= max_steps:\n",
    "        \n",
    "        # On initialise toutes les variables ... Sinon Python ne sera pas content !\n",
    "        best_id_before = best_reward_before = best_id_after = best_reward_after = 0\n",
    "        \n",
    "        # On commence par déterminer la valeur maximum de récompense cumulée des derniers épisodes ainsi que l'épisode\n",
    "        # à laquelle cette valeur maximum a été obtenue. Il faut s'assurer que la liste des récompenses cumulées\n",
    "        # n'est pas vide. Sinon Python ne sera pas content !\n",
    "        if len(episodes_rewards_list) > 0:\n",
    "            # La fonction argsort() permet de trouver une liste ordonnée des indices des valeurs les plus\n",
    "            # élevées d'une liste\n",
    "            # Par exemple, [0.65, -9.6, 808.12, 11.1, 0.0078] ==> [2, 3, 0, 4, 1]\n",
    "            best_id_before = np.array(episodes_rewards_list).argsort()[-1]\n",
    "            best_reward_before = np.array(episodes_rewards_list)[best_id_before]\n",
    "        \n",
    "        # On rajoute la dernière récompense cumulée à la liste d'enregistrement des récompenses cumulées\n",
    "        episodes_rewards_list.append(total_reward)\n",
    "        \n",
    "        # On rajoute la dernière valeur des atterrissages réussis à la liste d'enregistrement \n",
    "        # des atterrissages réussis\n",
    "        sucessfull_landings_list.append(sucessfull_landings)\n",
    "        \n",
    "        #  On rajoute la durée du dernier épisode à la liste d'enregistrement des durées\n",
    "        episode_duration_list.append(episode_duration)\n",
    "        \n",
    "        # On calcule ensuite la moyenne glissante des récompenses cumulées sur les 25, 50 et 100 derniers épisodes    \n",
    "        last25_episodes_rewards_list.append(np.mean(episodes_rewards_list[-25:]))\n",
    "        last50_episodes_rewards_list.append(np.mean(episodes_rewards_list[-50:]))\n",
    "        last100_episodes_rewards_list.append(np.mean(episodes_rewards_list[-100:]))\n",
    "        \n",
    "        # On détermine la valeur maximum de récompense cumulée après avoir rajouté la dernière récompense cumulée\n",
    "        # ainsi que l'épisode à laquelle cette valeur maximum a été obtenue. \n",
    "        # Il faut s'assurer que la liste des récompenses cumulées n'est pas vide.  Sinon Python ne sera pas content !    \n",
    "        if len(episodes_rewards_list) > 0:\n",
    "            # On réutilise la fonction argsort()\n",
    "            best_id_after = np.array(episodes_rewards_list).argsort()[-1]\n",
    "            best_reward_after = np.array(episodes_rewards_list)[best_id_after]\n",
    "        \n",
    "        # Si la récompense cumulée a augmenté après le rajout de la valeur du dernier épisode, alors \n",
    "        # l'agent s'est amélioré. \n",
    "        if best_reward_after > best_reward_before :\n",
    "            # On affiche cette amélioration\n",
    "            print(\"\\033[1m\" + '\\033[92m' + \"[INFO] : Récompense obtenue améliorée de {:.1f} (épisode {}) à {:.1f}\\n\".\n",
    "                  format(best_reward_before, best_id_before+1, best_reward_after) + \"\\033[00m\"+ \"\\033[0;0m\")\n",
    "            \n",
    "            # Et on passe la variable is_better à True\n",
    "            is_better = True\n",
    "        \n",
    "                \n",
    "        # Si la récompense cumulée n'a pas augmenté, on rappelle juste la valeur maximale obtenue jusqu'ici\n",
    "        # et l'épisode auquel cette valeur maximale a été obtenue\n",
    "        else:\n",
    "            print(\"[INFO] : Récompense maximale obtenue jusqu'ici : {:.1f} (épisode {})\\n\".\n",
    "                  format(best_reward_before, best_id_before+1))\n",
    "        \n",
    "        # Dans cette partie, on va afficher à la fin d'un épisode des statistiques sur la performance de l'agent \n",
    "        # On utilise un objet Texttable de largeur maximum 100 pixels\n",
    "        t = Texttable(max_width=100)\n",
    "        \n",
    "        # On centre l'ensemble des colonnes avec l'argument \"c\"\n",
    "        t.set_cols_align([\"c\", \"c\", \"c\", \"c\", \"c\", \"c\", \"c\"])\n",
    "        \n",
    "        # On rajoute et on affiche les statistiques\n",
    "        t.add_rows([['Récompense', 'Steps', \"Durée de l'épisode\",'Récompense moy. [100]', 'Récompense moy. [50]', 'Récompense moy. [25]', 'Atterissage'], \n",
    "                    [\"{:.1f}\".format(total_reward), current_step, \n",
    "                     \"{:.1f} secondes\".format(episode_duration_list[-1]),\n",
    "                     \"{:.1f}\".format(last100_episodes_rewards_list[-1]), \n",
    "                     \"{:.1f}\".format(last50_episodes_rewards_list[-1]), \n",
    "                     \"{:.1f}\".format(last25_episodes_rewards_list[-1]), \n",
    "                     np.sum(sucessfull_landings_list)]])\n",
    "        print(t.draw(), \"\\n\")\n",
    "\n",
    "        \n",
    "        # A la fin de l'épisode, on mets la barre de progression à 100% (max_value)\n",
    "        episode_bar.update(int(episode_bar.max_value))\n",
    "        episode_bar.finish()\n",
    "        \n",
    "        # Si on atteint la fréquence d'affichage des courbes d'apprentissage\n",
    "        if len(episodes_rewards_list)%display_frequency == 0:\n",
    "            \n",
    "            # On divise le graphique en 4 sous-graphiques pour afficher 4 variables        \n",
    "            fig, axs = plt.subplots(4, figsize=(12, 12))\n",
    "            \n",
    "            # On donne un nom au graphique principal\n",
    "            fig.suptitle(\"Evolution de performances à l'épisode {}\".format(len(episodes_rewards_list)))\n",
    "            \n",
    "            ######## On s'occupe ici du 1er sous-graphique représenté par la variable axs[0]: \n",
    "            ######## afficher l'évolution de la récompense cumulée au cours de l'entrainement\n",
    "            ######## La méthode plot() va afficher un graphe \"ligne\" par défaut\n",
    "            axs[0].plot(np.array(range(len(episodes_rewards_list)))+1, np.array(episodes_rewards_list))\n",
    "            \n",
    "            # On va tracer une ligne verticale sur le sous-graphique ax[0] qui va représenté la valeur maximale\n",
    "            # obtenue jusqu'ici pour la récompense cumulée\n",
    "            \n",
    "            # Pour ce faire, on va déterminer la limite haute et la limite basse de cette ligne\n",
    "            ymin,ymax = axs[0].get_ylim()\n",
    "            \n",
    "            # On détermine aussi la position en x, c'est-à-dire l'épisode auquel cette valeur a été obtenue\n",
    "            x_line = np.max([best_id_before, best_id_after])+1\n",
    "            \n",
    "            # On dessine enfin la ligne verticale avec les éléments calculés précédemment\n",
    "            axs[0].vlines(x=x_line, ymin=ymin, ymax=ymax, colors='red', label=\"Meilleure valeur\")\n",
    "            \n",
    "            # On affiche une légende pour une meilleure interprétabilité du graphique\n",
    "            axs[0].legend(loc='best')\n",
    "            \n",
    "            # On affiche le titre du sous-graphique pour une meilleure interprétabilité également\n",
    "            axs[0].title.set_text(\"Récompense cumulée\")\n",
    "            \n",
    "            # Enfin, on affiche une étiquette pour indiquer à l'écrit la valeur maximale et l'épisode\n",
    "            axs[0].text(x_line-1, \n",
    "                        ymin+1, \n",
    "                        \"Valeur max = {:.1f} | Episode = {}\".format(episodes_rewards_list[x_line-1], x_line),\n",
    "                        fontsize=10,\n",
    "                        bbox = dict(facecolor=\"wheat\", alpha=0.7))\n",
    "            \n",
    "            \n",
    "            ######## On s'occupe ici du 1er sous-graphique représenté par la variable axs[1]: \n",
    "            ######## afficher l'évolution de la moyenne des 25 dernières récompenses cumulées \n",
    "            ######## La méthode plot() va afficher un graphe \"ligne\" par défaut\n",
    "            axs[1].plot(np.array(range(len(last25_episodes_rewards_list)))+1, np.array(last25_episodes_rewards_list), 'o')\n",
    "            \n",
    "            # On va tracer une ligne verticale sur le sous-graphique ax[1] qui va représenté la valeur maximale\n",
    "            # obtenue jusqu'ici pour la moyenne des 25 dernières récompenses cumulées\n",
    "            \n",
    "            # Pour ce faire, on va déterminer la limite haute et la limite basse de cette ligne\n",
    "            ymin,ymax = axs[1].get_ylim()\n",
    "            \n",
    "            # On détermine aussi la position en x, c'est-à-dire l'épisode auquel cette valeur a été obtenue\n",
    "            x_line = np.argmax(np.array(last25_episodes_rewards_list))+1\n",
    "            \n",
    "            # On dessine enfin la ligne verticale avec les éléments calculés précédemment\n",
    "            axs[1].vlines(x=x_line, ymin=ymin, ymax=ymax, colors='red', label=\"Meilleure valeur\")\n",
    "            \n",
    "            # On affiche une légende pour une meilleure interprétabilité du graphique\n",
    "            axs[1].legend(loc='best')\n",
    "            \n",
    "            # On affiche le titre du sous-graphique pour une meilleure interprétabilité également\n",
    "            axs[1].title.set_text(\"Récompense moyenne des 25 derniers épisodes\")\n",
    "            \n",
    "            # Enfin, on affiche une étiquette pour indiquer à l'écrit la valeur maximale et l'épisode\n",
    "            axs[1].text(x_line-1, \n",
    "                        ymin+1, \n",
    "                        \"Valeur max = {:.1f} | Episode = {}\".format(last25_episodes_rewards_list[x_line-1], x_line),\n",
    "                        fontsize=10,\n",
    "                        bbox = dict(facecolor=\"wheat\", alpha=0.7))\n",
    "\n",
    "            ######## On s'occupe ici du 1er sous-graphique représenté par la variable axs[2]: \n",
    "            ######## afficher l'évolution de la moyenne des 100 dernières récompenses cumulées \n",
    "            ######## La méthode plot() va afficher un graphe \"ligne\" par défaut\n",
    "            axs[2].plot(np.array(range(len(last100_episodes_rewards_list)))+1, np.array(last100_episodes_rewards_list), '+')\n",
    "            \n",
    "            # On va tracer une ligne verticale sur le sous-graphique ax[2] qui va représenté la valeur maximale\n",
    "            # obtenue jusqu'ici pour la moyenne des 100 dernières récompenses cumulées\n",
    "            \n",
    "            # Pour ce faire, on va déterminer la limite haute et la limite basse de cette ligne\n",
    "            ymin,ymax = axs[2].get_ylim()\n",
    "            \n",
    "            # On détermine aussi la position en x, c'est-à-dire l'épisode auquel cette valeur a été obtenue\n",
    "            x_line = np.argmax(np.array(last100_episodes_rewards_list))+1\n",
    "            \n",
    "            # On dessine enfin la ligne verticale avec les éléments calculés précédemment\n",
    "            axs[2].vlines(x=x_line, ymin=ymin, ymax=ymax, colors='red', label=\"Meilleure valeur\")\n",
    "            \n",
    "            # On affiche une légende pour une meilleure interprétabilité du graphique\n",
    "            axs[2].legend(loc='best')\n",
    "            \n",
    "            # On affiche le titre du sous-graphique pour une meilleure interprétabilité également\n",
    "            axs[2].title.set_text(\"Récompense moyenne des 100 derniers épisodes\")\n",
    "            \n",
    "            # Enfin, on affiche une étiquette pour indiquer à l'écrit la valeur maximale et l'épisode\n",
    "            axs[2].text(x_line-1, \n",
    "                        ymin+1, \n",
    "                        \"Valeur max = {:.1f} | Episode = {}\".format(last100_episodes_rewards_list[x_line-1], x_line),\n",
    "                        fontsize=10,\n",
    "                        bbox = dict(facecolor=\"wheat\", alpha=0.7))\n",
    "\n",
    "\n",
    "            ######## On s'occupe ici du 1er sous-graphique représenté par la variable axs[2]: \n",
    "            ######## afficher l'évolution du nombre d'atterrissages réussis \n",
    "            ######## La méthode plot() va afficher un graphe \"ligne\" par défaut\n",
    "            cumsum_landings = np.cumsum(sucessfull_landings_list) \n",
    "            axs[3].plot(np.array(range(len(cumsum_landings)))+1, cumsum_landings)\n",
    "            \n",
    "            # On va tracer une ligne verticale sur le sous-graphique ax[3] qui va représenté la valeur maximale\n",
    "            # obtenue jusqu'ici pour la moyenne des 100 dernières récompenses cumulées\n",
    "            \n",
    "            # Pour ce faire, on va déterminer la limite haute et la limite basse de cette ligne\n",
    "            ymin,ymax = axs[3].get_ylim()\n",
    "            \n",
    "            # On détermine aussi la position en x, c'est-à-dire l'épisode auquel cette valeur a été obtenue\n",
    "            x_line = np.argmax(cumsum_landings)+1\n",
    "            \n",
    "            # On dessine enfin la ligne verticale avec les éléments calculés précédemment\n",
    "            axs[3].vlines(x=x_line, ymin=ymin, ymax=ymax, colors='red', label=\"Meilleure valeur\")\n",
    "            \n",
    "            # On affiche une légende pour une meilleure interprétabilité du graphique\n",
    "            axs[3].legend(loc='best')\n",
    "            \n",
    "            # On affiche le titre du sous-graphique pour une meilleure interprétabilité également\n",
    "            axs[3].title.set_text(\"Nombre d'atterrissages réussis depuis le début\")\n",
    "            \n",
    "            # Enfin, on affiche une étiquette pour indiquer à l'écrit la valeur maximale et l'épisode\n",
    "            axs[3].text(x_line-1, \n",
    "                        0, \n",
    "                        \"Valeur max = {:.1f} | Episode = {}\".format(cumsum_landings.tolist()[x_line-1], x_line),\n",
    "                        fontsize=10,\n",
    "                        bbox = dict(facecolor=\"wheat\", alpha=0.7))\n",
    "            \n",
    "            # On appelle enfin la méthode plot qui afficher afficher à l'ecran le graphique\n",
    "            plt.show()\n",
    "    \n",
    "    # Dans le cas où l'épisode n'est pas terminé\n",
    "    else:\n",
    "        # On incrémente juste la progressbar pour l'affichage\n",
    "        episode_bar.update(current_step)\n",
    "    \n",
    "    return is_better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% (0 of 300) |                        | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[94m####################################### Episode 1 / 1000 ######################################################\u001b[00m\u001b[0;0m\n",
      "226.72090208162552 313.7205293343435 266.69653516238935 312.32454946624347 269.13949993156444 382.28190735758017 229.16386685080062 383.6778872256802\n",
      "226.6511030882205 311.7217476803053 266.62673616898434 310.3257678122053 269.06970093815943 380.283125703542 229.0940678573956 381.679105571642\n",
      "227.86040201318997 309.00243669192366 267.86040201318997 309.00243669192366 267.86040201318997 379.00243669192366 227.86040201318997 379.00243669192366\n",
      "227.86040201318997 307.00243669192366 267.86040201318997 307.00243669192366 267.86040201318997 377.00243669192366 227.86040201318997 377.00243669192366\n",
      "229.16386685080062 304.32698615816713 269.13949993156444 305.72296602626716 266.69653516238935 375.68032391760386 226.72090208162552 374.28434404950383\n",
      "229.23366584420563 302.32820450412896 269.20929892496946 303.724184372229 266.76633415579437 373.6815422635657 226.79070107503054 372.28556239546566\n",
      "230.62970852333615 299.6998740493513 270.5322705337291 302.4901329991163 265.64931737164034 372.31961651730404 225.74675536124735 369.529357567539\n",
      "230.7692214708244 297.70474594883166 270.67178348121735 300.4950048985967 265.7888303191286 370.3244884167844 225.8862683087356 367.5342294670194\n",
      "230.90873441831263 295.709617848312 270.8112964287056 298.499876798077 265.9283432666168 368.32936031626474 226.02578125622384 365.5391013664997\n",
      "231.04824736580088 293.71448974779236 270.95080937619383 296.5047486975574 266.06785621410506 366.3342322157451 226.16529420371208 363.54397326598007\n",
      "232.5351670234907 291.1364815877895 272.3160428382216 295.31762011849565 264.99905040948585 364.93415279427484 225.2181745947549 360.7530142635687\n",
      "232.744223950026 289.14743779705293 272.52509976475693 293.3285763277591 265.20810733602116 362.9451090035383 225.42723152129022 358.7639704728321\n",
      "232.9532808765613 287.15839400631637 272.73415669129224 291.33953253702254 265.41716426255647 360.9560652128017 225.63628844782554 356.77492668209555\n",
      "231.81493109289505 285.75223027506297 271.717493103288 288.54248922482805 266.83453994119924 358.3719727430158 226.93197793080626 355.58171379325074\n",
      "233.30185075058486 283.1742221150601 273.0827265653158 287.3553606457663 265.76573413658 356.97189332154556 225.9848583218491 352.79075479083934\n",
      "232.1635009669186 281.76805838380676 272.06606297731156 284.55831733357184 267.1831098152228 354.3878008517596 227.2805478048298 351.59754190199453\n",
      "233.65042062460842 279.1900502238039 273.43129643933935 283.3711887545101 266.1143040106036 352.98772143028935 226.33342819587264 348.80658289958313\n",
      "235.22640567829703 276.66550526440716 274.83712842795984 282.23242930280986 265.0950113607553 351.55119411471986 225.4842886110924 345.98427007631716\n",
      "236.8895360329584 274.1974992747731 276.28184615344674 281.1434263814504 264.1264737167616 350.0799690923051 224.73416359627325 343.1340419856278\n",
      "238.63778542044096 271.789039140033 277.7636894497932 280.10550677274347 263.20987109255003 348.57583882411 224.08396706319778 340.25937119139957\n",
      "240.4690238681633 269.4430591978662 279.28085291920314 279.119935021853 262.3463202272264 347.04063586117286 223.53449117618652 337.36376003718607\n",
      "239.53745259327582 267.89215248601346 278.66335662262804 276.2086201187239 264.10953826538486 344.67895217009044 224.98363423603263 336.36248453738\n",
      "241.36869104099816 265.54617254384664 280.18052009203797 275.2230483678334 263.2459874000612 343.1437492071533 224.43415834902137 333.4668733831665\n",
      "243.2806874669923 263.26553100944204 281.73115530452503 274.29102524212215 262.436540397335 341.57934395780455 223.98607255980232 330.55384972512456\n",
      "245.27111239813368 261.05300649320355 283.31337304993986 273.4136862682016 261.6821834436934 339.9876424088624 223.63992279188733 327.6269626338645\n",
      "244.44999616737618 259.4408945849751 282.900464004909 270.46638881765523 263.60584909771893 337.75470753333764 225.1553812601862 326.72921330065753\n",
      "246.44042109851756 257.22837006873664 284.4826817503238 269.5890498437347 262.8514921440773 336.1630059843955 224.8092314922712 323.80232620939745\n",
      "247.05845508726745 255.32625703614633 285.10071573907373 267.6869368111444 263.46952613282724 334.2608929518052 225.4272654810211 321.90021317680714\n",
      "246.23733885650995 253.71414512791787 284.6878066940429 264.73963936059795 265.39319178685275 332.02795807628036 226.94272394931997 321.00246384360025\n",
      "248.22776378765133 251.50162061167939 286.2700244394577 263.8623003866774 264.63883483321115 330.43625652733823 226.59657418140497 318.07557675234017\n",
      "247.40664755689383 249.88950870345093 285.8571153944268 260.91500293613103 266.56250048723666 328.20332165181344 228.11203264970385 317.17782741913334\n",
      "249.3970724880352 247.67698418721244 287.4393331398416 260.0376639622105 265.80814353359506 326.6116201028713 227.76588288178885 314.25094032787325\n",
      "248.5759562572777 246.06487227898398 287.0264240948107 257.0903665116641 267.73180918762057 324.3786852273465 229.28134135008773 313.3531909946664\n",
      "250.5663811884191 243.8523477627455 288.6086418402255 256.2130275377436 266.97745223397897 322.7869836784044 228.93519158217273 310.42630390340634\n",
      "249.7452649576616 242.24023585451704 288.1957327951946 253.2657300871972 268.9011178880045 320.5540488028796 230.4506500504716 309.5285545701995\n",
      "251.73568988880297 240.02771133827855 289.7779505406094 252.38839111327664 268.1467609343629 318.9623472539375 230.10450028255661 306.6016674789394\n",
      "250.91457365804547 238.4155994300501 289.3650414955785 249.44109366273025 270.0704265883884 316.7294123784127 231.6199587508555 305.7039181457326\n",
      "250.03769573488466 236.833126120026 288.84952478592476 246.5100019440129 271.9149920939479 314.4307027833328 233.10316304290788 304.75382695934593\n",
      "250.521539526084 234.89253466747402 289.3333685771241 244.56941049146093 272.3988358851472 312.4901113307808 233.58700683410723 302.81323550679394\n",
      "252.43353595207816 232.61189313306946 290.88400378961114 243.63738736574965 271.589388882421 310.92570608143205 233.13892104488818 299.90021184875195\n",
      "252.98481066371215 230.68936974119282 291.43527850124514 241.714863973873 272.140663594055 309.0031826895554 233.69019575652217 297.9776884568753\n",
      "253.53608537534615 228.76684634931618 291.98655321287913 239.79234058199637 272.691938305689 307.0806592976787 234.24147046815617 296.0551650649986\n",
      "255.52651030648752 226.5543218330777 293.5687709582939 238.9150016080758 271.9375813520474 305.4889577487366 233.89532070024117 293.12827797373853\n",
      "256.14454429523744 224.6522088004874 294.1868049470438 237.0128885754855 272.5556153407973 303.5868447161463 234.51335468899106 291.2261649411482\n",
      "258.2109727053489 222.5104969227247 295.7986775367855 236.1913026557517 271.8572675039885 301.9697861107653 234.26956267255207 288.2889803777384\n",
      "257.4466185706387 220.87071052632524 295.48887922244506 233.2313903013234 273.85768961619857 299.80534644198417 235.81542896439228 287.4446666669861\n",
      "258.0646525593886 218.96859749373493 296.106913211195 231.32927726873308 274.4757236049485 297.90323340939386 236.43346295314217 285.5425536343958\n",
      "257.2435363286311 217.35648558550648 295.6940041661641 228.3819798181867 276.399389258974 295.6702985338691 237.94892142144104 284.64480430118897\n",
      "259.2339612597725 215.143961069268 297.27622191157894 227.50464084426613 275.6450323053324 294.07859698492695 237.60277165352605 281.7179172099289\n",
      "258.412845029015 213.53184916103953 296.8633128665481 224.55734339371975 277.5686979593579 291.84566210940216 239.11823012182492 280.82016787672205\n",
      "258.964119740649 211.6093257691629 297.4145875781821 222.6348200018431 278.1199726709919 289.9231387175255 239.66950483345892 278.8976444848454\n",
      "258.0872418174881 210.0268524591388 296.8990708685284 219.70372828312577 279.9645381765514 287.6244291224456 241.1527091255113 277.94755329845873\n",
      "257.15567054260066 208.47594574728603 296.2815745719533 216.79241337999665 281.72775621470987 285.26274543136316 242.60185218535742 276.94627779865266\n",
      "[INFO] : Récompense maximale obtenue jusqu'ici : 0.0 (épisode 1)\n",
      "\n",
      "+------------+-------+---------------+---------------+---------------+---------------+-------------+\n",
      "| Récompense | Steps |   Durée de    |  Récompense   |  Récompense   |  Récompense   | Atterissage |\n",
      "|            |       |   l'épisode   |  moy. [100]   |   moy. [50]   |   moy. [25]   |             |\n",
      "+============+=======+===============+===============+===============+===============+=============+\n",
      "|  -74.200   |  53   | 0.2 secondes  |    -74.200    |    -74.200    |    -74.200    |      0      |\n",
      "+------------+-------+---------------+---------------+---------------+---------------+-------------+"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (300 of 300) |######################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "  0% (0 of 300) |                        | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "\u001b[1m\u001b[94m####################################### Episode 2 / 1000 ######################################################\u001b[00m\u001b[0;0m\n",
      "229.30346483761065 312.32454946624347 269.2790979183745 313.7205293343435 266.8361331491994 383.6778872256802 226.86050006843556 382.28190735758017\n",
      "229.37326383101566 310.3257678122053 269.3488969117795 311.7217476803053 266.9059321426044 381.679105571642 226.93029906184057 380.283125703542\n",
      "230.76930651014618 307.69743735742765 270.67186852053914 310.48769630719266 265.78891535845037 380.3171798253804 225.88635334805738 377.52692087561536\n",
      "232.256226167836 305.1194291974248 272.0371019825669 309.30056772813094 264.72010955383115 378.9171004039101 224.93923373910022 374.73596187320396\n",
      "233.8322112215246 302.59488423802804 273.4429339711874 308.1618082764307 263.70081690398285 377.48057308834063 224.09009415431998 371.913649049938\n",
      "232.74362929629143 301.1498492692051 272.5245051110224 305.33098779991127 265.2075126822866 374.94752047569045 225.42663686755566 370.7663819449843\n",
      "231.60527951262517 299.7436855379517 271.50784152301816 302.5339444877168 266.6248883609294 372.36342800590455 226.72232635053638 369.5731690561395\n",
      "tf.Tensor([[249.11508 336.05356]], shape=(1, 2), dtype=float32)\n",
      "1/1 [==============================] - 0s 154ms/step\n",
      "230.41854877438792 298.37810623817154 270.39418185515177 299.7740861062716 267.9512170859767 369.7314439976084 227.97558400521282 368.3354641295083\n",
      "229.18488293018228 297.0547751178899 269.1848829301823 297.05477511788996 269.1848829301823 367.05477511789 229.18488293018225 367.05477511788996\n",
      "227.9057850118078 295.7753044522334 267.88141809257166 294.3793245841334 270.32438286174676 364.3366824754702 230.34874978098287 365.73266234357015\n",
      "229.11508393677727 293.05599346385173 269.1150839367773 293.0559934638518 269.1150839367773 363.05599346385185 229.11508393677724 363.0559934638518\n",
      "229.11508393677727 291.05599346385173 269.1150839367773 291.0559934638518 269.1150839367773 361.05599346385185 229.11508393677724 361.0559934638518\n",
      "227.8359860184028 289.77652279819523 267.81161909916665 288.38054293009526 270.25458386834174 358.337900821432 230.27895078757786 359.733880689532\n",
      "229.04528494337225 287.05721180981357 269.0452849433723 287.0572118098136 269.0452849433723 357.0572118098137 229.04528494337222 357.0572118098136\n",
      "229.04528494337225 285.05721180981357 269.0452849433723 285.0572118098136 269.0452849433723 355.0572118098137 229.04528494337222 355.0572118098136\n",
      "227.76618702499778 283.77774114415706 267.74182010576163 282.3817612760571 270.1847848749367 352.33911916739385 230.20915179417284 353.7350990354938\n",
      "227.69638803159276 281.7789594901189 267.6720211123566 280.3829796220189 270.1149858815317 350.3403375133557 230.13935280076782 351.73631738145565\n",
      "228.90568695656222 279.05964850173723 268.90568695656225 279.0596485017373 268.90568695656225 349.05964850173734 228.9056869565622 349.0596485017373\n",
      "227.62658903818775 277.7801778360807 267.6022221189516 276.38419796798075 270.0451868881267 346.3415558593175 230.0695538073628 347.7375357274175\n",
      "227.55679004478273 275.78139618204256 267.5324231255466 274.3854163139426 269.9753878947217 344.34277420527934 229.9997548139578 345.7387540733793\n",
      "228.7660889697522 273.0620851936609 268.7660889697522 273.06208519366095 268.7660889697522 343.062085193661 228.76608896975216 343.06208519366095\n",
      "227.48699105137771 271.7826145280044 267.46262413214157 270.3866346599044 269.90558890131666 340.3439925512412 229.92995582055278 341.73997241934114\n",
      "226.16401944261804 270.5485631548917 266.06658145301105 267.75830420512676 270.9495346150998 337.58778772331453 231.0469726047068 340.3780466730795\n",
      "224.79878598059025 269.36143457583 264.5796617953212 265.1802960451239 271.896654224057 334.79682872090314 232.115778409326 338.97796725160924\n",
      "224.58972905405494 267.37239078509344 264.3706048687859 263.19125225438734 271.6875972975217 332.8077849301666 231.9067214827907 336.9889234608727\n",
      "224.38067212751963 265.3833469943569 264.1615479422506 261.2022084636508 271.47854037098637 330.81874113943 231.69766455625538 334.9998796701361\n",
      "225.39733571552387 262.5863036821624 265.2998977259169 259.7960447323974 270.18285088800565 329.6255282505852 230.28028887761263 332.4157872003502\n",
      "225.25782276803562 260.59117558164274 265.16038477842864 257.8009166318777 270.0433379405174 327.63040015006555 230.1407759301244 330.4206590998306\n",
      "225.11830982054738 258.5960474811231 265.0208718309404 255.80578853135808 269.90382499302916 325.6352720495459 230.00126298263615 328.4255309993109\n",
      "224.97879687305914 256.60091938060344 264.88135888345215 253.81066043083842 269.7643120455409 323.64014394902625 229.8617500351479 326.4304028987913\n",
      "224.8392839255709 254.6057912800838 264.7418459359639 251.81553233031877 269.6247990980527 321.6450158485066 229.72223708765966 324.4352747982716\n",
      "225.9529435934373 251.8459328986386 265.92857667420117 250.44995303053855 268.37154144337626 320.4073109218754 228.39590836261237 321.80329078997545\n",
      "224.62997198467764 250.61188152552594 264.53253399507065 247.82162257576093 269.4154871571594 317.6511060939488 229.5129251467664 320.4413650437138\n",
      "225.74363165254405 247.85202314408076 265.7192647333079 246.4560432759807 268.162229502483 316.4134011673176 228.18659642171912 317.80938103541763\n",
      "225.67383265913904 245.85324149004256 265.6494657399029 244.4572616219425 268.092430509078 314.41461951327943 228.1167974283141 315.81059938137946\n",
      "224.35086105037936 244.6191901169299 264.2534230607724 241.82893116716488 269.13637622286114 311.6584146853528 229.23381421246813 314.4486736351178\n",
      "224.21134810289112 242.62406201641025 264.11391011328413 239.83380306664523 268.9968632753729 309.66328658483314 229.0943012649799 312.45354553459816\n",
      "tf.Tensor([[246.60411 276.14368]], shape=(1, 2), dtype=float32)\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "222.84611464086333 241.43693343734853 262.6269904555943 237.2557949066424 269.94398288433007 306.87232758242175 230.16310706959908 311.0534661131279\n",
      "221.44028265224284 240.29817398564822 261.0510054019056 234.73124994724563 270.79312246911024 304.0500147591558 231.18239971944743 309.6169387975584\n",
      "222.3587115124079 237.46735350912883 262.1395873271388 233.2862149784227 269.45657975587454 302.9027476542021 229.67570394114364 307.08388618490824\n",
      "223.37537510041213 234.67031019693434 263.2779371108051 231.88005124716932 268.1608902728938 301.70953476535726 228.2583282625009 304.49979371512234\n",
      "223.2358621529239 232.6751820964147 263.13842416331687 229.88492314664967 268.0213773254056 299.7144066648376 228.11881531501265 302.5046656146027\n",
      "221.8706286908961"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (300 of 300) |######################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "  0% (0 of 300) |                        | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 231.48805351735297 261.65150450562703 227.30691498664683 268.96849693436275 296.9234476624262 229.18762111963184 301.10458619313243\n",
      "220.4647967022756 230.34929406565266 260.07551945193836 224.78237002725004 269.817636519143 294.10113483916024 230.2069137694802 299.66805887756294\n",
      "220.18645050035548 228.36875792816952 259.7971732500182 222.8018338897669 269.5392903172228 292.1205987016771 229.92856756756007 297.6875227400798\n",
      "221.10487936052053 225.53793745165012 260.8857551752514 221.35679892094396 268.2027476039871 290.9733315967234 228.42187178925627 295.15447012742965\n",
      "219.69904737190004 224.3991779999498 259.30977012156274 218.8322539615472 269.0518871887673 288.15101877345745 229.44116443910463 293.71794281186015\n",
      "220.6174762320651 221.5683575234304 260.39835204679594 217.38721899272426 267.7153444755316 287.00375166850375 227.93446866080083 291.18489019921\n",
      "220.40841930552978 219.57931373269383 260.1892951202606 215.3981752019877 267.5062875489963 285.0147078777672 227.72541173426552 289.1958464084734\n",
      "220.19936237899446 217.59026994195727 259.9802381937253 213.40913141125114 267.297230622461 283.02566408703063 227.5163548077302 287.20680261773686\n",
      "218.79353039037397 216.45151049025696 258.40425314003664 210.88458645185435 268.1463702072412 280.20335126376466 228.53564745757856 285.77027530216736\n",
      "218.51518418845384 214.47097435277382 258.1259069381165 208.9040503143712 267.86802400532105 278.22281512628155 228.25730125565843 283.78973916468425\n",
      "[INFO] : Récompense maximale obtenue jusqu'ici : -74.2 (épisode 1)\n",
      "\n",
      "+------------+-------+---------------+---------------+---------------+---------------+-------------+\n",
      "| Récompense | Steps |   Durée de    |  Récompense   |  Récompense   |  Récompense   | Atterissage |\n",
      "|            |       |   l'épisode   |  moy. [100]   |   moy. [50]   |   moy. [25]   |             |\n",
      "+============+=======+===============+===============+===============+===============+=============+\n",
      "|    -95     |  52   | 0.4 secondes  |    -84.600    |    -84.600    |    -84.600    |      0      |\n",
      "+------------+-------+---------------+---------------+---------------+---------------+-------------+ \n",
      "\n",
      "\u001b[1m\u001b[94m####################################### Episode 3 / 1000 ######################################################\u001b[00m\u001b[0;0m\n",
      "226.72090208162552 313.7205293343435 266.69653516238935 312.32454946624347 269.13949993156444 382.28190735758017 229.16386685080062 383.6778872256802\n",
      "227.93020100659498 311.00121834596183 267.930201006595 311.00121834596183 267.930201006595 381.00121834596183 227.93020100659498 381.00121834596183\n",
      "tf.Tensor([[247.9302  346.00122]], shape=(1, 2), dtype=float32)\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "226.6511030882205 309.7217476803053 266.62673616898434 308.3257678122053 269.06970093815943 378.283125703542 229.0940678573956 379.679105571642\n",
      "227.86040201318997 307.00243669192366 267.86040201318997 307.00243669192366 267.86040201318997 377.00243669192366 227.86040201318997 377.00243669192366\n",
      "229.16386685080062 304.32698615816713 269.13949993156444 305.72296602626716 266.69653516238935 375.68032391760386 226.72090208162552 374.28434404950383\n",
      "227.93020100659498 303.0036550378855 267.930201006595 303.0036550378855 267.930201006595 373.0036550378855 227.93020100659498 373.0036550378855\n",
      "226.6511030882205 301.724184372229 266.62673616898434 300.32820450412896 269.06970093815943 370.28556239546566 229.0940678573956 371.6815422635657\n",
      "227.86040201318997 299.0048733838473 267.86040201318997 299.0048733838473 267.86040201318997 369.0048733838473 227.86040201318997 369.0048733838473\n",
      "226.5813040948155 297.7254027181908 266.5569371755793 296.3294228500908 268.9999019447544 366.2867807414275 229.02426886399059 367.6827606095275\n",
      "225.25833248605582 296.49135134507816 265.1608944964488 293.70109239531314 270.0438476585376 363.53057591350085 230.14128564814462 366.3208348632659\n",
      "226.37199215392224 293.731492963633 266.34762523468606 292.335513095533 268.79059000386115 362.2928709868697 228.81495692309733 363.6888508549697\n",
      "225.04902054516256 292.49744159052034 264.95158255555555 289.7071826407553 269.8345357176443 359.53666615894304 229.93197370725136 362.32692510870805\n",
      "226.16268021302898 289.7375832090752 266.1383132937928 288.34160334097515 268.5812780629679 358.29896123231185 228.60564498220407 359.6949411004119\n",
      "226.09288121962396 287.738801555037 266.0685143003878 286.342821686937 268.5114790695629 356.3001795782737 228.53584598879905 357.6961594463737\n",
      "224.7699096108643 286.50475018192435 264.6724716212573 283.71449123215933 269.55542478334604 353.54397475034705 229.65286277295309 356.33423370011207\n",
      "223.4046761488365 285.31762160286263 263.18555196356743 281.13648307215647 270.5025443923032 350.75301574793565 230.72166857757227 354.9341542786418\n",
      "223.1956192223012 283.32857781212607 262.9764950370321 279.1474392814199 270.2934874657679 348.7639719571991 230.51261165103696 352.94511048790525\n",
      "222.98656229576588 281.3395340213895 262.7674381104968 277.15839549068335 270.0844305392326 346.77492816646253 230.30355472450165 350.9560666971687\n",
      "222.77750536923057 279.35049023065295 262.5583811839615 275.1693516999468 269.8753736126973 344.78588437572597 230.09449779796634 348.96702290643213\n",
      "223.7941689572348 276.55344691845846 263.6967309676278 273.7631879686934 268.57968412971655 343.59267148688116 228.6771221193236 346.38293043664623\n",
      "222.428935495207 275.36631833939674 262.20981130993795 271.1851798086905 269.5268037386737 340.80171248446976 229.7459279239428 344.982851015176\n",
      "221.02310350658652 274.2275588876965 260.63382625624934 268.6606348492938 270.3759433234539 337.9793996612038 230.76522057379114 343.5463236996065\n",
      "219.57838578109963 273.13855596633704 258.97069590158793 266.19262885965975 271.12606833827306 335.1291715705144 231.73375821778478 342.0750986771917\n",
      "218.09654248475312 272.1006363576301 257.22244651410534 263.78416872491965 271.77626487134853 332.2545007762862 232.6503608419963 340.57096840899663\n",
      "216.57937901534316 271.1150646067396 255.391208066383 261.43818878275283 272.3257407583598 329.3588896220727 233.51391170731995 339.0357654460595\n",
      "216.09553522414382 269.17447315418764 254.90736427518365 259.49759733020085 271.8418969671605 327.4182981695207 233.0300679161206 337.0951739935075\n",
      "216.71303152071889 266.26315825105854 255.8389355500711 257.9466906183481 270.3927539073144 326.41702266971464 231.26684987796207 334.7334903024251\n",
      "216.29720813908335 264.30686304959096 255.42311216843558 255.9903954168805 269.9769305256789 324.46072746824706 230.85102649632654 332.7771951009575\n",
      "214.7800446696734 263.3212912987005 253.59187372071324 253.64441547471367 270.52640641269016 321.56511631403356 231.71457736165019 331.24199213802035\n",
      "213.22940945718636 262.38926817298915 251.6798772947191 251.36377394030913 270.97449220190924 318.65209265599157 232.52402436437634 329.6775868886716\n",
      "213.74492616684006 259.45817645427184 252.5567552178799 249.78130063028505 269.49128790985685 317.7020014696049 230.67945885881684 327.3788772935917\n",
      "212.19429095435302 258.5261533285605 250.64475879188575 247.5006590958805 269.93937369907593 314.7889778115629 231.488905861543 325.81447204424296\n",
      "210.61207320893826 257.64881435463997 248.65433386074437 245.28813457964202 270.2855234669909 311.86209072030283 232.2432628151846 324.22277049530084\n",
      "209.00020061919656 256.82722843490615 246.5879054506329 243.14642270187937 270.5293154834299 308.92490615689303 232.94161065199341 322.60571188991986\n",
      "207.36063700358864 256.0623965452467 244.4479911862601 241.07813280861015 270.6704527253742 305.98100262828535 233.58309854270252 320.96526636492194\n",
      "tf.Tensor([[239.01555 281.0217 ]], shape=(1, 2), dtype=float32)\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "205.69537991782468 255.3552505155058 242.23719822352868 239.08578479247373 270.708763238835 303.03396682745586 234.16694493313074 319.30343255048797\n",
      "204.88190663167308 253.5281596002206 241.42372493737707 237.25869387718853 269.8952899526834 301.20687591217063 233.35347164697913 317.47634163520274\n",
      "203.1929849349989 252.879560978908 239.14474678696553 235.34471510734483 269.8307270622013 298.26029834828654 233.87896521023436 315.79514421984976\n",
      "202.31624264142073 251.08197288630964 238.26800449338737 233.54712701474648 268.9539847686231 296.4627102556882 233.0022229166562 313.9975561272514\n",
      "200.6057140247653 250.49271189093645 235.92361773912234 231.71384937950077 268.786627134135 293.5201808796257 233.4687234197777 312.29904339106145\n",
      "200.5005572222708 247.51848960799342 236.45231907423744 229.98364373643028 267.1382993494732 292.899226977372 231.18653749750627 310.4340728489352\n",
      "199.62381492869264 245.72090151539507 235.57557678065928 228.18605564383193 266.261557055895 291.10163888477365 230.3097952039281 308.63648475633687\n",
      "198.74707263511448 243.92331342279672 234.69883448708111 226.3884675512336 265.38481476231686 289.3040507921753 229.43305291034994 306.8388966637385\n",
      "197.03654401845904 243.33405242742356 232.35444773281608 224.55518991598788 265.21745712782877 286.3615214161128 229.89955341347144 305.14038392754856\n",
      "196.09760089288727 241.5681572417057 231.41550460724432 222.78929473027003 264.278514002257 284.59562623039494 228.96061028789967 303.3744887418307\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 961.8278 - accuracy: 0.7812\n",
      "194.36754937188323 241.0389517960261 229.00856552326073 221.03895179602605 264.00856552326104 281.66073006093677 229.36754937188326 301.6607300609369\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 658.8092 - accuracy: 0.8906\n",
      "194.1586577673155 238.07021124841899 229.47656148167255 219.29134873698328 262.33957087668523 281.0976802371082 227.0216671623279 299.87654274854395\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 592.9454 - accuracy: 0.8750\n",
      "193.21971464174374 236.30431606270113 228.53761835610078 217.52545355126543 261.40062775111346 279.33178505139034 226.08272403675613 298.1106475628261\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 320.4942 - accuracy: 0.7812\n",
      "tf.Tensor([[227.31017 257.81805]], shape=(1, 2), dtype=float32)\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "191.4896631207397 235.77511061702154 226.1306792721172 215.77511061702145 261.1306792721175 276.39688888193217 226.48966312073972 296.3968888819323\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 256.1451 - accuracy: 0.7500\n",
      "190.4896631207397 234.04305980945267 225.1306792721172 214.0430598094526 260.1306792721175 274.6648380743633 225.48966312073972 294.6648380743634\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 154.9521 - accuracy: 0.8750\n",
      "190.28077151617197 231.07431926184555 225.598675230529 212.2954567504098 258.4616846255417 274.1017882505347 223.14378091118436 292.88065076197046\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 19.7905 - accuracy: 0.9844\n",
      "190.17561471367748 228.1000969789025 226.1273765656441 210.56525110733932 256.81335684087986 273.480834348281 220.86159498891294 291.0156802198442\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 12.4632 - accuracy: 0.7656\n",
      "189.29887242009931 226.30250888630417 225.25063427206595 208.76766301474098 255.9366145473017 271.68324625568266 219.98485269533478 289.2180921272459\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 275.2020 - accuracy: 0.7188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (300 of 300) |######################| Elapsed Time: 0:00:01 Time:  0:00:01\n",
      "  0% (0 of 300) |                        | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] : Récompense maximale obtenue jusqu'ici : -74.2 (épisode 1)\n",
      "\n",
      "+------------+-------+---------------+---------------+---------------+---------------+-------------+\n",
      "| Récompense | Steps |   Durée de    |  Récompense   |  Récompense   |  Récompense   | Atterissage |\n",
      "|            |       |   l'épisode   |  moy. [100]   |   moy. [50]   |   moy. [25]   |             |\n",
      "+============+=======+===============+===============+===============+===============+=============+\n",
      "|  -95.100   |  53   | 1.4 secondes  |    -88.100    |    -88.100    |    -88.100    |      0      |\n",
      "+------------+-------+---------------+---------------+---------------+---------------+-------------+ \n",
      "\n",
      "\u001b[1m\u001b[94m####################################### Episode 4 / 1000 ######################################################\u001b[00m\u001b[0;0m\n",
      "226.72090208162552 313.7205293343435 266.69653516238935 312.32454946624347 269.13949993156444 382.28190735758017 229.16386685080062 383.6778872256802\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 16.6361 - accuracy: 0.9375\n",
      "tf.Tensor([[247.9302  348.00122]], shape=(1, 2), dtype=float32)\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "226.6511030882205 311.7217476803053 266.62673616898434 310.3257678122053 269.06970093815943 380.283125703542 229.0940678573956 381.679105571642\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 158.9359 - accuracy: 0.9531\n",
      "226.5813040948155 309.72296602626716 266.5569371755793 308.32698615816713 268.9999019447544 378.28434404950383 229.02426886399059 379.68032391760386\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 12.7941 - accuracy: 0.9688\n",
      "226.51150510141048 307.724184372229 266.4871381821743 306.32820450412896 268.9301029513494 376.28556239546566 228.95446987058557 377.6815422635657\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 153.9531 - accuracy: 0.7969\n",
      "227.72080402637994 305.0048733838473 267.72080402637994 305.0048733838473 267.72080402637994 375.0048733838473 227.72080402637994 375.0048733838473\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.2062 - accuracy: 0.6094\n",
      "229.02426886399059 302.3294228500908 268.9999019447544 303.7254027181908 266.5569371755793 373.6827606095275 226.5813040948155 372.2867807414275\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 151.3389 - accuracy: 0.8125\n",
      "230.4203115431211 299.70109239531314 270.32287355351406 302.49135134507816 265.4399203914253 372.3208348632659 225.5373583810323 369.53057591350085\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 304.3747 - accuracy: 0.8906\n",
      "231.9072312008109 297.1230842353103 271.68810701554185 301.30422276601644 264.3711145868061 370.9207554417956 224.59023877207514 366.73961691108946\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 10.6381 - accuracy: 0.9219\n",
      "233.48321625449952 294.59853927591354 273.09393900416234 300.1654633143162 263.3518219369578 369.4842281262261 223.7410991872949 363.9173040878235\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 299.3136 - accuracy: 0.9531\n",
      "235.1463466091609 292.1305332862795 274.53865672964923 299.07646039295673 262.3832842929641 368.01300310381134 222.99097417247575 361.0670759971341\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 179.7306 - accuracy: 0.8906\n",
      "235.49364296449477 290.1609177802551 274.8859530849831 297.1068448869323 262.730580648298 366.04338759778693 223.3382705278096 359.0974604911097\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 326.1774 - accuracy: 0.9375\n",
      "234.4561551670874 288.6787721263816 274.06687791675023 294.24569616478425 264.32476084954567 363.5644609766942 224.71403809988277 357.99753693829155\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 170.6967 - accuracy: 0.9531\n",
      "233.36757324185422 287.23373715755866 273.1484490565852 291.41487568826483 265.83145662784943 361.031408364044 226.05058081311844 356.85026983333785\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 7.6080 - accuracy: 1.0000\n",
      "233.57663016838953 285.2446933668221 273.3575059831205 289.42583189752827 266.04051355438475 359.04236457330745 226.25963773965375 354.8612260426013\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 312.4293 - accuracy: 1.0000\n",
      "233.78568709492484 283.25564957608555 273.56656290965583 287.4367881067917 266.24957048092006 357.0533207825709 226.46869466618907 352.87218225186473\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 7.2204 - accuracy: 1.0000\n",
      "232.64733731125858 281.8494858448322 272.5498993216516 284.6397447945972 267.6669461595628 354.46922831278494 227.76438414916979 351.6789693630199\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3366 - accuracy: 1.0000\n",
      "tf.Tensor([[250.15714 318.15936]], shape=(1, 2), dtype=float32)\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "234.1342569689484 279.27147768482934 273.9151327836794 283.4526162155355 266.5981403549436 353.0691488913147 226.81726454021262 348.8880103606085\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 318.6121 - accuracy: 0.7344\n",
      "tf.Tensor([[250.3662  316.17032]], shape=(1, 2), dtype=float32)\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "232.99590718528214 277.865313953576 272.89846919567515 280.655572903341 268.0155160335864 350.4850564215287 228.11295402319334 347.6947974717637\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 155.4980 - accuracy: 0.8438\n",
      "233.13542013277038 275.87018585305634 273.0379821431634 278.66044480282136 268.1550289810746 348.4899283210091 228.25246697068158 345.69966937124406\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 159.3030 - accuracy: 0.7500\n",
      "234.6223397904602 273.2921776930535 274.4032156051912 277.47331622375964 267.0862231764554 347.0898488995388 227.30534736172442 342.90871036883266\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 151.5328 - accuracy: 1.0000\n",
      "233.48399000679393 271.88601396180013 273.38655201718694 274.67627291156515 268.5035988550982 344.50575642975286 228.60103684470513 341.71549747998785\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 298.4851 - accuracy: 1.0000\n",
      "232.29725926855667 270.52043466201997 272.27289234932056 271.91641453012 269.82992758014547 341.8737724214567 229.85429449938158 340.47779255335666\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 150.0740 - accuracy: 0.9844\n",
      "231.06359342435104 269.19710354173833 271.0635934243511 269.19710354173833 271.0635934243511 339.19710354173833 231.06359342435104 339.19710354173833\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 158.1447 - accuracy: 0.9688\n",
      "231.06359342435104 267.19710354173833 271.0635934243511 267.19710354173833 271.0635934243511 337.19710354173833 231.06359342435104 337.19710354173833\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 142.9685 - accuracy: 0.9844\n",
      "229.78449550597657 265.9176328760818 269.76012858674045 264.5216530079818 272.20309335591554 334.4790108993185 232.22746027515166 335.8749907674185\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 434.6521 - accuracy: 0.9844\n",
      "230.99379443094602 263.19832188770016 270.9937944309461 263.19832188770016 270.9937944309461 333.19832188770016 230.99379443094602 333.19832188770016\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 290.9837 - accuracy: 1.0000\n",
      "232.29725926855667 260.52287135394363 272.27289234932056 261.91885122204366 269.82992758014547 331.87620911338036 229.85429449938158 330.4802292452803\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.7395 - accuracy: 1.0000\n",
      "233.6933019476872 257.894540899166 273.5958639580802 260.684799848931 268.71291079599143 330.5142833671187 228.8103487855984 327.7240244173537\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 143.0087 - accuracy: 0.6406\n",
      "233.83281489517543 255.89941279864632 273.73537690556844 258.68967174841134 268.8524237434797 328.51915526659906 228.94986173308664 325.72889631683404\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 148.6801 - accuracy: 0.8906\n",
      "233.97232784266367 253.90428469812667 273.8748898530567 256.6945436478917 268.9919366909679 326.5240271660794 229.08937468057488 323.7337682163144\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 147.9932 - accuracy: 0.9688\n",
      "tf.Tensor([[251.48213 290.21414]], shape=(1, 2), dtype=float32)\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "235.45924750035348 251.32627653812384 275.2401233150845 255.50741506882997 267.9231308863487 325.12394774460915 228.1422550716177 320.942809213903\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.4273 - accuracy: 0.9375\n",
      "235.6683044268888 249.33723274738728 275.4491802416198 253.5183712780934 268.132187812884 323.1349039538726 228.35131199815302 318.95376542316643\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 555.0865 - accuracy: 0.8594\n",
      "234.52995464322254 247.9310690161339 274.43251665361555 250.72132796589892 269.5495634915268 320.5508114840867 229.64700148113374 317.7605525343216\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 143.5451 - accuracy: 0.9688\n",
      "233.34322390498528 246.56548971635368 273.31885698574916 247.96146958445374 270.8758922165741 317.9188274757905 230.9002591358102 316.52284760769044\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 143.2670 - accuracy: 0.9844\n",
      "233.4130228983903 244.56670806231548 273.3886559791542 245.96268793041554 270.9456912099791 315.92004582175235 230.9700581292152 314.52406595365227\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 9.1991 - accuracy: 1.0000\n",
      "233.4828218917953 242.5679264082773 273.4584549725592 243.96390627637734 271.0154902033841 313.9212641677142 231.03985712262022 312.5252842996141\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 135.0563 - accuracy: 0.8594\n",
      "tf.Tensor([[252.24916 278.2446 ]], shape=(1, 2), dtype=float32)\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "234.87886457092583 239.93959595349966 274.78142658131884 242.72985490326468 269.8984734192301 312.55933842145254 229.99591140883703 309.76907947168746\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.1656 - accuracy: 0.8438\n",
      "235.01837751841407 237.94446785298 274.9209395288071 240.73472680274503 270.0379863667183 310.5642103209329 230.13542435632527 307.7739513711678\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.3483 - accuracy: 0.8281\n",
      "236.50529717610388 235.36645969297717 276.28617299083487 239.5475982236833 268.9691805620991 309.16413089946263 229.1883047473681 304.9829923687564\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 128.1893 - accuracy: 0.7812\n",
      "tf.Tensor([[252.73724 272.2653 ]], shape=(1, 2), dtype=float32)\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "238.0812822297925 232.8419147335804 277.6920049794554 238.408838771983 267.9498879122508 307.72760358389314 228.33916516258788 302.16067954549044\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 287.0742 - accuracy: 0.8906\n",
      "239.74441258445387 230.37390874394637 279.1367227049423 237.31983585062358 266.98135026825713 306.25637856147836 227.5890401477687 299.31045145480107\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.0337 - accuracy: 0.9219\n",
      "238.7069247870465 228.89176309007286 278.31764753670944 234.45868712847545 268.5755304695048 303.7774519403856 228.96480771984187 298.2105279019829\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 404.9912 - accuracy: 0.7969\n",
      "240.37005514170787 226.42375710043882 279.76236526219634 233.36968420711602 267.60699282551116 302.30622691797083 228.2146827050227 295.36029981129354\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 6.9989 - accuracy: 0.8906\n",
      "239.3325673443005 224.9416114465653 278.94329009396347 230.5085354849679 269.20117302675885 299.8273002968781 229.59045027709587 294.2603762584754\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 8.4672 - accuracy: 0.8438\n",
      "238.24398541906731 223.4965764777424 278.0248612337984 227.67771500844847 270.7078688050627 297.2942476842279 230.92699299033154 293.1131091535217\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.7151 - accuracy: 0.8750\n",
      "237.10563563540106 222.09041274648902 277.00819764579416 224.88067169625396 272.12524448370544 294.710155214442 232.22268247331226 291.9198962646769\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.3526 - accuracy: 0.8750\n",
      "235.9189048971638 220.7248334467088 275.8945379779277 222.12081331480877 273.45157320875273 292.07817120614584 233.4759401279887 290.6821913380457\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.1166 - accuracy: 0.9062\n",
      "234.68523905295817 219.4015023264272 274.68523905295825 219.4015023264271 274.68523905295837 289.4015023264275 234.68523905295817 289.40150232642736\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 130.1030 - accuracy: 0.9219\n",
      "234.68523905295817 217.4015023264272 274.68523905295825 217.4015023264271 274.68523905295837 287.4015023264275 234.68523905295817 287.40150232642736\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 127.1244 - accuracy: 0.7812\n",
      "233.4061411345837 216.12203166077066 273.3817742153476 214.72605179267055 275.8247389845228 284.68340968400764 235.84910590375878 286.07938955210756\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.9564 - accuracy: 0.9688\n",
      "232.08316952582402 214.887980287658 271.9857315362171 212.09772133789292 276.86868469830597 281.927204856081 236.96612268791282 284.7174638058459\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.6011 - accuracy: 0.9219\n",
      "230.71793606379623 213.70085170859628 270.49881187852725 209.51971317789008 277.81580430726314 279.1362458536696 238.034928492532 283.31738438437566\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 153.1355 - accuracy: 0.8750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (300 of 300) |######################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
      "  0% (0 of 300) |                        | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] : Récompense maximale obtenue jusqu'ici : -74.2 (épisode 1)\n",
      "\n",
      "+------------+-------+---------------+---------------+---------------+---------------+-------------+\n",
      "| Récompense | Steps |   Durée de    |  Récompense   |  Récompense   |  Récompense   | Atterissage |\n",
      "|            |       |   l'épisode   |  moy. [100]   |   moy. [50]   |   moy. [25]   |             |\n",
      "+============+=======+===============+===============+===============+===============+=============+\n",
      "|  -94.100   |  52   | 4.1 secondes  |    -89.600    |    -89.600    |    -89.600    |      0      |\n",
      "+------------+-------+---------------+---------------+---------------+---------------+-------------+ \n",
      "\n",
      "\u001b[1m\u001b[94m####################################### Episode 5 / 1000 ######################################################\u001b[00m\u001b[0;0m\n",
      "226.72090208162552 313.7205293343435 266.69653516238935 312.32454946624347 269.13949993156444 382.28190735758017 229.16386685080062 383.6778872256802\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 137.3416 - accuracy: 0.8438\n",
      "225.39793047286585 312.48647796123083 265.30049248325884 309.6962190114658 270.1834456453476 379.52570252965353 230.28088363495465 382.31596147941855\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 556.6156 - accuracy: 0.8281\n",
      "226.51159014073227 309.7266195797857 266.4872232214961 308.33063971168565 268.9301879906712 378.28799760302235 228.95455490990736 379.6839774711224\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 525.4845 - accuracy: 0.7500\n",
      "226.44179114732725 307.7278379257475 266.4174242280911 306.3318580576475 268.86038899726617 376.2892159489842 228.88475591650234 377.6851958170842\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 137.6637 - accuracy: 1.0000\n",
      "227.6510900722967 305.00852693736584 267.6510900722967 305.00852693736584 267.6510900722967 375.00852693736584 227.6510900722967 375.00852693736584\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 159.9761 - accuracy: 1.0000\n",
      "227.6510900722967 303.00852693736584 267.6510900722967 303.00852693736584 267.6510900722967 373.00852693736584 227.6510900722967 373.00852693736584\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 262.5569 - accuracy: 1.0000\n",
      "227.6510900722967 301.00852693736584 267.6510900722967 301.00852693736584 267.6510900722967 371.00852693736584 227.6510900722967 371.00852693736584\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 144.5931 - accuracy: 0.8281\n",
      "226.37199215392224 299.72905627170934 266.34762523468606 298.3330764036093 268.79059000386115 368.290434294946 228.81495692309733 369.68641416304604\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 13.0812 - accuracy: 0.9375\n",
      "225.04902054516256 298.4950048985967 264.95158255555555 295.70474594883166 269.8345357176443 365.5342294670194 229.93197370725136 368.3244884167844\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 144.1966 - accuracy: 0.9531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kouss\\anaconda3\\lib\\tkinter\\__init__.py\", line 1883, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"<ipython-input-19-2d50488aa0a0>\", line 291, in train\n",
      "    next_state, reward, done = env.step(action)\n",
      "  File \"<ipython-input-19-2d50488aa0a0>\", line 177, in step\n",
      "    self.button_f.invoke()\n",
      "  File \"C:\\Users\\kouss\\anaconda3\\lib\\tkinter\\__init__.py\", line 2667, in invoke\n",
      "    return self.tk.call(self._w, 'invoke')\n",
      "_tkinter.TclError: invalid command name \".!frame.!button3\"\n"
     ]
    }
   ],
   "source": [
    "env.window.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
