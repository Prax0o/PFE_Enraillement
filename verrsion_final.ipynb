{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b17f92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from collections import deque\n",
    "import progressbar\n",
    "\n",
    "from texttable import Texttable\n",
    "from IPython.display import clear_output\n",
    "import tensorflow as tf\n",
    "tf_config=tf.compat.v1.ConfigProto()\n",
    "tf_config.gpu_options.allow_growth=True\n",
    "sess = tf.compat.v1.Session(config=tf_config)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18ff3365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_2center(x0, y0, x1, y1):\n",
    "    width = x1 - x0\n",
    "    height = y1 - y0\n",
    "    x = x0 + width / 2\n",
    "    y = y0 + height / 2\n",
    "    return x, y\n",
    "\n",
    "def get_4points(x_center, y_center, width, height):\n",
    "    x0 = x_center - width / 2\n",
    "    y0 = y_center - height / 2\n",
    "    x1 = x_center + width / 2\n",
    "    y1 = y_center - height / 2\n",
    "    x2 = x_center + width / 2\n",
    "    y2 = y_center + height / 2\n",
    "    x3 = x_center - width / 2\n",
    "    y3 = y_center + height / 2\n",
    "    return x0, y0, x1, y1, x2, y2, x3, y3\n",
    "\n",
    "# Dessin des rails\n",
    "def init_rails(height, thickness, canvas):\n",
    "    canvas.create_line(0, height-thickness, 500, height-thickness, width=5)\n",
    "    canvas.create_line(0, height+thickness, 500, height+thickness, width=5)\n",
    "\n",
    "# Dessin de la route\n",
    "def init_roads(width, thickness, canvas):\n",
    "    canvas.create_line(width-thickness, 0, width-thickness, 500, width=2)\n",
    "    canvas.create_line(width+thickness, 0, width+thickness, 500, width=2)\n",
    "\n",
    "def rotate_point(x, y, center_x, center_y, angle_degre):\n",
    "    radians = angle_degre * math.pi / 180\n",
    "    x_rotated = (x - center_x) * math.cos(radians) - (y - center_y) * math.sin(radians) + center_x\n",
    "    y_rotated = (x - center_x) * math.sin(radians) + (y - center_y) * math.cos(radians) + center_y\n",
    "    return (x_rotated, y_rotated)\n",
    "\n",
    "def rotate(polygon_id, angle, canvas):\n",
    "    x0, y0, x1, y1, x2, y2, x3, y3 = canvas.coords(polygon_id)\n",
    "    pt0, pt1, pt2, pt3 = [x0, y0], [x1, y1], [x2, y2], [x3, y3]\n",
    "    x_center, y_center = get_2center(*pt0, *pt2)\n",
    "    new_pt0 = rotate_point(*pt0, x_center, y_center, angle)\n",
    "    new_pt1 = rotate_point(*pt1, x_center, y_center, angle)\n",
    "    new_pt2 = rotate_point(*pt2, x_center, y_center, angle)\n",
    "    new_pt3 = rotate_point(*pt3, x_center, y_center, angle)\n",
    "    canvas.coords(polygon_id, *new_pt0, *new_pt1, *new_pt2, *new_pt3)\n",
    "\n",
    "def move_forward(polygon, distance, angle):\n",
    "    \"\"\"Move the given polygon forward by the given distance (in pixels) along the path it faces.\n",
    "\n",
    "  Args:\n",
    "      polygon: The polygon object.\n",
    "      distance: The distance to move the polygon (in pixels).\n",
    "      angle: The angle at which the polygon is facing (in degrees).\n",
    "\n",
    "  Returns:\n",
    "      A tuple representing the amount of pixels to move the polygon in the x and y directions.\n",
    "  \"\"\"\n",
    "    # Convert the angle to radians. \n",
    "    angle -= 90\n",
    "    angle_rad = angle * math.pi / 180\n",
    "\n",
    "    # Calculate the movement in the x and y directions.\n",
    "    dx = distance * math.cos(angle_rad)\n",
    "    dy = distance * math.sin(angle_rad)\n",
    "\n",
    "    return (dx, dy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b9f6d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH = 500        #si 15---8, 120 step----1\n",
    "HEIGHT = 500\n",
    "\n",
    "ROTATION = [-2, 0, +2] \n",
    "MOTION = [-1, 0, +1]\n",
    "SPEED = 2\n",
    "\n",
    "class VehicleSimulator:\n",
    "    def __init__(self):\n",
    "        self.window = tk.Tk()\n",
    "        self.window.title(\"Simulator\")\n",
    "        self.canvas = tk.Canvas(self.window, width=WIDTH, height=HEIGHT)\n",
    "        self.canvas.pack()\n",
    "        self.steps = 0\n",
    "        # Draw rails\n",
    "        self.rails_thickness = 20\n",
    "        self.init_rails(HEIGHT/2, self.rails_thickness)\n",
    "        #self.init_rails(HEIGHT/3, self.rails_thickness)\n",
    "       \n",
    "        #horizontal road\n",
    "        self.canvas.create_line(0, 130, 150, 130, width=2)\n",
    "        self.canvas.create_line(0, 198, 150, 198, width=2)\n",
    "\n",
    "        # vertical road\n",
    "        #up\n",
    "        self.canvas.create_line(210, 120, 210, 0, width=2)\n",
    "        self.canvas.create_line(288, 120, 288, 0, width=2)\n",
    "\n",
    "        #down\n",
    "        self.canvas.create_line(210, 300, 210, 500, width=2)\n",
    "        self.canvas.create_line(288, 300, 288, 500, width=2)\n",
    "\n",
    "        #Draw transition zone\n",
    "        self.transition_zone()\n",
    "\n",
    "        #Draw vehicule\n",
    "        self.x = 248\n",
    "        self.y = HEIGHT-50\n",
    "        self.aligned=False\n",
    "        self.in_zone=False\n",
    "        self.motion = MOTION[2]\n",
    "        self.rotation = ROTATION[1]\n",
    "        self.vehicle_angle = 0\n",
    "        self.v_width, self.v_height = 40, 70\n",
    "        self.vehicule(self.x,self.y,self.v_width, self.v_height)\n",
    "\n",
    "        # Create buttons on grid(frame)\n",
    "        self.frame = tk.Frame(self.window)\n",
    "        self.frame.pack()\n",
    "\n",
    "        label = tk.Label(self.frame, text=\"MOTION\")\n",
    "        label.grid(row=0, column=0)\n",
    "        button1 = tk.Button(self.frame, text=\"backward\", command=lambda: self.button_motion(0))\n",
    "        button2 = tk.Button(self.frame, text=\"still\", command=lambda: self.button_motion(1))\n",
    "        self.button_f = tk.Button(self.frame, text=\"forward\", command=lambda: self.button_motion(2))\n",
    "        \n",
    "        button1.grid(row=0, column=1)\n",
    "        button2.grid(row=0, column=2)\n",
    "        self.button_f.grid(row=0, column=3)\n",
    "\n",
    "        label = tk.Label(self.frame, text=\"ROTATION\")\n",
    "        label.grid(row=1, column=0)\n",
    "        self.button_l = tk.Button(self.frame, text=\"left\", command=lambda: self.button_rotation(0))\n",
    "        button5 = tk.Button(self.frame, text=\"still\", command=lambda: self.button_rotation(1))\n",
    "        self.button_r = tk.Button(self.frame, text=\"right\", command=lambda: self.button_rotation(2))\n",
    "        self.button_l.grid(row=1, column=1)\n",
    "        button5.grid(row=1, column=2)\n",
    "        self.button_r.grid(row=1, column=3)\n",
    "\n",
    "        button_reset = tk.Button(self.frame, text=\"RESET\", command=self.reset)\n",
    "        button_reset.grid(row=0, column=4)\n",
    "        button_start = tk.Button(self.frame, text=\"TRAIN\", command=self.train)\n",
    "        button_start.grid(row=1, column=4)\n",
    "\n",
    "        # Text creation \n",
    "        self.text_x_y = self.canvas.create_text(400, 15, text=\"(x,y) = ({},{})\".format(round(self.x,1),round(self.y,1)), font=('Arial', 10))\n",
    "        self.text_vehicle_angle = self.canvas.create_text(400, 45, text=\"vehicle_angle = {}\".format(round(self.vehicle_angle,1)), font=('Arial', 10))\n",
    "        self.text_motion = self.canvas.create_text(400, 60, text=\"motion = {}\".format(round(self.motion,1)), font=('Arial', 10))\n",
    "        self.text_step = self.canvas.create_text(400, 80, text=\"step = {}\".format(self.steps), font=('Arial', 10))\n",
    "\n",
    "    # Buttons functions\n",
    "    def button_motion(self, number):\n",
    "        if number==0:\n",
    "            self.motion = MOTION[0]\n",
    "        elif number==1:\n",
    "            self.motion = MOTION[1]\n",
    "        elif number==2:\n",
    "            self.motion = MOTION[2]  \n",
    "\n",
    "        self.rotation = ROTATION[1]\n",
    "        self.update_position()\n",
    "\n",
    "\n",
    "    def button_rotation(self, number):\n",
    "        if number==0:\n",
    "            self.rotation = ROTATION[0]\n",
    "        elif number==1:\n",
    "            self.rotation = ROTATION[1]\n",
    "        elif number==2:\n",
    "            self.rotation = ROTATION[2]\n",
    "\n",
    "        #self.motion = MOTION[1]\n",
    "        self.update_position()\n",
    "    \n",
    "    def sample(self):\n",
    "        l = [0,1,2]\n",
    "        return np.random.choice(l)\n",
    "        \n",
    "\n",
    "    # Draw roads function\n",
    "    def init_roads(self, width, thickness):\n",
    "        self.canvas.create_line(width-thickness, 0, width-thickness, HEIGHT, width=2)\n",
    "        self.canvas.create_line(width+thickness, 0, width+thickness, HEIGHT, width=2)\n",
    "\n",
    "    # Draw transition zone function\n",
    "    def transition_zone(self):\n",
    "        self.canvas.create_line(150, 120, 340, 120, width=1)\n",
    "        self.canvas.create_line(150, 300, 340, 300, width=1)\n",
    "        self.canvas.create_line(150, 120, 150, 300, width=1)\n",
    "        self.canvas.create_line(340, 120, 340, 300, width=1)\n",
    "        self.canvas.create_line(150, 210, 340, 210, width=1)\n",
    "\n",
    "    # Draw rails function   \n",
    "    def init_rails(self,height, thickness):\n",
    "        self.canvas.create_line(340, height-thickness, WIDTH, height-thickness, width=5)\n",
    "        self.canvas.create_line(340, height+thickness, WIDTH, height+thickness, width=5)\n",
    "\n",
    "    # Update position function\n",
    "    def update_position(self):\n",
    "        rotate(self.vehicle, self.rotation, self.canvas)\n",
    "        self.vehicle_angle += self.rotation\n",
    "        if self.rotation == 0: movement = move_forward(self.vehicle, self.motion*SPEED, self.vehicle_angle)\n",
    "        else : movement = move_forward(self.vehicle, self.motion*(SPEED/2), self.vehicle_angle)\n",
    "        self.canvas.move(self.vehicle, movement[0], movement[1])\n",
    "        x0, y0, x1, y1, x2, y2, x3, y3 = self.canvas.coords(self.vehicle)\n",
    "        self.x, self.y = get_2center(x0, y0, x2, y2)\n",
    "        self.update_display()\n",
    "        \n",
    "\n",
    "    # Update text\n",
    "    def update_display(self):\n",
    "        self.canvas.itemconfig(self.text_x_y, text=\"(x,y) = ({},{})\".format(round(self.x,1),round(self.y,1)))\n",
    "        self.canvas.itemconfig(self.text_vehicle_angle, text=\"vehicle_angle = {}\".format(round(self.vehicle_angle,1)))\n",
    "        self.canvas.itemconfig(self.text_motion, text=\"motion = {}\".format(round(self.motion,1)))\n",
    "        self.canvas.itemconfig(self.text_step,text=\"step = {}\".format(self.steps), font=('Arial', 10))\n",
    "    \n",
    "    #create vehicule\n",
    "    def vehicule(self,x,y,v_width,v_height):\n",
    "        self.vehicle = self.canvas.create_polygon(*get_4points(x,y,v_width,v_height), fill=\"green\")\n",
    "    \n",
    "    # Reset the vehicle\n",
    "    def reset(self):\n",
    "        self.x = 248\n",
    "        self.y = HEIGHT-50\n",
    "        self.motion = MOTION[1]\n",
    "        self.rotation = ROTATION[1]\n",
    "        self.vehicle_angle = 0\n",
    "        self.v_width, self.v_height = 40, 70\n",
    "        self.canvas.delete(self.vehicle)\n",
    "        self.vehicule(self.x,self.y,self.v_width, self.v_height)\n",
    "        self.update_display()\n",
    "     \n",
    "        return [self.x,self.y]\n",
    "    \n",
    "    def step(self,rotation):\n",
    "        self.steps += 1\n",
    "        done = False\n",
    "        \n",
    "        reward=-0.1\n",
    "        \n",
    "        #mis à jour de position\n",
    "        #self.button_rotation(rotation)\n",
    "        if rotation == 0:\n",
    "            self.button_l.invoke()\n",
    "        elif rotation == 1:\n",
    "            self.button_f.invoke()\n",
    "        else:\n",
    "            self.button_r.invoke()\n",
    "       \n",
    "        x0, y0, x1, y1, x2, y2, x3, y3 = self.canvas.coords(self.vehicle)\n",
    "        \n",
    "        print(x0, y0, x1, y1, x2, y2, x3, y3)\n",
    "\t\t\n",
    "        if y0>300 and y1>300 and y2>300 and y3>300:\n",
    "            self.in_zone=False\n",
    "        \n",
    "        if y0<300 and y1<300 and y2<300 and y3<300:\n",
    "            self.in_zone=True\n",
    "            \n",
    "        # sortir de la route principale (amélioration possible)\n",
    "        if x0<210 or x1>288 and not self.in_zone :\n",
    "            reward=-100\n",
    "            done = True\n",
    "\n",
    "        #alignement avel les rails\n",
    "        if  y0==230 and y1==270 and y0==y2 and y1==y3 and self.aligned==False:\n",
    "            self.aligned = True\n",
    "            reward=5\n",
    "\n",
    "        # se désaligner\n",
    "        if self.aligned and rotation!=0:\n",
    "            reward=-10\n",
    "            self.aligned=False\n",
    "            \n",
    "        \n",
    "        #Punition à la sortie de la zone de transition\n",
    "        if self.in_zone and ((y0<210 or y1<210 or y2<210 or y3<210) or (x0<150 or x1<150 or x2<150 or x3<150) or (y0>300 or y1>300 or y2>300 or y3>300) or ( self.aligned==False and (x0>340 or x1>340 or x2>340 or x3>340))):\n",
    "            reward=-100\n",
    "            done=True\n",
    "            \n",
    "\n",
    "        #success\n",
    "        if self.aligned==True and self.x>=248:\n",
    "            reward=100\n",
    "            done=True\n",
    "        # Angle  reset\n",
    "        self.vehicle_angle %= 360\n",
    "        #time.sleep(0.001)\n",
    "        \n",
    "       \n",
    "        \n",
    "        return [self.x,self.y],reward,done\n",
    "    \n",
    "    def train(self):\n",
    "        for episode in range(episodes):\n",
    "            self.steps = 0\n",
    "             # On affiche l'épisode en cours\n",
    "            print(\"\\033[1m\" + \"\\033[94m\" + \"###\"*13 + \" Episode {} / {} \".format(episode+1, episodes) + \"###\"*18 + \"\\033[00m\" + \"\\033[0;0m\")\n",
    "\n",
    "            # A chaque épisode, on ré-initialise l'état de l'environnement\n",
    "            state = env.reset()\n",
    "\n",
    "            # La conséquence de l'initialisation est que le jeu ne peut pas être terminé, donc done=False    \n",
    "            done = False\n",
    "\n",
    "            # On initialise à 0 la variable steps qui est le compteur du nombre d'étapes (et donc d'actions) réalisés\n",
    "            # au cours d'un épisode\n",
    "            episode_steps = 0\n",
    "\n",
    "            # On initialise à 0 la durée de l'épisode\n",
    "            episode_duration = 0\n",
    "\n",
    "            # On initialise la récompense cumulée sur un episode\n",
    "            total_reward = 0\n",
    "\n",
    "            # On initialise le nombre d'atterissages réussis\n",
    "            sucessfull_landings = 0\n",
    "\n",
    "            # On calcule le nombre de steps maximum à réaliser pour l'épisode en cours\n",
    "            max_steps = get_max_steps(episode)\n",
    "\n",
    "\n",
    "            # On initialise les bars de progression\n",
    "            episode_bar = progressbar.ProgressBar()\n",
    "            episode_bar.start(max_value=get_max_steps(episode+1))\n",
    "\n",
    "\n",
    "            # On lance le chronomètre pour cet épisode\n",
    "            t0 = time.time()\n",
    "\n",
    "            # On commence par itérer tant que l'épisode n'est pas terminé\n",
    "            #def store_transition(self, state, action, reward, next_state, done):\n",
    "            while not done:\n",
    "                action = agent.choose_action(state)\n",
    "                print(action)\n",
    "                next_state, reward, done = env.step(action)\n",
    "                env.window.update()\n",
    "                print(next_state,reward,done)\n",
    "                agent.store_transition(state,action,reward,next_state,done)\n",
    "                agent.train_on_batch()\n",
    "                if overall_steps % agent.update_rate == 0:\n",
    "                    agent.update_target_model()\n",
    "                agent.perform_epsilon_decay()\n",
    "                total_reward += reward\n",
    "                state = next_state\n",
    "                episode_steps += 1\n",
    "                if done and reward == 100:\n",
    "                    sucessfull_landings += 1\n",
    "\n",
    "                if episode_steps >= max_steps or done:\n",
    "                    episode_duration = time.time() - t0\n",
    "                    break\n",
    "\n",
    "            episode_duration_list.append(episode_duration)\n",
    "            episodes_rewards_list.append(total_reward)\n",
    "            sucessfull_landings_list.append(sucessfull_landings)\n",
    "            if episode % 25: last25_episodes_rewards_list.append(np.mean(episodes_rewards_list[-25:]))\n",
    "            if episode % 50: last25_episodes_rewards_list.append(np.mean(episodes_rewards_list[-50:]))\n",
    "            if episode % 100: last25_episodes_rewards_list.append(np.mean(episodes_rewards_list[-100:]))\n",
    "            test = display_metrics(total_reward = total_reward, sucessfull_landings = sucessfull_landings, \n",
    "                                   episode_duration = episode_duration,episode_bar = episode_bar,current_step = episode_steps,\n",
    "                                   done = done,max_steps = max_steps)\n",
    "            if test: agent.model_policy.save('Model.h5')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3429695",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "\n",
    "    \"\"\"\n",
    "    Classe de l'agent DQN implémentant l'algorithme Deep Q-Network\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size):\n",
    "        \n",
    "        # L'attribut state_size représente la dimension de l'espace d'états, \n",
    "        # c'est-à-dire les dimensions de l'image de l'environnement de jeu\n",
    "        self.state_size = state_size\n",
    "        \n",
    "        # L'attribut action_size représente la dimension de l'espace d'actions\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # L'attribut render représente l'autorisation d'afficher le jeu pendant l'entrainement\n",
    "        self.render = True\n",
    "        \n",
    "        # Hyperparametres\n",
    "        \n",
    "        # L'attribut gamma représente le facteur de dépréciation dans le calcul de la récompense cumulée\n",
    "        self.gamma = 0.99\n",
    "        \n",
    "        # L'attribut epsilon représente le facteur d'exploration initial\n",
    "        self.epsilon = 1 \n",
    "        \n",
    "        # L'attribut lr représente le taux d'apprentissage du réseau de neurones DQN\n",
    "        self.lr = 0.0005 \n",
    "        \n",
    "        # L'attribut batch_size représente la taille de lots utilisée pendant l'entrainement\n",
    "        self.batch_size = 64\n",
    "        \n",
    "        # L'attribut epsilon_min représente le facteur d'exploration minimal \n",
    "        # en deça duquel l'agent ne peut jamais descendre pendant son entrainement\n",
    "        self.epsilon_min = 0.01\n",
    "        \n",
    "        # L'attribut epsilon_decay représente le facteur de décroissance qu'on applique à epsilon \n",
    "        # pour réduire le facteur d'exploration au cours de l'entrainement\n",
    "        self.epsilon_decay = 0.9995\n",
    "        \n",
    "        # L'attribut update_rate représente le nombre d'étapes au terme duquel on transfère \n",
    "        # les poids du réseau \"policy\" vers le réseau \"target\"\n",
    "        self.update_rate = 100\n",
    "        \n",
    "        # L'attribut replay_buffer_size représente la taille du buffer d'expériences utilisé pendant l'entrainement\n",
    "        self.replay_buffer_size = 15000\n",
    "        \n",
    "        # L'attribut replay_buffer_ùin_size représente la taille minimale du buffer d'expériences \n",
    "        # avant de pouvoir commencer à l'utiliser pendant l'entrainement\n",
    "        self.replay_buffer_min_size = 150\n",
    "        \n",
    "        # L'attribut mémory représente le buffer d'expérience. \n",
    "        # C'est un objet deque d'une profondeur de 5000 éléments maximum\n",
    "        self.replay_buffer = deque(maxlen=self.replay_buffer_size)\n",
    "        \n",
    "        self.model_policy = self.build_model()\n",
    "        self.model_target = self.build_model()\n",
    "        \n",
    "        self.update_target_model()\n",
    "    \n",
    "    #\n",
    "    # Méthode pour réduire graduellement la valeur du facteur d'exploration\n",
    "    #\n",
    "    def perform_epsilon_decay(self):\n",
    "        \n",
    "        '''\n",
    "        Méthode permettant de réduire la valeur du facteur d'exploration (epsilon)\n",
    "\n",
    "                Parameters:\n",
    "                        - self : l'instance de classe, permettant d'accéder à tous les attributs de la classe\n",
    "\n",
    "                Returns:\n",
    "                        Rien : la fonction réduit à chaque appel l'attribut epsilon d'un facteur de epsilon_decay\n",
    "                               sans jamais descendre en dessous de epsilon_min\n",
    "        '''\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "    #\n",
    "    # Méthode pour construire un modèle de réseau neuronal\n",
    "    #\n",
    "    def build_model(self):\n",
    "        \n",
    "        '''\n",
    "        Méthode permettant de définir et de compiler un modèle de réseau de neurones\n",
    "\n",
    "                Parameters:\n",
    "                        - self : l'instance de classe, permettant d'accéder à tous les attributs de la classe\n",
    "\n",
    "\n",
    "                Returns:\n",
    "                        model (Keras Model) : Modèle Keras représentant le modèle compilé défini \n",
    "                                              suivant une architecture\n",
    "        '''\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.Dense(128, activation='relu', input_shape=self.state_size))\n",
    "        model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(3, activation='linear'))\n",
    "\n",
    "        model.compile(optimizer=Adam(learning_rate=self.lr), loss='mean_squared_error', metrics=['accuracy'])\n",
    "        \n",
    "        return model\n",
    "            \n",
    " \n",
    "\n",
    "    #\n",
    "    # Méthode pour stocker des explériences (transitions) dans un buffer\n",
    "    #\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        '''\n",
    "        Méthode permettant rajouter une transition dans le buffer d'expérience\n",
    "\n",
    "                Parameters:\n",
    "                        - self : l'instance de classe, permettant d'accéder à tous les attributs de la classe\n",
    "                        - state (Numpy ndarray) : Matrice représentant l'état courant de l'environnement\n",
    "                        - action (int) : Nombre entier représentant l'action entreprise par l'agent\n",
    "                        - reward (float) : Nombre réel représentant la récompense immédiate obtenue de l'environnement\n",
    "                        - next_state (Numpy ndarray) : Matrice représentant l'état suivant de l'environnement\n",
    "\n",
    "\n",
    "                Returns:\n",
    "                        Rien : modification de l'attribut memory pour y rajouter une transition\n",
    "        '''\n",
    "        self.replay_buffer.append((state,action,reward,next_state,done))\n",
    "            \n",
    "\n",
    "        \n",
    "    #\n",
    "    # Méthode pour choisir l'action à entreprendre suivant une politique epsilon-greedy\n",
    "    #\n",
    "    def choose_action(self, state):\n",
    "        \n",
    "        '''\n",
    "        Méthode permettant de choisir l'action à entreprendre par l'action en suivant une politique epsilon-greedy\n",
    "\n",
    "                Parameters:\n",
    "                        - self : l'instance de classe, permettant d'accéder à tous les attributs de la classe\n",
    "                        - state (Numpy ndarray) : Matrice représentant l'état courant de l'environnement\n",
    "\n",
    "\n",
    "                Returns:\n",
    "                        - action (int) : Nombre entier représentant l'action à entreprendre par l'agent\n",
    "        '''\n",
    "        policy = self.model_policy\n",
    "        n = random.uniform(0, 1)\n",
    "        if n < self.epsilon_decay:\n",
    "            action = env.sample()\n",
    "            return action\n",
    "        else:\n",
    "            state_array = tf.expand_dims(state, axis=0)\n",
    "            print(state_array)\n",
    "            temp = self.model_policy.predict(state_array)\n",
    "            return np.argmax(temp[0])\n",
    "      \n",
    "    \n",
    "    #\n",
    "    # Méthode pour sélectionner des batchs de données dans le replay_buffer\n",
    "    #    \n",
    "    def sample_batches_from_buffer(self):\n",
    "        \n",
    "        '''\n",
    "        Méthode permettant d'échantillonner des vecteurs listes de valeurs à partir du buffer d'expérience\n",
    "\n",
    "                Parameters:\n",
    "                        - self : l'instance de classe, permettant d'accéder à tous les attributs de la classe\n",
    "\n",
    "\n",
    "                Returns:\n",
    "                        - states_list (List) : Liste Python contenant l'ensemble des 'batch_size' états de l'environnement\n",
    "                                          aléatoirement choisis dans le replay_buffer\n",
    "                        - actions_list (List) : Liste Python contenant l'ensemble des 'batch_size' actions de \n",
    "                                           l'environnement aléatoirement choisis dans le replay_buffer\n",
    "                        - rewards_list (List) : Liste Python contenant l'ensemble des 'batch_size' récompenses \n",
    "                                           de l'environnement aléatoirement choisis dans le replay_buffer\n",
    "                        - next_states_list (List) : Liste Python contenant l'ensemble des 'batch_size' états suivants \n",
    "                                               de l'environnement aléatoirement choisis dans le replay_buffer\n",
    "                        - dones_list (List) : Liste Python contenant l'ensemble des 'batch_size' done de \n",
    "                                              l'environnement aléatoirement choisis dans le replay_buffer\n",
    "        '''\n",
    "        \n",
    "        # Par exemple, pour batch_size = 4, on choisit un batch aléatoire de transitions\n",
    "        # batch = [(s1, a1, r1, s1', d1), (s2, a2, r2, s2', d2), (s3, a3, r3, s3', d3), (s4, a4, r4, s4', d4)]\n",
    "        # La fonction doit créer des batchs pour chaque variable (states, actions, rewards, next_states, dones)\n",
    "        # Par exemple :\n",
    "        # states      = [s1, s2, s3, s4]\n",
    "        # actions     = [a1, a2, a3, a4]\n",
    "        # rewards     = [r1, r2, r3, r4]\n",
    "        # next_states = [s1', s2', s3', s4']\n",
    "        # dones       = [d1, d2, d3, d4]\n",
    "        \n",
    "        indices = np.random.randint(0,len(self.replay_buffer), size=self.batch_size)\n",
    "   \n",
    "        batch = [self.replay_buffer[i] for i in indices]\n",
    "        list_index = []\n",
    "        states_list = []\n",
    "        actions_list = []\n",
    "        rewards_list = []\n",
    "        next_states_list = []\n",
    "        dones_list = []\n",
    "        \n",
    "        \n",
    "        # On boucle enfin sur la liste des transitions, et on stocke chaque élément dans la bonne liste\n",
    "        for temp in batch:\n",
    "            states_list.append(temp[0])\n",
    "            actions_list.append(temp[1])\n",
    "            rewards_list.append(temp[2])\n",
    "            next_states_list.append(temp[3])\n",
    "            dones_list.append(temp[4])\n",
    "\n",
    "        return states_list, actions_list, rewards_list, next_states_list, dones_list\n",
    "\n",
    "    \n",
    "\n",
    "    #\n",
    "    # Méthode pour calculer les valeur à cible (de la Q-value) à utiliser comme valeurs d'apprentissage des réseaux de neurones\n",
    "    #    \n",
    "    def compute_targets(self, states_list, actions_list, rewards_list, next_states_list, dones_list):\n",
    "        \n",
    "        '''\n",
    "        Méthode permettant de calculer les valeurs cibles des valeurs d'actions pour les batchs d'états dans actions_list\n",
    "        states_list\n",
    "\n",
    "                Parameters:\n",
    "                        - self : l'instance de classe, permettant d'accéder à tous les attributs de la classe\n",
    "                        - states_list (List) : Liste Python contenant l'ensemble des 'batch_size' états de l'environnement\n",
    "                                          aléatoirement choisis dans le replay_buffer\n",
    "                        - actions_list (List) : Liste Python contenant l'ensemble des 'batch_size' actions de \n",
    "                                           l'environnement aléatoirement choisis dans le replay_buffer\n",
    "                        - rewards_list (List) : Liste Python contenant l'ensemble des 'batch_size' récompenses \n",
    "                                           de l'environnement aléatoirement choisis dans le replay_buffer\n",
    "                        - next_states_list (List) : Liste Python contenant l'ensemble des 'batch_size' états suivants \n",
    "                                               de l'environnement aléatoirement choisis dans le replay_buffer\n",
    "                        - dones_list (List) : Liste Python contenant l'ensemble des 'batch_size' done de \n",
    "                                              l'environnement aléatoirement choisis dans le replay_buffer\n",
    "\n",
    "\n",
    "                Returns:\n",
    "                        - final_targets (List) : Liste Python contenant les 'batch_size' vecteurs contenant \n",
    "                                                       chacun les valeurs de chaque action\n",
    "        '''\n",
    "        \n",
    "        # Par exemple, pour un batch_size = 6, la liste des vecteurs de valeurs cibles pour les 6 états \n",
    "        # states = [s1, s2, s3, s4, s5, s6] choisis aléatoirement dans le replay_buffer aura cette forme \n",
    "        # [[0.1, 0.4, 0.1, 1.1], ==> vecteur des valeurs-cibles d'action pour s1\n",
    "        #  [4.1, 0.9, 0.3, 0.1], ==> vecteur des valeurs-cibles d'action pour s2\n",
    "        #  [0.8, 0.3, 9.8, 9.0], ==> vecteur des valeurs-cibles d'action pour s3\n",
    "        #  [0.1, 0.2, 7.3, 7.9], ==> vecteur des valeurs-cibles d'action pour s4\n",
    "        #  [6.3, 0.1, 2.0, 5.4], ==> vecteur des valeurs-cibles d'action pour s5\n",
    "        #  [3.9, 0.1, 0.3, 5.3]] ==> vecteur des valeurs-cibles d'action pour s6\n",
    "        \n",
    "        #np.squeeze est utilisé ici pour réduire la dimension du np.array\n",
    "        # (batch_size, 8, 1) --> (batch_size, 8)\n",
    "        # Cf https://numpy.org/doc/stable/reference/generated/numpy.squeeze.html   \n",
    "        \n",
    "        states_list = np.squeeze(np.array(states_list))\n",
    "        next_states_list = np.squeeze(np.array(next_states_list))\n",
    "        \n",
    "        current_val = self.model_policy.predict_on_batch(states_list)\n",
    "        final_val = current_val \n",
    "        \n",
    "        future_val = self.model_policy.predict_on_batch(next_states_list)\n",
    "        real_targets = np.array(rewards_list)+ self.gamma * (np.amax(np.array(future_val),axis=1)) * (1 - np.array(dones_list))\n",
    "       \n",
    "        final_val[ [i for i in range(self.batch_size)] , [np.array(actions_list)] ] = real_targets\n",
    "        \n",
    "        return final_val\n",
    "\n",
    "\n",
    "    #\n",
    "    # Méthode pour entrainer le modèle neuronal en utilisant des échantillons de données dans le buffer d'expériences\n",
    "    #\n",
    "    def train_on_batch(self):\n",
    "        \n",
    "        '''\n",
    "        Méthode permettant d'entrainer le modèle neuronal policy \n",
    "\n",
    "                Parameters:\n",
    "                        - self : l'instance de classe, permettant d'accéder à tous les attributs de la classe\n",
    "\n",
    "\n",
    "                Returns:\n",
    "                        - Rien : la fonction échantillonne les données du replay_buffer\n",
    "                                 calcule les target et entraine le réseau \"policy\"\n",
    "        '''\n",
    "        \n",
    "        # On vérifie la taille du replay_buffer : s'il n'y a pas assez d'éléments dans le replay_buffer,\n",
    "        # on ne fait rien\n",
    "        if len(self.replay_buffer) < self.replay_buffer_min_size:\n",
    "            return \n",
    "        else:\n",
    "            states_list, actions_list, rewards_list, next_states_list, dones_list = self.sample_batches_from_buffer();\n",
    "            final_targets = self.compute_targets(states_list, actions_list, rewards_list, next_states_list, dones_list)\n",
    "            self.model_policy.fit(np.squeeze(np.array(states_list)),np.array(final_targets))\n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "    #\n",
    "    # Méthode pour de mettre à jour les poids du réseau \"target\" à partir du réseau local\n",
    "    #\n",
    "    def update_target_model(self):\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        Méthode permettant  de de mettre à jour les poids du réseau \"target\" à partir du réseau local\n",
    "\n",
    "                Parameters:\n",
    "                        - self : l'instance de classe, permettant d'accéder à tous les attributs de la classe\n",
    "\n",
    "\n",
    "                Returns:\n",
    "                        - Rien : transfère les poids de model dans target_model\n",
    "        '''\n",
    "        \n",
    "        wt = self.model_policy.get_weights()\n",
    "        self.model_target.set_weights(wt)\n",
    "        \n",
    "\n",
    "                        \n",
    "                    \n",
    "    #\n",
    "    # Méthode pour charger un fichier de poids dans le modèle local\n",
    "    #\n",
    "    def load(self, name):\n",
    "        \n",
    "        '''\n",
    "        Méthode permettant de charger un fichier de poids dans le modèle local\n",
    "\n",
    "                Parameters:\n",
    "                        - self : l'instance de classe, permettant d'accéder à tous les attributs de la classe\n",
    "\n",
    "\n",
    "                Returns:\n",
    "                        - Rien : Charge les poids d'un fichier de poids stocké au chemin d'accès \"name\" dans model \n",
    "        '''\n",
    "        \n",
    "        print(\"[INFO] : Loading model from disk at \", name, \"\\n\")     \n",
    "        self.policy_model.load_weights(name)\n",
    "\n",
    "    #\n",
    "    # Méthode pour enregistrer les poids de model dans un fichier de poids\n",
    "    #\n",
    "    def save(self, name):\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        Méthode permettant d'enregistrer les poids de model dans un fichier de poids\n",
    "\n",
    "                Parameters:\n",
    "                        - self : l'instance de classe, permettant d'accéder à tous les attributs de la classe\n",
    "\n",
    "\n",
    "                Returns:\n",
    "                        - Rien : Enregistre les paramètres de model dans un fichier de poids stocké au chemin d'accès \"name\"\n",
    "        '''\n",
    "        \n",
    "        print(\"[INFO] : Saving model to disk at \", name, \"\\n\")\n",
    "        self.policy_model.save_weights(name)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "485077a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensions d'un état environnement après redimensionnement\n",
    "state_size = (2,)\n",
    "\n",
    "# Nombre d'actions possibles dans l'environnement\n",
    "action_size = 3\n",
    "\n",
    "# On instancie notre agent DQN\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "# Nombre d'épisodes pendant lequel il faut entrainer\n",
    "episodes = 500\n",
    "\n",
    "# Batch size pour échantillonner les transitions\n",
    "batch_size = 32 \n",
    "\n",
    "# Compteur du nombre de timestep\n",
    "overall_steps = 0  \n",
    "\n",
    "# Liste d'enregistrement des récompenses cumulées à chaque épisode\n",
    "episodes_rewards_list = list()\n",
    "\n",
    "# Liste d'enregistrement des moyennes de récompenses cumulées sur les 25 derniers épisodes\n",
    "last25_episodes_rewards_list = list()\n",
    "\n",
    "# Liste d'enregistrement des moyennes de récompenses cumulées sur les 50 derniers épisodes\n",
    "last50_episodes_rewards_list = list()\n",
    "\n",
    "# Liste d'enregistrement des moyennes de récompenses cumulées sur les 100 derniers épisodes\n",
    "last100_episodes_rewards_list = list()\n",
    "\n",
    "# Liste d'enregistrement des atterissages réussis\n",
    "sucessfull_landings_list = list()\n",
    "\n",
    "# Liste d'enregistrement des durées de chaque épisode\n",
    "episode_duration_list = list()\n",
    "\n",
    "# Intervalles d'épisodes avec le nombre de steps maximum. Cela définit une stratégie d'entrainement\n",
    "# Ici STRATEGY_DICT permet d'appliquer la stratégie suivante\n",
    "# Pour les 150 premiers épisodes, l'agent ne doit pas éffectuer plus de 300 steps\n",
    "# Pour les épisodes entre 150 et 400, l'agent ne doit pas effectuer plus de 500 steps\n",
    "# Au delà de 500 épisodes, l'agent ne doit pas effectuer plus de 700 épisodes\n",
    "# Cela permet d'avoir un entrainement plus rapide, et un apprentissage par pallier.\n",
    "STRATEGY_DICT = {\"0-150\":300, \"150-400\":500, \"400-{}\".format(episodes+1):700}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f77c6825",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_steps(episode_number):\n",
    "    \n",
    "    '''\n",
    "        Fonction permettant de retourner le nombre maximum de steps à effectuer sur l'épisode en cours\n",
    "\n",
    "                Parameters:\n",
    "                       \n",
    "                        - episode_number (Int) : Nombre entier représentant le numéro de l'épisode actuel\n",
    "                        \n",
    "                Returns:\n",
    "                        - steps_max (Int) : Nombre entier représentant le nombre maximum de steps \n",
    "                                            à effectuer sur l'épisode en cours\n",
    "    '''\n",
    "    \n",
    "    for interval_str, steps_max in STRATEGY_DICT.items():\n",
    "        low_interval = int(interval_str.split(\"-\")[0])\n",
    "        high_interval = int(interval_str.split(\"-\")[1])\n",
    "        \n",
    "        if episode_number < high_interval and episode_number >= low_interval:\n",
    "            \n",
    "            return steps_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68dec1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_metrics(total_reward, \n",
    "                    sucessfull_landings, \n",
    "                    episode_duration, \n",
    "                    episode_bar, \n",
    "                    current_step, \n",
    "                    done, \n",
    "                    max_steps,\n",
    "                    display_frequency = 10):\n",
    "    \n",
    "    '''\n",
    "        Fonction permettant d'afficher la progression de l'entrainement à chaque action réalisée, d'afficher \n",
    "        un récapitulatif des performances à chaque fin d'épisode, et d'afficher des graphiques d'entrainement\n",
    "        à intervalle régulier\n",
    "\n",
    "                Parameters:\n",
    "                       \n",
    "                        - total_reward (Float) : Nombre réel représentant la récompense cumulée obtenue à la fin \n",
    "                                                 d'un épisode.\n",
    "                        - sucessfull_landings (Int) : Nombre entier représentant, à la fin d'un épisode, le nombre\n",
    "                                                      d'atterrissage réussis par l'agent depuis le début de \n",
    "                                                      l'entrainement \n",
    "                        - episode_duration (Float) : Nombre réel représentant la durée en secondes de l'épisode\n",
    "                        - episode_bar (ProgressBar()) : Objet ProgressBar() représentant un widget destiné à \n",
    "                                                        afficher la progression de l'entrainement\n",
    "                        - current_step (Int) : Nombre entier représentant le nombre de steps réalisés à \n",
    "                                               un instant donné d'un épisode.\n",
    "                        - done (Booléen) : Booléen représentant si l'épisode est terminé ou non\n",
    "                        - episode_number (Int) : Nombre entier représentant le numéro de l'épisode actuel\n",
    "                        - max_steps (Int) : Nombre entier représentant le nombre maximum de steps autorisés\n",
    "                                            pour l'épisode en cours\n",
    "                        - display_frequency (Int) : Nombre entier représentant la fréquence (en nombre d'épisodes)\n",
    "                                                    à laquelle afficher les courbes d'apprentissages de l'agent\n",
    "\n",
    "                        \n",
    "                Returns:\n",
    "                        - is_better (Booléen) : Booléen représentant l'agent a progressé ou non entre les derniers\n",
    "                                                épisodes et l'épisode en cours. L'agent a progressé s'il a augmenté\n",
    "                                                sa récompense cumulée.\n",
    "    '''\n",
    "    \n",
    "    # On commence par initialiser is_better à False. \n",
    "    # On le passera dans la suite à True si les conditions sont réunies\n",
    "    is_better = False\n",
    "    \n",
    "    # Premier cas de traitement : l'épisode est terminé ou l'agent à excéder le nombre maximum de steps autorisés\n",
    "    if done or current_step >= max_steps:\n",
    "        \n",
    "        # On initialise toutes les variables ... Sinon Python ne sera pas content !\n",
    "        best_id_before = best_reward_before = best_id_after = best_reward_after = 0\n",
    "        \n",
    "        # On commence par déterminer la valeur maximum de récompense cumulée des derniers épisodes ainsi que l'épisode\n",
    "        # à laquelle cette valeur maximum a été obtenue. Il faut s'assurer que la liste des récompenses cumulées\n",
    "        # n'est pas vide. Sinon Python ne sera pas content !\n",
    "        if len(episodes_rewards_list) > 0:\n",
    "            # La fonction argsort() permet de trouver une liste ordonnée des indices des valeurs les plus\n",
    "            # élevées d'une liste\n",
    "            # Par exemple, [0.65, -9.6, 808.12, 11.1, 0.0078] ==> [2, 3, 0, 4, 1]\n",
    "            best_id_before = np.array(episodes_rewards_list).argsort()[-1]\n",
    "            best_reward_before = np.array(episodes_rewards_list)[best_id_before]\n",
    "        \n",
    "        # On rajoute la dernière récompense cumulée à la liste d'enregistrement des récompenses cumulées\n",
    "        episodes_rewards_list.append(total_reward)\n",
    "        \n",
    "        # On rajoute la dernière valeur des atterrissages réussis à la liste d'enregistrement \n",
    "        # des atterrissages réussis\n",
    "        sucessfull_landings_list.append(sucessfull_landings)\n",
    "        \n",
    "        #  On rajoute la durée du dernier épisode à la liste d'enregistrement des durées\n",
    "        episode_duration_list.append(episode_duration)\n",
    "        \n",
    "        # On calcule ensuite la moyenne glissante des récompenses cumulées sur les 25, 50 et 100 derniers épisodes    \n",
    "        last25_episodes_rewards_list.append(np.mean(episodes_rewards_list[-25:]))\n",
    "        last50_episodes_rewards_list.append(np.mean(episodes_rewards_list[-50:]))\n",
    "        last100_episodes_rewards_list.append(np.mean(episodes_rewards_list[-100:]))\n",
    "        \n",
    "        # On détermine la valeur maximum de récompense cumulée après avoir rajouté la dernière récompense cumulée\n",
    "        # ainsi que l'épisode à laquelle cette valeur maximum a été obtenue. \n",
    "        # Il faut s'assurer que la liste des récompenses cumulées n'est pas vide.  Sinon Python ne sera pas content !    \n",
    "        if len(episodes_rewards_list) > 0:\n",
    "            # On réutilise la fonction argsort()\n",
    "            best_id_after = np.array(episodes_rewards_list).argsort()[-1]\n",
    "            best_reward_after = np.array(episodes_rewards_list)[best_id_after]\n",
    "        \n",
    "        # Si la récompense cumulée a augmenté après le rajout de la valeur du dernier épisode, alors \n",
    "        # l'agent s'est amélioré. \n",
    "        if best_reward_after > best_reward_before :\n",
    "            # On affiche cette amélioration\n",
    "            print(\"\\033[1m\" + '\\033[92m' + \"[INFO] : Récompense obtenue améliorée de {:.1f} (épisode {}) à {:.1f}\\n\".\n",
    "                  format(best_reward_before, best_id_before+1, best_reward_after) + \"\\033[00m\"+ \"\\033[0;0m\")\n",
    "            \n",
    "            # Et on passe la variable is_better à True\n",
    "            is_better = True\n",
    "        \n",
    "                \n",
    "        # Si la récompense cumulée n'a pas augmenté, on rappelle juste la valeur maximale obtenue jusqu'ici\n",
    "        # et l'épisode auquel cette valeur maximale a été obtenue\n",
    "        else:\n",
    "            print(\"[INFO] : Récompense maximale obtenue jusqu'ici : {:.1f} (épisode {})\\n\".\n",
    "                  format(best_reward_before, best_id_before+1))\n",
    "        \n",
    "        # Dans cette partie, on va afficher à la fin d'un épisode des statistiques sur la performance de l'agent \n",
    "        # On utilise un objet Texttable de largeur maximum 100 pixels\n",
    "        t = Texttable(max_width=100)\n",
    "        \n",
    "        # On centre l'ensemble des colonnes avec l'argument \"c\"\n",
    "        t.set_cols_align([\"c\", \"c\", \"c\", \"c\", \"c\", \"c\", \"c\"])\n",
    "        \n",
    "        # On rajoute et on affiche les statistiques\n",
    "        t.add_rows([['Récompense', 'Steps', \"Durée de l'épisode\",'Récompense moy. [100]', 'Récompense moy. [50]', 'Récompense moy. [25]', 'Atterissage'], \n",
    "                    [\"{:.1f}\".format(total_reward), current_step, \n",
    "                     \"{:.1f} secondes\".format(episode_duration_list[-1]),\n",
    "                     \"{:.1f}\".format(last100_episodes_rewards_list[-1]), \n",
    "                     \"{:.1f}\".format(last50_episodes_rewards_list[-1]), \n",
    "                     \"{:.1f}\".format(last25_episodes_rewards_list[-1]), \n",
    "                     np.sum(sucessfull_landings_list)]])\n",
    "        print(t.draw(), \"\\n\")\n",
    "\n",
    "        \n",
    "        # A la fin de l'épisode, on mets la barre de progression à 100% (max_value)\n",
    "        episode_bar.update(int(episode_bar.max_value))\n",
    "        episode_bar.finish()\n",
    "        \n",
    "        # Si on atteint la fréquence d'affichage des courbes d'apprentissage\n",
    "        if len(episodes_rewards_list)%display_frequency == 0:\n",
    "            \n",
    "            # On divise le graphique en 4 sous-graphiques pour afficher 4 variables        \n",
    "            fig, axs = plt.subplots(4, figsize=(12, 12))\n",
    "            \n",
    "            # On donne un nom au graphique principal\n",
    "            fig.suptitle(\"Evolution de performances à l'épisode {}\".format(len(episodes_rewards_list)))\n",
    "            \n",
    "            ######## On s'occupe ici du 1er sous-graphique représenté par la variable axs[0]: \n",
    "            ######## afficher l'évolution de la récompense cumulée au cours de l'entrainement\n",
    "            ######## La méthode plot() va afficher un graphe \"ligne\" par défaut\n",
    "            axs[0].plot(np.array(range(len(episodes_rewards_list)))+1, np.array(episodes_rewards_list))\n",
    "            \n",
    "            # On va tracer une ligne verticale sur le sous-graphique ax[0] qui va représenté la valeur maximale\n",
    "            # obtenue jusqu'ici pour la récompense cumulée\n",
    "            \n",
    "            # Pour ce faire, on va déterminer la limite haute et la limite basse de cette ligne\n",
    "            ymin,ymax = axs[0].get_ylim()\n",
    "            \n",
    "            # On détermine aussi la position en x, c'est-à-dire l'épisode auquel cette valeur a été obtenue\n",
    "            x_line = np.max([best_id_before, best_id_after])+1\n",
    "            \n",
    "            # On dessine enfin la ligne verticale avec les éléments calculés précédemment\n",
    "            axs[0].vlines(x=x_line, ymin=ymin, ymax=ymax, colors='red', label=\"Meilleure valeur\")\n",
    "            \n",
    "            # On affiche une légende pour une meilleure interprétabilité du graphique\n",
    "            axs[0].legend(loc='best')\n",
    "            \n",
    "            # On affiche le titre du sous-graphique pour une meilleure interprétabilité également\n",
    "            axs[0].title.set_text(\"Récompense cumulée\")\n",
    "            \n",
    "            # Enfin, on affiche une étiquette pour indiquer à l'écrit la valeur maximale et l'épisode\n",
    "            axs[0].text(x_line-1, \n",
    "                        ymin+1, \n",
    "                        \"Valeur max = {:.1f} | Episode = {}\".format(episodes_rewards_list[x_line-1], x_line),\n",
    "                        fontsize=10,\n",
    "                        bbox = dict(facecolor=\"wheat\", alpha=0.7))\n",
    "            \n",
    "            \n",
    "            ######## On s'occupe ici du 1er sous-graphique représenté par la variable axs[1]: \n",
    "            ######## afficher l'évolution de la moyenne des 25 dernières récompenses cumulées \n",
    "            ######## La méthode plot() va afficher un graphe \"ligne\" par défaut\n",
    "            axs[1].plot(np.array(range(len(last25_episodes_rewards_list)))+1, np.array(last25_episodes_rewards_list), 'o')\n",
    "            \n",
    "            # On va tracer une ligne verticale sur le sous-graphique ax[1] qui va représenté la valeur maximale\n",
    "            # obtenue jusqu'ici pour la moyenne des 25 dernières récompenses cumulées\n",
    "            \n",
    "            # Pour ce faire, on va déterminer la limite haute et la limite basse de cette ligne\n",
    "            ymin,ymax = axs[1].get_ylim()\n",
    "            \n",
    "            # On détermine aussi la position en x, c'est-à-dire l'épisode auquel cette valeur a été obtenue\n",
    "            x_line = np.argmax(np.array(last25_episodes_rewards_list))+1\n",
    "            \n",
    "            # On dessine enfin la ligne verticale avec les éléments calculés précédemment\n",
    "            axs[1].vlines(x=x_line, ymin=ymin, ymax=ymax, colors='red', label=\"Meilleure valeur\")\n",
    "            \n",
    "            # On affiche une légende pour une meilleure interprétabilité du graphique\n",
    "            axs[1].legend(loc='best')\n",
    "            \n",
    "            # On affiche le titre du sous-graphique pour une meilleure interprétabilité également\n",
    "            axs[1].title.set_text(\"Récompense moyenne des 25 derniers épisodes\")\n",
    "            \n",
    "            # Enfin, on affiche une étiquette pour indiquer à l'écrit la valeur maximale et l'épisode\n",
    "            axs[1].text(x_line-1, \n",
    "                        ymin+1, \n",
    "                        \"Valeur max = {:.1f} | Episode = {}\".format(last25_episodes_rewards_list[x_line-1], x_line),\n",
    "                        fontsize=10,\n",
    "                        bbox = dict(facecolor=\"wheat\", alpha=0.7))\n",
    "\n",
    "            ######## On s'occupe ici du 1er sous-graphique représenté par la variable axs[2]: \n",
    "            ######## afficher l'évolution de la moyenne des 100 dernières récompenses cumulées \n",
    "            ######## La méthode plot() va afficher un graphe \"ligne\" par défaut\n",
    "            axs[2].plot(np.array(range(len(last100_episodes_rewards_list)))+1, np.array(last100_episodes_rewards_list), '+')\n",
    "            \n",
    "            # On va tracer une ligne verticale sur le sous-graphique ax[2] qui va représenté la valeur maximale\n",
    "            # obtenue jusqu'ici pour la moyenne des 100 dernières récompenses cumulées\n",
    "            \n",
    "            # Pour ce faire, on va déterminer la limite haute et la limite basse de cette ligne\n",
    "            ymin,ymax = axs[2].get_ylim()\n",
    "            \n",
    "            # On détermine aussi la position en x, c'est-à-dire l'épisode auquel cette valeur a été obtenue\n",
    "            x_line = np.argmax(np.array(last100_episodes_rewards_list))+1\n",
    "            \n",
    "            # On dessine enfin la ligne verticale avec les éléments calculés précédemment\n",
    "            axs[2].vlines(x=x_line, ymin=ymin, ymax=ymax, colors='red', label=\"Meilleure valeur\")\n",
    "            \n",
    "            # On affiche une légende pour une meilleure interprétabilité du graphique\n",
    "            axs[2].legend(loc='best')\n",
    "            \n",
    "            # On affiche le titre du sous-graphique pour une meilleure interprétabilité également\n",
    "            axs[2].title.set_text(\"Récompense moyenne des 100 derniers épisodes\")\n",
    "            \n",
    "            # Enfin, on affiche une étiquette pour indiquer à l'écrit la valeur maximale et l'épisode\n",
    "            axs[2].text(x_line-1, \n",
    "                        ymin+1, \n",
    "                        \"Valeur max = {:.1f} | Episode = {}\".format(last100_episodes_rewards_list[x_line-1], x_line),\n",
    "                        fontsize=10,\n",
    "                        bbox = dict(facecolor=\"wheat\", alpha=0.7))\n",
    "\n",
    "\n",
    "            ######## On s'occupe ici du 1er sous-graphique représenté par la variable axs[2]: \n",
    "            ######## afficher l'évolution du nombre d'atterrissages réussis \n",
    "            ######## La méthode plot() va afficher un graphe \"ligne\" par défaut\n",
    "            cumsum_landings = np.cumsum(sucessfull_landings_list) \n",
    "            axs[3].plot(np.array(range(len(cumsum_landings)))+1, cumsum_landings)\n",
    "            \n",
    "            # On va tracer une ligne verticale sur le sous-graphique ax[3] qui va représenté la valeur maximale\n",
    "            # obtenue jusqu'ici pour la moyenne des 100 dernières récompenses cumulées\n",
    "            \n",
    "            # Pour ce faire, on va déterminer la limite haute et la limite basse de cette ligne\n",
    "            ymin,ymax = axs[3].get_ylim()\n",
    "            \n",
    "            # On détermine aussi la position en x, c'est-à-dire l'épisode auquel cette valeur a été obtenue\n",
    "            x_line = np.argmax(cumsum_landings)+1\n",
    "            \n",
    "            # On dessine enfin la ligne verticale avec les éléments calculés précédemment\n",
    "            axs[3].vlines(x=x_line, ymin=ymin, ymax=ymax, colors='red', label=\"Meilleure valeur\")\n",
    "            \n",
    "            # On affiche une légende pour une meilleure interprétabilité du graphique\n",
    "            axs[3].legend(loc='best')\n",
    "            \n",
    "            # On affiche le titre du sous-graphique pour une meilleure interprétabilité également\n",
    "            axs[3].title.set_text(\"Nombre d'atterrissages réussis depuis le début\")\n",
    "            \n",
    "            # Enfin, on affiche une étiquette pour indiquer à l'écrit la valeur maximale et l'épisode\n",
    "            axs[3].text(x_line-1, \n",
    "                        0, \n",
    "                        \"Valeur max = {:.1f} | Episode = {}\".format(cumsum_landings.tolist()[x_line-1], x_line),\n",
    "                        fontsize=10,\n",
    "                        bbox = dict(facecolor=\"wheat\", alpha=0.7))\n",
    "            \n",
    "            # On appelle enfin la méthode plot qui afficher afficher à l'ecran le graphique\n",
    "            plt.show()\n",
    "    \n",
    "    # Dans le cas où l'épisode n'est pas terminé\n",
    "    else:\n",
    "        # On incrémente juste la progressbar pour l'affichage\n",
    "        episode_bar.update(current_step)\n",
    "    \n",
    "    return is_better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be992f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% (0 of 300) |                        | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[94m####################################### Episode 1 / 500 ######################################################\u001b[00m\u001b[0;0m\n",
      "0\n",
      "226.79070107503054 415.71931098838166 266.76633415579437 414.32333112028164 269.20929892496946 484.28068901161834 229.23366584420563 485.67666887971836\n",
      "[248.0, 450.0] -0.1 False\n",
      "0\n",
      "225.6072424137591 416.48038771578865 265.5098044241521 413.69012876602363 270.39275758624086 483.51961228421135 230.4901955758479 486.30987123397637\n",
      "[248.0, 450.0] -0.1 False\n",
      "0\n",
      "224.45106587826663 417.2823029274635 264.23194169299757 413.1011643967573 271.54893412173334 482.7176970725365 231.7680583070024 486.8988356032427\n",
      "[248.0, 450.0] -0.1 False\n",
      "0\n",
      "223.32358009156627 418.12407961324635 262.9343028412291 412.5571555748437 272.6764199084337 481.87592038675365 233.0656971587709 487.4428444251563\n",
      "[247.99999999999997, 450.0] -0.1 False\n",
      "1\n",
      "223.04523388964614 416.14354347576324 262.65595663930895 410.5766194373606 272.3980737065135 479.89538424927053 232.78735095685076 485.4623082876732\n",
      "[247.72165379807984, 448.0194638625169] -0.1 False\n",
      "0\n",
      "221.77416434182618 416.039348307416 261.1664744623145 409.09342120073876 273.32184689899964 478.02996391159337 233.92953677851133 484.9758910182706\n",
      "[247.5480056204129, 447.0346561095047] -0.1 False\n",
      "2\n",
      "222.73241261101913 414.1684676540095 262.34313536068197 408.60154361560683 272.08525242788653 477.9203084275168 232.47452967822375 483.4872324659194\n",
      "[247.40883251945283, 446.0443880407631] -0.1 False\n",
      "0\n",
      "221.46134306319917 414.06427248566223 260.8536531836875 407.118345378985 273.00902562037265 476.0548880898396 233.61671549988432 483.00081519651684\n",
      "[247.2351843417859, 445.0595802877509] -0.1 False\n",
      "1\n",
      "221.1140467078653 412.0946569796378 260.50635682835366 405.1487298729606 272.6617292650388 474.0852725838152 233.26941914455045 481.03119969049243\n",
      "[246.88788798645203, 443.0899647817265] -0.1 False\n",
      "0\n",
      "219.84011510233657 412.03488497166467 258.9660191316888 403.7184173389543 273.519837488932 472.18874939032077 234.39393345957976 480.50521702303115\n",
      "[246.67997629563428, 442.1118171809927] -0.1 False\n",
      "1\n",
      "219.42429172070104 410.0785897701971 258.5501957500533 401.7621221374867 273.1040141072965 470.2324541888532 233.97811007794422 478.54892182156357\n",
      "[246.26415291399877, 440.15552197952513] -0.1 False\n",
      "1\n",
      "219.0084683390655 408.1222945687295 258.1343723684178 399.8058269360191 272.688190725661 468.2761589873856 233.5622866963087 476.592626620096\n",
      "[245.84832953236327, 438.19922677805755] -0.1 False\n",
      "2\n",
      "219.90084007610955 406.2191112229566 259.293150196598 399.2731841162794 271.4485226332831 468.20972682713403 232.0562125127947 475.15565393381127\n",
      "[245.67468135469633, 437.21441902504534] -0.1 False\n",
      "2\n",
      "220.8590883453025 404.3482305695501 260.46981109496545 398.7813065311475 270.21192816217 468.10007134305744 230.6012054125071 473.6669953814601\n",
      "[245.53550825373625, 436.2241509563038] -0.1 False\n",
      "0\n",
      "219.58801879748253 404.24403540120284 258.980328917971 397.29810829452566 271.13570135465613 466.2346510053803 231.7433912341677 473.1805781120575\n",
      "[245.36186007606932, 435.2393432032916] -0.1 False\n",
      "0\n",
      "218.3140871919538 404.1842633932297 257.4399912213062 395.86779576051936 271.99380957854936 464.3381278118858 232.867905549197 472.6545954445962\n",
      "[245.15394838525157, 434.26119560255773] -0.1 False\n",
      "0\n",
      "217.03884561814354 404.1689873686152 255.85067466918355 394.49211154462853 272.7852073611603 462.4128123839483 233.97337831012032 472.08968820793507\n",
      "[244.9120264896519, 433.29089987628174] -0.1 False\n",
      "2\n",
      "217.8642536055364 402.2358200662199 256.99015763488876 393.9193524335096 271.54397599213195 462.389684484876 232.4180719627796 470.70615211758644\n",
      "[244.70411479883415, 432.31275227554795] -0.1 False\n",
      "1\n",
      "217.44843022390086 400.2795248647523 256.57433425325326 391.963057232042 271.12815261049644 460.4333892834084 232.00224858114407 468.74985691611886\n",
      "[244.28829141719865, 430.35645707408037] -0.1 False\n",
      "2\n",
      "218.3408019609449 398.3763415189794 257.73311208143343 391.4304144123023 269.88848451811856 460.3669571231569 230.49617439763009 467.31288422983414\n",
      "[244.11464323953174, 429.37164932106816] -0.1 False\n",
      "1\n",
      "217.99350560561103 396.406726012955 257.38581572609957 389.4607989062779 269.5411881627847 458.3973416171325 230.14887804229622 465.34326872380973\n",
      "[243.76734688419788, 427.40203381504375] -0.1 False\n",
      "0\n",
      "216.7195740000823 396.34695400498185 255.84547802943473 388.0304863722716 270.3992963866779 456.50081842363807 231.27339235732552 464.81728605634845\n",
      "[243.55943519338012, 426.42388621430996] -0.1 False\n",
      "1\n",
      "216.30375061844677 394.39065880351427 255.4296546477992 386.074191170804 269.9834730050424 454.5445232221705 230.85756897569 462.86099085488087\n",
      "[243.1436118117446, 424.4675910128424] -0.1 False\n",
      "1\n",
      "215.88792723681124 392.4343636020467 255.01383126616366 384.1178959693364 269.5676496234069 452.5882280207029 230.44174559405445 460.9046956534133\n",
      "[242.72778843010906, 422.5112958113748] -0.1 False\n",
      "2\n",
      "216.78029897385528 390.53118025627384 256.17260909434384 383.5852531495967 268.327981531029 452.5217958604514 228.93567141054046 459.46772296712857\n",
      "[242.55414025244215, 421.5264880583626] -0.1 False\n",
      "1\n",
      "216.4330026185214 388.56156475024943 255.82531273900997 381.6156376435723 267.98068517569516 450.552180354427 228.5883750552066 457.49810746110415\n",
      "[242.20684389710829, 419.5568725523382] -0.1 False\n",
      "0\n",
      "215.15907101299268 388.5017927422763 254.28497504234517 380.185325109566 268.8387933995884 448.6556571609325 229.7128893702359 456.9721247936429\n",
      "[241.99893220629053, 418.5787249516044] -0.1 False\n",
      "0\n",
      "213.88382943918242 388.4865167176618 252.69565849022254 378.8096408936752 269.6301911821993 446.730341732995 230.81836213115923 456.4072175569817\n",
      "[241.75701031069087, 417.6084292253284] -0.1 False\n",
      "0\n",
      "212.60883158251238 388.5157552878888 251.0592994200454 377.4902610552089 270.3539143272354 444.7785797708913 231.9034464897024 455.8040740035713\n",
      "[241.48137295487388, 416.64716752939006] -0.1 False\n",
      "2\n",
      "213.36627018776576 386.5549592954475 252.17809923880588 376.87808347146085 269.11263193078264 444.79878431078066 230.30080287974258 454.4756601347674\n",
      "[241.2394510592742, 415.67687180311407] -0.1 False\n",
      "2\n",
      "214.19167817515861 384.62179199305217 253.3175822045111 376.3053243603419 267.8714005617543 444.7756564117084 228.74549653240183 453.09212404441877\n",
      "[241.03153936845644, 414.6987242023803] -0.1 False\n",
      "0\n",
      "212.91643660134835 384.6065159684377 251.72826565238847 374.92964014445107 268.6627983443652 442.8503409837709 229.85096929332516 452.5272168077576\n",
      "[240.78961747285678, 413.7284284761043] -0.1 False\n",
      "2\n",
      "213.7418445887412 382.6733486660424 252.86774861809369 374.3568810333321 267.4215669753369 442.8272130846986 228.29566294598442 451.143680717409\n",
      "[240.58170578203902, 412.7502808753705] -0.1 False\n",
      "0\n",
      "212.46660301493094 382.6580726414279 251.27843206597106 372.9811968174413 268.2129647579478 440.9018976567611 229.40113570690775 450.5787734807478\n",
      "[240.33978388643936, 411.7799851490945] -0.1 False\n",
      "2\n",
      "213.2920110023238 380.7249053390326 252.41791503167627 372.40843770632233 266.97173338891946 440.8787697576888 227.845829359567 449.1952373903992\n",
      "[240.1318721956216, 410.8018375483607] -0.1 False\n",
      "2\n",
      "214.18438273936783 378.82172199325976 253.5766928598564 371.87579488658264 265.7320652965416 440.8123375974373 226.33975517605302 447.7582647041145\n",
      "[239.9582240179547, 409.8170297953485] -0.1 False\n",
      "0\n",
      "212.9104511338391 378.7619499852866 252.0363551631916 370.44548235257633 266.5901735204348 438.9158144039428 227.46426949108232 447.2322820366532\n",
      "[239.75031232713695, 408.8388821946147] -0.1 False\n",
      "0\n",
      "211.63520956002884 378.7466739606721 250.44703861106896 369.0697981366855 267.38157130304575 436.9904989760053 228.56974225200565 446.66737479999205\n",
      "[239.5083904315373, 407.8685864683387] -0.1 False\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (300 of 300) |######################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "  0% (0 of 300) |                        | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212.4606175474217 376.8135066582768 251.58652157677417 368.49703902556655 266.1403399340174 436.96737107693303 227.01443590466494 445.2838387096434\n",
      "[239.30047874071954, 406.8904388676049] -0.1 False\n",
      "0\n",
      "211.18537597361143 376.79823063366234 249.99720502465155 367.1213548096757 266.93173771662833 435.04205564899553 228.11990866558827 444.71893147298226\n",
      "[239.05855684511988, 405.92014314132894] -0.1 False\n",
      "2\n",
      "212.01078396100428 374.86506333126704 251.13668799035676 366.54859569855677 265.6905063476 435.01892774992325 226.56460231824752 443.33539538263364\n",
      "[238.85064515430213, 404.94199554059514] -0.1 False\n",
      "0\n",
      "210.73554238719402 374.84978730665256 249.54737143823414 365.17291148266594 266.4819041302109 433.09361232198574 227.67007507917086 442.7704881459725\n",
      "[238.60872325870247, 403.97169981431915] -0.1 False\n",
      "0\n",
      "209.46054453052398 374.87902587687955 247.911012368057 363.8535316441997 267.20562727524697 431.1418503598821 228.755159437714 442.1673445925621\n",
      "[238.33308590288547, 403.0104381183808] -100 True\n",
      "[INFO] : Récompense maximale obtenue jusqu'ici : -104.2 (épisode 1)\n",
      "\n",
      "+------------+-------+---------------+---------------+---------------+---------------+-------------+\n",
      "| Récompense | Steps |   Durée de    |  Récompense   |  Récompense   |  Récompense   | Atterissage |\n",
      "|            |       |   l'épisode   |  moy. [100]   |   moy. [50]   |   moy. [25]   |             |\n",
      "+============+=======+===============+===============+===============+===============+=============+\n",
      "|  -104.200  |  43   | 0.4 secondes  |   -104.200    |   -104.200    |   -104.200    |      0      |\n",
      "+------------+-------+---------------+---------------+---------------+---------------+-------------+ \n",
      "\n",
      "\u001b[1m\u001b[94m####################################### Episode 2 / 500 ######################################################\u001b[00m\u001b[0;0m\n",
      "0\n",
      "226.79070107503054 415.71931098838166 266.76633415579437 414.32333112028164 269.20929892496946 484.28068901161834 229.23366584420563 485.67666887971836\n",
      "[248.0, 450.0] -0.1 False\n",
      "1\n",
      "226.72090208162552 413.7205293343435 266.69653516238935 412.32454946624347 269.13949993156444 482.28190735758017 229.16386685080062 483.6778872256802\n",
      "[247.93020100659498, 448.00121834596183] -0.1 False\n",
      "0\n",
      "225.46768694660997 413.48404201149066 265.37024895700296 410.69378306172564 270.2532021190917 480.52326657991335 230.35064010869877 483.3135255296784\n",
      "[247.86044453285086, 447.003654295702] -0.1 False\n",
      "0\n",
      "224.20698194784984 413.2914353277972 263.9878577625808 409.11029679709105 271.3048501913166 478.72682947287024 231.5239743765856 482.9079680035764\n",
      "[247.7559160695832, 446.0091324003337] -0.1 False\n",
      "1\n",
      "223.99792502131453 411.30239153706066 263.7788008360455 407.1212530063545 271.09579326478126 476.7377856821337 231.3149174500503 480.91892421283984\n",
      "[247.5468591430479, 444.02008860959717] -0.1 False\n",
      "1\n",
      "223.78886809477922 409.3133477463241 263.5697439095102 405.13220921561793 270.88673633824595 474.7487418913971 231.105860523515 478.9298804221033\n",
      "[247.33780221651259, 442.0310448188606] -0.1 False\n",
      "1\n",
      "223.5798111682439 407.32430395558754 263.36068698297487 403.1431654248814 270.67767941171064 472.75969810066056 230.89680359697968 476.9408366313667\n",
      "[247.12874528997727, 440.04200102812405] -0.1 False\n",
      "0\n",
      "222.31315228058347 407.17581257262884 261.92387503024634 401.6088885342262 271.6659920974509 470.92765334613614 232.05526934778808 476.4945773845388\n",
      "[246.98957218901717, 439.0517329593825] -0.1 False\n",
      "2\n",
      "223.33610960401617 405.3395139914777 263.11698541874716 401.15837546077154 270.43397784748294 470.7749081365507 230.65310203275195 474.9560466672569\n",
      "[246.88504372574954, 438.0572110640142] -0.1 False\n",
      "0\n",
      "222.06945071635573 405.191022608519 261.68017346601863 399.62409857011636 271.4222905332232 468.9428633820263 231.81156778356035 474.50978742042895\n",
      "[246.74587062478946, 437.06694299527265] -0.1 False\n",
      "1\n",
      "221.7911045144356 403.2104864710359 261.4018272640985 397.64356243263325 271.14394433130303 466.9623272445432 231.53322158164022 472.52925128294584\n",
      "[246.4675244228693, 435.08640685778954] -0.1 False\n",
      "0\n",
      "220.52003496661564 403.10629130268865 259.91234508710403 396.1603641960114 272.06771752378916 465.096906906866 232.6754074033008 472.04283401354326\n",
      "[246.2938762452024, 434.10159910477734] -0.1 False\n",
      "0\n",
      "219.2461033610869 403.0465192947155 258.3720073904392 394.7300516620051 272.9258257476824 463.2003837133716 233.7999217183301 471.516851346082\n",
      "[246.08596455438465, 433.12345150404354] -0.1 False\n",
      "0\n",
      "217.97086178727665 403.031243270101 256.78269083831657 393.3543674461143 273.7172235302933 461.2750682854341 234.9053944792534 470.9519441094208\n",
      "[245.84404265878499, 432.15315577776755] -0.1 False\n",
      "2\n",
      "218.7962697746695 401.0980759677057 257.9221738040218 392.7816083349953 272.47599216126497 461.2519403863618 233.35008813191268 469.5684080190722\n",
      "[245.63613096796723, 431.17500817703376] -0.1 False\n",
      "0\n",
      "217.52102820085923 401.08279994309123 256.33285725189916 391.4059241191045 273.2673899438759 459.3266249584243 234.455560892836 469.00350078241104\n",
      "[245.39420907236757, 430.20471245075777] -0.1 False\n",
      "1\n",
      "217.0371844096599 399.14220849053925 255.8490134606998 389.4653326665525 272.7835461526766 457.3860335058723 233.97171710163664 467.06290932985905\n",
      "[244.91036528116825, 428.2641209982058] -0.1 False\n",
      "0\n",
      "215.76218655298985 399.17144706076624 254.21265439052266 388.14595282808625 273.50726929771264 455.43427154376866 235.0568014601798 466.45976577644865\n",
      "[244.63472792535123, 427.30285930226745] -0.1 False\n",
      "0\n",
      "214.48898580195 399.2451646031409 252.5312464537562 386.88448482814294 274.1624360600026 453.45844096880376 236.12017540819633 465.8191207438017\n",
      "[244.3257109309763, 426.3518027859723] -0.1 False\n",
      "0\n",
      "213.21913335553398 399.36327130419295 250.8068381869704 385.6824655711662 274.7482482197673 451.46094902617983 237.1605433883308 465.1417547592066\n",
      "[243.98369078765063, 425.4121101651864] -0.1 False\n",
      "0\n",
      "211.954176333342 399.52562326910027 249.04153051601355 384.54135953246373 275.26399205512746 449.4442293521389 238.17663787245584 464.42849308877544\n",
      "[243.60908419423473, 424.4849263106196] -0.1 False\n",
      "2\n",
      "212.5025066187924 397.4963948288402 250.09021145022882 383.81558909581344 274.0316214830257 449.5940725508271 236.44391665158923 463.27487828385387\n",
      "[243.26706405090903, 423.54523368983365] -0.1 False\n",
      "0\n",
      "211.23754959660042 397.6587467937475 248.32490377927198 382.674483057111 274.54736531838586 447.57735287678616 237.46001113571427 462.5616166134227\n",
      "[242.89245745749315, 422.61804983526685] -0.1 False\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (300 of 300) |######################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "  0% (0 of 300) |                        | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209.97902915391228 397.8651462216492 246.52084745961636 381.5956804986171 274.9924124749224 445.5438625335993 238.4505941692183 461.81332825663134\n",
      "[242.48572081441733, 421.70450437762423] -100 True\n",
      "[INFO] : Récompense maximale obtenue jusqu'ici : -102.3 (épisode 3)\n",
      "\n",
      "+------------+-------+---------------+---------------+---------------+---------------+-------------+\n",
      "| Récompense | Steps |   Durée de    |  Récompense   |  Récompense   |  Récompense   | Atterissage |\n",
      "|            |       |   l'épisode   |  moy. [100]   |   moy. [50]   |   moy. [25]   |             |\n",
      "+============+=======+===============+===============+===============+===============+=============+\n",
      "|  -102.300  |  24   | 0.3 secondes  |   -103.200    |   -103.200    |   -103.200    |      0      |\n",
      "+------------+-------+---------------+---------------+---------------+---------------+-------------+ \n",
      "\n",
      "\u001b[1m\u001b[94m####################################### Episode 3 / 500 ######################################################\u001b[00m\u001b[0;0m\n",
      "1\n",
      "228.0 413.0 268.0 413.0 268.0 483.0 228.0 483.0\n",
      "[248.0, 448.0] -0.1 False\n",
      "0\n",
      "226.75580157832803 412.71992016136255 266.73143465909186 411.3239402932625 269.17439942826695 481.2812981845992 229.19876634750312 482.67727805269925\n",
      "[247.9651005032975, 447.0006091729809] -0.1 False\n",
      "0\n",
      "225.50258644331248 412.4834328385097 265.40514845370546 409.6931738887447 270.28810161579423 479.5226574069324 230.38553960540128 482.31291635669743\n",
      "[247.89534402955337, 446.00304512272106] -0.1 False\n",
      "2\n",
      "226.6511456078814 410.7229652840836 266.62677868864523 409.3269854159836 269.0697434578203 479.2843433073203 229.0941103770565 480.6803231754203\n",
      "[247.86044453285086, 445.00365429570195] -0.1 False\n",
      "1\n",
      "226.5813466144764 408.72418363004545 266.5569796952402 407.3282037619454 268.9999444644153 477.2855616532821 229.02431138365148 478.68154152138214\n",
      "[247.79064553944585, 443.0048726416638] -0.1 False\n",
      "2\n",
      "227.79064553944585 407.0048726416638 267.79064553944585 407.0048726416638 267.79064553944585 477.0048726416638 227.79064553944585 477.0048726416638\n",
      "[247.79064553944585, 442.0048726416638] -0.1 False\n",
      "0\n",
      "226.54644711777388 406.72479280302633 266.5220801985377 405.3288129349263 268.9650449677128 475.286170826263 228.98941188694897 476.68215069436303\n",
      "[247.75574604274334, 441.00548181464467] -0.1 False\n",
      "0\n",
      "225.29323198275833 406.4883054801735 265.1957939931513 403.6980465304085 270.0787471552401 473.5275300485962 230.17618514484712 476.3177889983612\n",
      "[247.68598956899922, 440.00791776438484] -0.1 False\n",
      "1\n",
      "225.1537190352701 404.49317737965384 265.05628104566307 401.7029184298888 269.93923420775184 471.53240194807654 230.03667219735888 474.32266089784156\n",
      "[247.54647662151098, 438.0127896638652] -0.1 False\n",
      "1\n",
      "225.01420608778184 402.4980492791342 264.9167680981748 399.7077903293692 269.7997212602636 469.5372738475569 229.89715924987064 472.3275327973219\n",
      "[247.40696367402273, 436.01766156334554] -0.1 False\n",
      "2\n",
      "226.16276525235077 400.7375817247081 266.1383983331146 399.34160185660807 268.5813631022897 469.29895974794476 228.60573002152586 470.6949396160448\n",
      "[247.37206417732023, 435.01827073632643] -0.1 False\n",
      "1\n",
      "226.09296625894575 398.7388000706699 266.0685993397096 397.3428202025699 268.51156410888467 467.3001780939066 228.53593102812084 468.6961579620066\n",
      "[247.3022651839152, 433.01948908228826] -0.1 False\n",
      "1\n",
      "226.02316726554074 396.74001841663176 265.99880034630456 395.3440385485317 268.44176511547965 465.3013964398684 228.46613203471583 466.69737630796845\n",
      "[247.2324661905102, 431.0207074282501] -0.1 False\n",
      "1\n",
      "225.95336827213572 394.7412367625936 265.92900135289955 393.34525689449356 268.37196612207464 463.30261478583026 228.3963330413108 464.6985946539303\n",
      "[247.16266719710518, 429.0219257742119] -0.1 False\n",
      "0\n",
      "224.70015313712017 394.50474943974075 264.60271514751315 391.71449048997573 269.4856683096019 461.54397400816345 229.58310629920896 464.33423295792846\n",
      "[247.09291072336106, 428.0243617239521] -0.1 False\n",
      "1\n",
      "224.56064018963193 392.5096213392211 264.4632022000249 389.7193623894561 269.3461553621137 459.5488459076438 229.44359335172072 462.3391048574088\n",
      "[246.95339777587282, 426.02923362343245] -0.1 False\n",
      "1\n",
      "224.42112724214368 390.51449323870145 264.32368925253667 387.72423428893643 269.20664241462543 457.55371780712414 229.30408040423248 460.34397675688916\n",
      "[246.81388482838457, 424.0341055229128] -0.1 False\n",
      "1\n",
      "224.28161429465544 388.5193651381818 264.1841763050484 385.7291061884168 269.0671294671372 455.5585897066045 229.16456745674424 458.3488486563695\n",
      "[246.67437188089633, 422.03897742239315] -0.1 False\n",
      "2\n",
      "225.43017345922436 386.7588975837557 265.4058065399882 385.36291771565567 267.8487713091633 455.32027560699237 227.87313822839945 456.7162554750924\n",
      "[246.63947238419382, 421.03958659537403] -0.1 False\n",
      "0\n",
      "224.1769583242088 386.52241026090286 264.0795203346018 383.73215131113784 268.96247349669056 453.56163482932556 229.0599114862976 456.3518937790906\n",
      "[246.5697159104497, 420.0420225451142] -0.1 False\n",
      "2\n",
      "225.32551748877773 384.76194270647676 265.30115056954156 383.36596283837673 267.74411533871665 453.32332072971343 227.76848225795283 454.71930059781346\n",
      "[246.5348164137472, 419.0426317180951] -0.1 False\n",
      "0\n",
      "224.07230235376218 384.5254553836239 263.97486436415517 381.7351964338589 268.85781752624393 451.5646799520466 228.95525551585098 454.35493890181164\n",
      "[246.46505994000307, 418.04506766783527] -0.1 False\n",
      "1\n",
      "223.93278940627394 382.53032728310427 263.8353514166669 379.74006833333925 268.7183045787557 449.56955185152697 228.81574256836274 452.359810801292\n",
      "[246.32554699251483, 416.0499395673156] -0.1 False\n",
      "0\n",
      "222.6720844075138 382.33772059941083 262.45296022224477 378.15658206870467 269.76995265098054 447.77311474448385 229.98907683624958 451.95425327519\n",
      "[246.22101852924717, 415.05541767194734] -0.1 False\n",
      "1\n",
      "222.4630274809785 380.34867680867427 262.24390329570946 376.1675382779681 269.56089572444523 445.7840709537473 229.78001990971427 449.96520948445345\n",
      "[246.01196160271186, 413.0663738812108] -0.1 False\n",
      "0\n",
      "221.19636859331806 380.2001854257156 260.8070913429809 374.6332613873129 270.5492084101855 443.95202619922287 230.93848566052267 449.5189502376255\n",
      "[245.87278850175176, 412.0761058124692] -0.1 False\n",
      "0\n",
      "219.9252990454981 380.0959902573683 259.3176091659865 373.1500631506911 271.4729816026716 442.0866058615457 232.08067148218325 449.03253296822294\n",
      "[245.69914032408485, 411.091298059457] -0.1 False\n",
      "0\n",
      "218.65136743996936 380.0362182493952 257.77727146932165 371.7197506166848 272.33108982656483 440.1900826680513 233.20518579721255 448.50655030076166\n",
      "[245.4912286332671, 410.1131504587232] -0.1 False\n",
      "0\n",
      "217.3761258661591 380.0209422247807 256.187954917199 370.34406640079396 273.1224876091758 438.26476724011377 234.31065855813586 447.9416430641005\n",
      "[245.24930673766744, 409.14285473244723] -0.1 False\n",
      "2\n",
      "218.20153385355195 378.0877749223854 257.32743788290423 369.771307289675 271.8812562401474 438.2416393410415 232.75535221079514 446.5581069737519\n",
      "[245.0413950468497, 408.16470713171344] -0.1 False\n",
      "1\n",
      "217.78571047191642 376.1314797209178 256.91161450126873 367.8150120882074 271.4654328585119 436.2853441395739 232.3395288291596 444.6018117722843\n",
      "[244.62557166521418, 406.20841193024586] -0.1 False\n",
      "2\n",
      "218.67808220896046 374.22829637514496 258.0703923294489 367.28236926846773 270.22576476613403 436.21891197932234 230.83345464564562 443.1648390859996\n",
      "[244.45192348754725, 405.22360417723365] -0.1 False\n",
      "2\n",
      "219.6363304781534 372.35741572173845 259.24705322781637 366.7904916833358 268.9891702950209 436.10925649524575 229.37844754535803 441.6761805336484\n",
      "[244.31275038658717, 404.2333361084921] -0.1 False\n",
      "1\n",
      "219.35798427623328 370.37687958425533 258.9687070258962 364.8099555458527 268.71082409310077 434.12872035776263 229.1001013434379 439.6956443961653\n",
      "[244.03440418466704, 402.252799971009] -0.1 False\n",
      "2\n",
      "220.380941599666 368.5405810031042 260.16181741439704 364.35944247239803 267.4788098431328 433.9759751481772 227.69793402840176 438.1571136788834\n",
      "[243.9298757213994, 401.2582780756407] -0.1 False\n",
      "0\n",
      "219.11428271200555 368.3920896201455 258.7250054616685 362.82516558174285 268.46712252887306 432.1439303936528 228.85639977921016 437.71085443205544\n",
      "[243.7907026204393, 400.26801000689915] -0.1 False\n",
      "0\n",
      "217.84321316418558 368.28789445179825 257.23552328467406 361.341967345121 269.3908957213592 430.27851005597563 229.99858560087074 437.22443716265286\n",
      "[243.61705444277237, 399.28320225388694] -0.1 False\n",
      "2\n",
      "218.80146143337853 366.41701379839174 258.4121841830415 360.8500897599891 268.1543012502461 430.16885457189903 228.54357850058315 435.7357786103017\n",
      "[243.4778813418123, 398.2929341851454] -0.1 False\n",
      "0\n",
      "217.53039188555857 366.3128186300445 256.9227020060471 359.36689152336726 269.0780744427322 428.30343423422187 229.68576432224373 435.2493613408991\n",
      "[243.3042331641454, 397.3081264321332] -0.1 False\n",
      "2\n",
      "218.48864015475152 364.441937976638 258.09936290441453 358.8750139382353 267.8414799716191 428.19377875014527 228.23075722195614 433.7607027885479\n",
      "[243.1650600631853, 396.3178583633916] -0.1 False\n",
      "0\n",
      "217.21757060693156 364.3377428082907 256.6098807274201 357.3918157016135 268.7652531641052 426.3283584124681 229.37294304361671 433.27428551914534\n",
      "[242.9914118855184, 395.3330506103794] -0.1 False\n",
      "2\n",
      "218.1758188761245 362.4668621548842 257.78654162578755 356.89993811648156 267.5286586929921 426.2187029283915 227.91793594332913 431.78562696679415\n",
      "[242.85223878455832, 394.34278254163786] -0.1 False\n",
      "0\n",
      "216.90474932830455 362.36266698653696 256.2970594487931 355.41673987985973 268.45243188547823 424.35328259071434 229.0601217649897 431.2992096973916\n",
      "[242.6785906068914, 393.35797478862565] -0.1 False\n",
      "2\n",
      "217.8629975974975 360.49178633313045 257.47372034716057 354.9248622947278 267.2158374143651 424.24362710663775 227.6051146647021 429.8105511450404\n",
      "[242.5394175059313, 392.3677067198841] -0.1 False\n",
      "1\n",
      "217.58465139557737 358.51125019564734 257.1953741452404 352.9443261572447 266.93749121244497 422.26309096915463 227.32676846278198 427.8300150075573\n",
      "[242.26107130401118, 390.387170582401] -0.1 False\n",
      "2\n",
      "218.60760871901007 356.6749516144962 258.38848453374123 352.49381308379003 265.705476962477 422.1103457595692 225.92460114774585 426.2914842902754\n",
      "[242.15654284074355, 389.3926486870327] -0.1 False\n",
      "1\n",
      "218.39855179247476 354.68590782375963 258.1794276072059 350.50476929305347 265.4964200359417 420.12130196883265 225.71554422121054 424.3024404995388\n",
      "[241.94748591420824, 387.40360489629614] -0.1 False\n",
      "1\n",
      "218.18949486593945 352.6968640330231 257.9703706806706 348.5157255023169 265.2873631094064 418.1322581780961 225.50648729467522 422.31339670880226\n",
      "[241.73842898767293, 385.4145611055596] -0.1 False\n",
      "2\n",
      "219.2759149276878 350.8973847710884 259.178476938081 348.1071258213234 264.0614301001698 417.9366093395111 224.1588680897766 420.7268682892761\n",
      "[241.6686725139288, 384.41699705529976] -0.1 False\n",
      "0\n",
      "218.01520992892767 350.70477808739497 257.79608574365886 346.5236395566888 265.11307817239464 416.140172232468 225.33220235766345 420.32131076317415\n",
      "[241.56414405066116, 383.4224751599315] -0.1 False\n",
      "0\n",
      "216.74855104126723 350.5562867044363 256.35927379093033 344.9893626660336 266.1013908581349 414.3081274779436 226.49066810847185 419.8750515163462\n",
      "[241.42497094970105, 382.4322070911899] -0.1 False\n",
      "1\n",
      "216.4702048393471 348.57575056695316 256.0809275890102 343.0088265285505 265.82304465621473 412.32759134046046 226.21232190655172 417.8945153788631\n",
      "[241.14662474778092, 380.4516709537068] -0.1 False\n",
      "0\n",
      "215.19913529152714 348.4715553986059 254.59144541201573 341.5256282919287 266.74681784870086 410.4621710027833 227.3545077282123 417.4080981094605\n",
      "[240.97297657011399, 379.4668632006946] -0.1 False\n",
      "1\n",
      "214.85183893619327 346.5019398925815 254.24414905668186 339.5560127859043 266.399521493367 408.4925554967589 227.00721137287843 415.4384826034361\n",
      "[240.62568021478012, 377.4972476946702] -0.1 False\n",
      "0\n",
      "213.57790733066454 346.44216788460835 252.70381136001706 338.12570025189797 267.2576297172602 406.59603230326445 228.13172568790773 414.91249993597484\n",
      "[240.41776852396237, 376.5191000939364] -0.1 False\n",
      "1\n",
      "213.162083949029 344.48587268314077 252.28798797838152 336.1694050504304 266.8418063356247 404.63973710179687 227.7159023062722 412.95620473450725\n",
      "[240.00194514232686, 374.5628048924688] -0.1 False\n",
      "2\n",
      "214.05445568607306 342.58268933736787 253.44676580656164 335.6367622306907 265.6021382432468 404.57330494154536 226.2098281227582 411.51923204822253\n",
      "[239.82829696465996, 373.5779971394566] -0.1 False\n",
      "0\n",
      "212.78052408054432 342.5229173293948 251.90642810989684 334.2064496966844 266.46024646714005 402.67678174805087 227.3343424377875 410.99324938076126\n",
      "[239.6203852738422, 372.5998495387228] -0.1 False\n",
      "0\n",
      "211.50528250673406 342.5076413047803 250.31711155777418 332.83076548079356 267.251644249751 400.75146632011337 228.43981519871085 410.4283421441001\n",
      "[239.37846337824254, 371.62955381244683] -0.1 False\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (300 of 300) |######################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "  0% (0 of 300) |                        | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210.23028465006402 342.5368798750073 248.68075248759703 331.5113856423273 267.97536739478704 398.7997043580097 229.524899557254 409.8251985906897\n",
      "[239.10282602242552, 370.6682921165085] -0.1 False\n",
      "0\n",
      "208.95708389902418 342.61059741738194 246.99934455083059 330.249917642384 268.630534157077 396.8238737830448 230.58827350527054 409.18455355804275\n",
      "[238.7938090280506, 369.71723560021337] -100 True\n",
      "[INFO] : Récompense maximale obtenue jusqu'ici : -102.3 (épisode 4)\n",
      "\n",
      "+------------+-------+---------------+---------------+---------------+---------------+-------------+\n",
      "| Récompense | Steps |   Durée de    |  Récompense   |  Récompense   |  Récompense   | Atterissage |\n",
      "|            |       |   l'épisode   |  moy. [100]   |   moy. [50]   |   moy. [25]   |             |\n",
      "+============+=======+===============+===============+===============+===============+=============+\n",
      "|    -106    |  61   | 0.7 secondes  |   -104.200    |   -104.200    |   -104.200    |      0      |\n",
      "+------------+-------+---------------+---------------+---------------+---------------+-------------+ \n",
      "\n",
      "\u001b[1m\u001b[94m####################################### Episode 4 / 500 ######################################################\u001b[00m\u001b[0;0m\n",
      "1\n",
      "228.0 413.0 268.0 413.0 268.0 483.0 228.0 483.0\n",
      "[248.0, 448.0] -0.1 False\n",
      "1\n",
      "228.0 411.0 268.0 411.0 268.0 481.0 228.0 481.0\n",
      "[248.0, 446.0] -0.1 False\n",
      "2\n",
      "229.26856534090814 409.3239402932625 269.24419842167197 410.71992016136255 266.8012336524969 480.67727805269925 226.82560057173305 479.2812981845992\n",
      "[248.0348994967025, 445.0006091729809] -0.1 False\n",
      "2\n",
      "230.59485154629454 407.6931738887447 270.4974135566875 410.4834328385097 265.6144603945987 480.31291635669743 225.71189838420574 477.5226574069324\n",
      "[248.10465597044663, 444.00304512272106] -0.1 False\n",
      "1\n",
      "230.73436449378278 405.69804578822504 270.63692650417573 408.48830473799006 265.75397334208697 478.3177882561778 225.85141133169398 475.52752930641276\n",
      "[248.24416891793487, 442.0079170222014] -0.1 False\n",
      "0\n",
      "229.512734258843 405.33185731546394 269.48836733960684 406.72783718356396 267.04540257043175 476.68519507490066 227.06976948966792 475.28921520680063\n",
      "[248.27906841463738, 441.0085261951823] -0.1 False\n",
      "2\n",
      "230.8390204642294 403.7010909109461 270.74158247462236 406.4913498607111 265.8586293125336 476.32083337889884 225.9560673021406 473.5305744291338\n",
      "[248.3488248883815, 440.0109621449225] -0.1 False\n",
      "2\n",
      "232.22141165865156 402.1176046463115 272.00228747338247 406.2987431770177 264.6852950446467 475.91527585279687 224.9044192299158 471.7341373220907\n",
      "[248.45335335164913, 439.0164402495542] -0.1 False\n",
      "2\n",
      "233.65822361138012 400.58332775565634 273.2689463610429 406.150251794059 263.5268292938383 475.46901660596893 223.9161065441755 469.9020925675663\n",
      "[248.5925264526092, 438.02617218081264] -0.1 False\n",
      "0\n",
      "232.4651132228793 400.1328146822017 272.2459890376102 404.31395321290785 264.9289966088744 473.93048588868703 225.14812079414352 469.74934735798087\n",
      "[248.69705491587683, 437.03165028544436] -0.1 False\n",
      "1\n",
      "232.6741701494146 398.1437708914651 272.4550459641455 402.3249094221713 265.1380535354097 471.94144209795047 225.35717772067883 467.7603035672443\n",
      "[248.90611184241214, 435.0426064947078] -0.1 False\n",
      "2\n",
      "234.11098210214317 396.60949400080995 273.7217048518059 402.1764180392126 263.97958778460134 471.49518285112254 224.36886503493855 465.9282588127199\n",
      "[249.04528494337225, 434.05233842596624] -0.1 False\n",
      "2\n",
      "235.6004642791376 395.1262957641881 274.99277439962583 402.07222287086535 262.8374019629407 471.00876558171996 223.44509184245246 464.0628384750427\n",
      "[249.21893312103916, 433.06753067295404] -0.1 False\n",
      "2\n",
      "237.14080197580242 393.6959832301818 276.26670600515456 402.0124508628922 261.7128876479114 470.4827829142587 222.58698361855923 462.1663152815483\n",
      "[249.42684481185688, 432.08938307222024] -0.1 False\n",
      "1\n",
      "237.55662535743792 391.7396880287142 276.68252938679007 400.0561556614246 262.1287110295469 468.5264877127911 223.00280700019474 460.2100200800807\n",
      "[249.8426681934924, 430.13308787075266] -0.1 False\n",
      "0\n",
      "236.3978475292578 391.20704520897453 275.790157649746 398.15297231565177 263.63478521306087 467.0895150265064 224.24247509257265 460.14358791982914\n",
      "[250.01631637115935, 429.14828011774046] -0.1 False\n",
      "2\n",
      "237.9381852259226 389.77673267496823 277.0640892552747 398.0932003076786 262.51027089803154 466.5635323590451 223.38436686867942 458.2470647263347\n",
      "[250.22422806197707, 428.17013251700666] -0.1 False\n",
      "2\n",
      "239.52750177804523 388.4010484590774 278.339330829085 398.07792428306414 261.40479813710823 465.99862512238394 222.59296908606848 456.3217492983972\n",
      "[250.46614995757673, 427.19983679073067] -0.1 False\n",
      "0\n",
      "238.38801881234002 387.82828934795845 277.51392284169214 396.14475698066883 262.96010448444895 464.6150890320353 223.83420045509683 456.29862139932493\n",
      "[250.67406164839448, 426.2216891899969] -0.1 False\n",
      "1\n",
      "238.80384219397553 385.87199414649086 277.92974622332764 394.18846177920125 263.37592786608445 462.65879383056773 224.25002383673234 454.34232619785735\n",
      "[251.08988503003, 424.2653939885293] -0.1 False\n",
      "0\n",
      "237.6450643657954 385.33935132675117 277.03737448628357 392.2852784334284 264.88200204959844 461.221821144283 225.48969192911025 454.2758940376058\n",
      "[251.26353320769692, 423.2805862355171] -0.1 False\n",
      "1\n",
      "237.99236072112927 383.36973582072676 277.38467084161744 390.315662927404 265.2292984049323 459.2522056382586 225.83698828444412 452.30627853158137\n",
      "[251.6108295630308, 421.3109707294927] -0.1 False\n",
      "2/2 [==============================] - 1s 5ms/step - loss: 5455.7002 - accuracy: 0.9844\n",
      "0\n",
      "236.81569982276184 382.87785823559483 276.4264225724245 388.4447822739975 266.6843055052199 457.7635470859074 227.07358275555723 452.1966230475048\n",
      "[251.75000266399087, 420.3207026607511] -0.1 False\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4096.6641 - accuracy: 1.0000\n",
      "0\n",
      "235.62258943426102 382.42734516214017 275.4034652489918 386.60848369284633 268.086472820256 456.2250163686255 228.30559700552524 452.04387783791935\n",
      "[251.8545311272585, 419.32618076538284] -0.1 False\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2868.9832 - accuracy: 0.9844\n",
      "0\n",
      "234.41448317685064 382.01874548114665 274.3170451872434 384.80900443091167 269.43409202515465 454.6384879490994 229.53153001476184 451.84822899933437\n",
      "[251.92428760100265, 418.328616715123] -0.1 False\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1745.2437 - accuracy: 1.0000\n",
      "1\n",
      "234.55399612433888 380.023617380627 274.45655813473167 382.813876330392 269.5736049726429 452.64335984857973 229.67104296225008 449.8531008988147\n",
      "[252.0638005484909, 416.33348861460337] -0.1 False\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1086.7849 - accuracy: 0.9844\n",
      "2\n",
      "235.93638731876104 378.4401311159925 275.71726313349177 382.6212696466986 268.400270704756 452.2378023224777 228.61939489002526 448.0566637917716\n",
      "[252.16832901175852, 415.3389667192351] -0.1 False\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 548.5881 - accuracy: 1.0000\n",
      "0\n",
      "234.72828106135066 378.0315314349989 274.6308430717434 380.8217903847639 269.74788990965465 450.65127390295163 229.84532789926186 447.8610149531866\n",
      "[252.23808548550267, 414.34140266897526] -0.1 False\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 844.1356 - accuracy: 0.9688\n",
      "1\n",
      "234.8677940088389 376.03640333447925 274.77035601923166 378.82666228424426 269.8874028571429 448.656145802432 229.9848408467501 445.86588685266696\n",
      "[252.3775984329909, 412.3462745684556] -0.1 False\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 145.2106 - accuracy: 0.5000\n",
      "0\n",
      "233.64616377389913 375.6702148617181 273.62179685466276 377.06619472981816 271.17883208548767 447.0235526211549 231.20319900472404 445.62757275305484\n",
      "[252.41249792969342, 411.3468837414365] -0.1 False\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 541.5287 - accuracy: 1.0000\n",
      "0\n",
      "232.4124979296935 375.34688374143644 272.4124979296933 375.3468837414365 272.4124979296933 445.34688374143656 232.41249792969347 445.3468837414365\n",
      "[252.41249792969342, 410.3468837414365] -0.1 False\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 63.7894 - accuracy: 1.0000\n",
      "1\n",
      "232.4124979296935 373.34688374143644 272.4124979296933 373.3468837414365 272.4124979296933 443.34688374143656 232.41249792969347 443.3468837414365\n",
      "[252.41249792969342, 408.3468837414365] -0.1 False\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 55.2745 - accuracy: 1.0000\n",
      "1\n",
      "232.4124979296935 371.34688374143644 272.4124979296933 371.3468837414365 272.4124979296933 441.34688374143656 232.41249792969347 441.3468837414365\n",
      "[252.41249792969342, 406.3468837414365] -0.1 False\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 508.8071 - accuracy: 1.0000\n",
      "0\n",
      "231.16829950802153 371.066803902799 271.14393258878516 369.670824034699 273.58689735796025 439.6281819260358 233.6112642771966 441.02416179413575\n",
      "[252.3775984329909, 405.3474929144174] -0.1 False\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 36.4162 - accuracy: 0.9062\n",
      "1\n",
      "231.09850051461652 369.0680222487608 271.07413359538015 367.67204238066086 273.51709836455524 437.6294002719976 233.54146528379158 439.0253801400976\n",
      "[252.3077994395859, 403.3487112603792] -0.1 False\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 602.9658 - accuracy: 1.0000\n",
      "0\n",
      "229.84528537960097 368.831534925908 269.74784738999375 366.041275976143 274.6308005520825 435.8707594943308 234.72823854168973 438.66101844409576\n",
      "[252.23804296584174, 402.3511472101194] -0.1 False\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 596.9816 - accuracy: 1.0000\n",
      "0\n",
      "228.58458038084083 368.63892824221455 268.3654561955716 364.45778971150844 275.68244862430737 434.0743223872877 235.90157280957658 438.2554609179938\n",
      "[252.1335145025741, 401.3566253147511] -0.1 False\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 41.6841 - accuracy: 1.0000\n",
      "0\n",
      "227.3179214931804 368.49043685925585 266.92864424284306 362.92351282085326 276.6707613100476 432.24227763276326 237.06003856038498 437.80920167116585\n",
      "[251.994341401614, 400.36635724600956] -0.1 False\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 333.7053 - accuracy: 1.0000\n",
      "0\n",
      "226.04685194536043 368.3862416909086 265.4391620658486 361.44031458423143 277.59453450253375 430.3768572950861 238.20222438204556 437.3227844017633\n",
      "[251.8206932239471, 399.38154949299735] -0.1 False\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 32.9438 - accuracy: 1.0000\n",
      "1\n",
      "225.69955559002656 366.4166261848842 265.09186571051475 359.470699078207 277.2472381471999 428.4072417890617 237.8549280267117 435.35316889573886\n",
      "[251.47339686861324, 397.41193398697294] -0.1 False\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 306.8464 - accuracy: 1.0000\n",
      "2\n",
      "226.6578038592195 364.5457455314777 266.2685266088822 358.9788214930751 276.0106436760868 428.2975863049851 236.3999209264241 433.8645103433877\n",
      "[251.33422376765316, 396.4216659182314] -0.1 False\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 10.6970 - accuracy: 1.0000\n",
      "0\n",
      "225.38673431139955 364.44155036313043 264.77904443188777 357.49562325645326 276.9344168685729 426.4321659673079 237.54210674808468 433.3780930739851\n",
      "[251.16057558998622, 395.4368581652192] -0.1 False\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 637.6164 - accuracy: 1.0000\n",
      "2\n",
      "226.3449825805925 362.5706697097239 265.9557053302552 357.0037456713213 275.6978223974598 426.3225104832313 236.0870996477971 431.8894345216339\n",
      "[251.02140248902614, 394.4465900964776] -0.1 False\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 307.8334 - accuracy: 0.9844\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ngong\\anaconda3\\lib\\tkinter\\__init__.py\", line 1892, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"C:\\Users\\ngong\\AppData\\Local\\Temp\\ipykernel_6204\\3750952494.py\", line 266, in train\n",
      "    next_state, reward, done = env.step(action)\n",
      "  File \"C:\\Users\\ngong\\AppData\\Local\\Temp\\ipykernel_6204\\3750952494.py\", line 176, in step\n",
      "    self.button_f.invoke()\n",
      "  File \"C:\\Users\\ngong\\anaconda3\\lib\\tkinter\\__init__.py\", line 2672, in invoke\n",
      "    return self.tk.call(self._w, 'invoke')\n",
      "_tkinter.TclError: invalid command name \".!frame.!button3\"\n"
     ]
    }
   ],
   "source": [
    "env = VehicleSimulator()\n",
    "env.window.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a99dd89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
